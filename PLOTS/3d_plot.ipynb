{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: peft in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: bitsandbytes in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.46.1)\n",
      "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.8.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (2.7.0+cu128)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==3.3.0->torch>=1.13.0->peft) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface transformers peft bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ad6573cdb84c2d920ce17c55b250d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (6.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from plotly) (1.47.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from plotly) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedModelComparator:\n",
    "    def __init__(self, target_modules=None, device='auto'):\n",
    "        \"\"\"\n",
    "        Initialize unified model comparator for base, LoRA, and GRIT models\n",
    "        \n",
    "        Args:\n",
    "            target_modules: List of target modules to analyze (e.g., [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"])\n",
    "            device: Device to use ('auto', 'cuda', 'cpu')\n",
    "        \"\"\"\n",
    "        self.device = self._setup_device(device)\n",
    "        self.target_modules = target_modules or [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "        self.all_weights = {}\n",
    "        \n",
    "        print(f\"ðŸ”§ Using device: {self.device}\")\n",
    "        print(f\"ðŸŽ¯ Target modules: {', '.join(self.target_modules)}\")\n",
    "        \n",
    "    def _setup_device(self, device):\n",
    "        \"\"\"Setup device for computations\"\"\"\n",
    "        if device == 'auto':\n",
    "            return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return torch.device(device)\n",
    "        \n",
    "    def _clear_gpu_memory(self):\n",
    "        \"\"\"Clear GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    def get_effective_weights(self, model, model_name):\n",
    "        \"\"\"Get the effective weights for target modules\"\"\"\n",
    "        print(f\"ðŸ“Š Extracting weights from {model_name}...\")\n",
    "        effective_weights = {}\n",
    "        \n",
    "        relevant_params = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(module in name for module in self.target_modules):\n",
    "                relevant_params.append((name, param))\n",
    "        \n",
    "        with tqdm(total=len(relevant_params), desc=f\"Extracting {model_name} weights\", unit=\"param\") as pbar:\n",
    "            for name, param in relevant_params:\n",
    "                # Move to CPU immediately to save GPU memory\n",
    "                effective_weights[name] = param.detach().cpu().clone()\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Clear memory periodically\n",
    "                if len(effective_weights) % 10 == 0:\n",
    "                    self._clear_gpu_memory()\n",
    "        \n",
    "        return effective_weights\n",
    "    \n",
    "    def merge_and_get_weights(self, base_model_path, adapter_path, model_name):\n",
    "        \"\"\"Load base model, merge with adapter, and get effective weights\"\"\"\n",
    "        print(f\"Loading base model from {base_model_path}...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Loading adapter from {adapter_path}...\")\n",
    "        model_with_adapter = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        \n",
    "        print(\"Merging adapter with base model...\")\n",
    "        model_with_adapter = model_with_adapter.merge_and_unload()\n",
    "        \n",
    "        weights = self.get_effective_weights(model_with_adapter, model_name)\n",
    "        \n",
    "        # Clean up\n",
    "        del base_model, model_with_adapter\n",
    "        self._clear_gpu_memory()\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def load_all_models(self, base_model_path, lora_model_path, grit_model_path):\n",
    "        \"\"\"Load all three models and extract their weights\"\"\"\n",
    "        print(\"ðŸš€ Loading all models for comparison...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load base model\n",
    "        print(\"\\n1. Loading base model...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        self.all_weights['Base'] = self.get_effective_weights(base_model, \"Base\")\n",
    "        del base_model\n",
    "        self._clear_gpu_memory()\n",
    "        \n",
    "        # Load LoRA model\n",
    "        print(\"\\n2. Loading LoRA model...\")\n",
    "        self.all_weights['LoRA'] = self.merge_and_get_weights(\n",
    "            base_model_path, lora_model_path, \"LoRA\"\n",
    "        )\n",
    "        \n",
    "        # Load GRIT model\n",
    "        print(\"\\n3. Loading GRIT model...\")\n",
    "        self.all_weights['GRIT'] = self.merge_and_get_weights(\n",
    "            base_model_path, grit_model_path, \"GRIT\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ… All models loaded successfully!\")\n",
    "    \n",
    "    def compute_weight_differences(self):\n",
    "        \"\"\"Compute weight differences between adapted models and base model\"\"\"\n",
    "        print(\"ðŸ“Š Computing weight differences...\")\n",
    "        \n",
    "        base_weights = self.all_weights['Base']\n",
    "        differences = {}\n",
    "        \n",
    "        for model_name in ['LoRA', 'GRIT']:\n",
    "            model_weights = self.all_weights[model_name]\n",
    "            differences[model_name] = {}\n",
    "            \n",
    "            for param_name in base_weights.keys():\n",
    "                if param_name in model_weights:\n",
    "                    # Compute difference: adapted - base\n",
    "                    diff = model_weights[param_name] - base_weights[param_name]\n",
    "                    differences[model_name][param_name] = diff\n",
    "        \n",
    "        return differences\n",
    "    \n",
    "    def create_unified_comparison_plot(self, points_per_module=1000):\n",
    "        \"\"\"Create unified 3D plot comparing all three models using all target modules\"\"\"\n",
    "        print(\"ðŸŽ¨ Creating unified comparison plot...\")\n",
    "        print(f\"ðŸ“Š Using ALL target modules: {', '.join(self.target_modules)}\")\n",
    "        \n",
    "        # Get weight differences\n",
    "        differences = self.compute_weight_differences()\n",
    "        \n",
    "        # Prepare data for visualization\n",
    "        all_points = []\n",
    "        all_colors = []\n",
    "        all_labels = []\n",
    "        all_models = []\n",
    "        all_modules = []\n",
    "        \n",
    "        # Color mapping\n",
    "        color_map = {\n",
    "            'Base': 'blue',\n",
    "            'LoRA': 'red', \n",
    "            'GRIT': 'green'\n",
    "        }\n",
    "        \n",
    "        model_index = 0\n",
    "        \n",
    "        # Process base model weights - ALL target module parameters\n",
    "        print(\"Processing base model weights from all target modules...\")\n",
    "        base_weights = self.all_weights['Base']\n",
    "        \n",
    "        # Group parameters by target module type\n",
    "        module_params = {module: [] for module in self.target_modules}\n",
    "        for param_name in base_weights.keys():\n",
    "            for module in self.target_modules:\n",
    "                if module in param_name:\n",
    "                    module_params[module].append(param_name)\n",
    "                    break\n",
    "        \n",
    "        total_params = sum(len(params) for params in module_params.values())\n",
    "        print(f\"ðŸ“Š Found {total_params} parameters across {len(self.target_modules)} target modules\")\n",
    "        \n",
    "        with tqdm(total=total_params, desc=\"Processing base weights\", unit=\"param\") as pbar:\n",
    "            for module_type, param_names in module_params.items():\n",
    "                if not param_names:\n",
    "                    continue\n",
    "                    \n",
    "                for param_name in param_names:\n",
    "                    weight_tensor = base_weights[param_name]\n",
    "                    points_3d = self._extract_3d_points(weight_tensor, points_per_module)\n",
    "                    \n",
    "                    all_points.extend(points_3d)\n",
    "                    all_colors.extend([model_index] * len(points_3d))\n",
    "                    all_labels.extend([param_name.split('.')[-2:]] * len(points_3d))  # layer + module\n",
    "                    all_models.extend(['Base'] * len(points_3d))\n",
    "                    all_modules.extend([module_type] * len(points_3d))\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "        \n",
    "        model_index += 1\n",
    "        \n",
    "        # Process adapted models (differences) - ALL target module parameters\n",
    "        for model_name in ['LoRA', 'GRIT']:\n",
    "            print(f\"Processing {model_name} differences from all target modules...\")\n",
    "            model_diffs = differences[model_name]\n",
    "            \n",
    "            with tqdm(total=total_params, desc=f\"Processing {model_name} diffs\", unit=\"param\") as pbar:\n",
    "                for module_type, param_names in module_params.items():\n",
    "                    if not param_names:\n",
    "                        continue\n",
    "                        \n",
    "                    for param_name in param_names:\n",
    "                        if param_name in model_diffs:\n",
    "                            diff_tensor = model_diffs[param_name]\n",
    "                            points_3d = self._extract_3d_points(diff_tensor, points_per_module)\n",
    "                            \n",
    "                            all_points.extend(points_3d)\n",
    "                            all_colors.extend([model_index] * len(points_3d))\n",
    "                            all_labels.extend([param_name.split('.')[-2:]] * len(points_3d))\n",
    "                            all_models.extend([model_name] * len(points_3d))\n",
    "                            all_modules.extend([module_type] * len(points_3d))\n",
    "                        \n",
    "                        pbar.update(1)\n",
    "            \n",
    "            model_index += 1\n",
    "        \n",
    "        # Create the unified 3D plot\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add traces for each model\n",
    "        for i, model_name in enumerate(['Base', 'LoRA', 'GRIT']):\n",
    "            mask = np.array(all_models) == model_name\n",
    "            if np.any(mask):\n",
    "                model_points = np.array(all_points)[mask]\n",
    "                model_labels = np.array(all_labels)[mask]\n",
    "                model_modules = np.array(all_modules)[mask]\n",
    "                \n",
    "                # Create hover text with module information\n",
    "                hover_text = []\n",
    "                for j, (label, module) in enumerate(zip(model_labels, model_modules)):\n",
    "                    if isinstance(label, list):\n",
    "                        layer_info = '.'.join(label)\n",
    "                    else:\n",
    "                        layer_info = str(label)\n",
    "                    hover_text.append(f\"{module} - {layer_info}\")\n",
    "                \n",
    "                fig.add_trace(go.Scatter3d(\n",
    "                    x=model_points[:, 0],\n",
    "                    y=model_points[:, 1],\n",
    "                    z=model_points[:, 2],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=4,\n",
    "                        color=color_map[model_name],\n",
    "                        opacity=0.7,\n",
    "                        symbol='circle'\n",
    "                    ),\n",
    "                    name=f'{model_name} {\"(Weights)\" if model_name == \"Base\" else \"(Î” Weights)\"}',\n",
    "                    text=hover_text,\n",
    "                    hovertemplate=f'<b>{model_name}</b><br>' +\n",
    "                                 'Module: %{text}<br>' +\n",
    "                                 'X: %{x:.4f}<br>' +\n",
    "                                 'Y: %{y:.4f}<br>' +\n",
    "                                 'Z: %{z:.4f}<extra></extra>'\n",
    "                ))\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'text': \"Comprehensive Model Comparison: Base vs LoRA vs GRIT<br>\" +\n",
    "                       f\"<sub>Target Modules: {', '.join(self.target_modules)}</sub>\",\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'font': {'size': 16}\n",
    "            },\n",
    "            scene=dict(\n",
    "                xaxis_title='Principal Component 1',\n",
    "                yaxis_title='Principal Component 2',\n",
    "                zaxis_title='Principal Component 3',\n",
    "                bgcolor='white',\n",
    "                xaxis=dict(gridcolor='lightgray'),\n",
    "                yaxis=dict(gridcolor='lightgray'),\n",
    "                zaxis=dict(gridcolor='lightgray')\n",
    "            ),\n",
    "            width=1000,\n",
    "            height=800,\n",
    "            showlegend=True,\n",
    "            legend=dict(\n",
    "                x=0.02,\n",
    "                y=0.98,\n",
    "                bgcolor='rgba(255,255,255,0.8)',\n",
    "                bordercolor='black',\n",
    "                borderwidth=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add annotations with module information\n",
    "        module_stats = {module: 0 for module in self.target_modules}\n",
    "        for module in all_modules:\n",
    "            if module in module_stats:\n",
    "                module_stats[module] += 1\n",
    "        \n",
    "        stats_text = \"Module Statistics:<br>\" + \"<br>\".join([\n",
    "            f\"â€¢ {module}: {count} points\" for module, count in module_stats.items() if count > 0\n",
    "        ])\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            text=\"â€¢ Base: Original model weights<br>â€¢ LoRA/GRIT: Weight differences (Î”)<br><br>\" + stats_text,\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.02, y=0.02,\n",
    "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "            bordercolor=\"black\",\n",
    "            borderwidth=1,\n",
    "            font=dict(size=9)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _extract_3d_points(self, tensor, points_per_layer):\n",
    "        \"\"\"Extract 3D points from tensor using PCA\"\"\"\n",
    "        # Convert to numpy\n",
    "        numpy_data = tensor.numpy()\n",
    "        \n",
    "        # Sample points if tensor is too large\n",
    "        if numpy_data.size > points_per_layer:\n",
    "            flat_data = numpy_data.flatten()\n",
    "            indices = np.random.choice(len(flat_data), points_per_layer, replace=False)\n",
    "            sampled_data = flat_data[indices]\n",
    "        else:\n",
    "            sampled_data = numpy_data.flatten()\n",
    "        \n",
    "        # Create 3D representation\n",
    "        if len(numpy_data.shape) == 2 and min(numpy_data.shape) >= 3:\n",
    "            # Use PCA for 2D matrices\n",
    "            pca = PCA(n_components=3)\n",
    "            try:\n",
    "                pca_data = pca.fit_transform(numpy_data)\n",
    "                \n",
    "                # Sample from PCA space\n",
    "                if len(pca_data) > points_per_layer:\n",
    "                    indices = np.random.choice(len(pca_data), points_per_layer, replace=False)\n",
    "                    points_3d = pca_data[indices]\n",
    "                else:\n",
    "                    points_3d = pca_data\n",
    "            except:\n",
    "                # Fallback to synthetic 3D representation\n",
    "                points_3d = self._create_synthetic_3d(sampled_data)\n",
    "        else:\n",
    "            # Create synthetic 3D representation for other cases\n",
    "            points_3d = self._create_synthetic_3d(sampled_data)\n",
    "        \n",
    "        return points_3d\n",
    "    \n",
    "    def _create_synthetic_3d(self, data):\n",
    "        \"\"\"Create synthetic 3D representation from 1D data\"\"\"\n",
    "        n_points = len(data)\n",
    "        std_dev = np.std(data) * 0.1\n",
    "        \n",
    "        points_3d = np.column_stack([\n",
    "            data,\n",
    "            np.random.normal(0, std_dev, n_points),\n",
    "            np.random.normal(0, std_dev, n_points)\n",
    "        ])\n",
    "        \n",
    "        return points_3d\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up and free memory\"\"\"\n",
    "        self.all_weights.clear()\n",
    "        self._clear_gpu_memory()\n",
    "        print(\"ðŸ§¹ Memory cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Unified Model Comparison Tool\n",
      "========================================\n",
      "ðŸ”§ Using device: cuda\n",
      "ðŸŽ¯ Target modules: q_proj, k_proj, v_proj, o_proj\n",
      "ðŸš€ Loading all models for comparison...\n",
      "==================================================\n",
      "\n",
      "1. Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8407195896f4d76a9cb88dbe6c068ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Extracting weights from Base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Base weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:03<00:00, 32.14param/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Loading LoRA model...\n",
      "Loading base model from meta-llama/Llama-3.2-3B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8077fa8db34e9ea9efdc58d3aa96f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter from te4bag/LoRA_alpaca_Llama3.2_3B...\n",
      "Merging adapter with base model...\n",
      "ðŸ“Š Extracting weights from LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting LoRA weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:03<00:00, 32.87param/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Loading GRIT model...\n",
      "Loading base model from meta-llama/Llama-3.2-3B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528594622ea94f4381dcbae31f81dd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter from te4bag/grit-lora-Llama-3.2-3B-bnb-4bit-alpaca...\n",
      "Merging adapter with base model...\n",
      "ðŸ“Š Extracting weights from GRIT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting GRIT weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:04<00:00, 27.90param/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All models loaded successfully!\n",
      "ðŸŽ¨ Creating unified comparison plot...\n",
      "ðŸ“Š Using ALL target modules: q_proj, k_proj, v_proj, o_proj\n",
      "ðŸ“Š Computing weight differences...\n",
      "Processing base model weights from all target modules...\n",
      "ðŸ“Š Found 112 parameters across 4 target modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing base weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [01:02<00:00,  1.78param/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LoRA differences from all target modules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LoRA diffs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [01:05<00:00,  1.70param/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GRIT differences from all target modules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GRIT diffs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [01:13<00:00,  1.53param/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Plot saved as model_comparison.html\n",
      "\n",
      "âœ… Unified comparison complete!\n",
      "ðŸ§¹ Memory cleaned up\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Unified Model Comparison Tool\")\n",
    "print(\"=\" * 40)\n",
    "    \n",
    "# Model paths\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B\"\n",
    "LORA_MODEL = \"te4bag/LoRA_alpaca_Llama3.2_3B\"\n",
    "GRIT_MODEL = \"te4bag/grit-lora-Llama-3.2-3B-bnb-4bit-alpaca\"\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    \n",
    "# Initialize comparator with chosen target modules\n",
    "comparator = UnifiedModelComparator(target_modules=TARGET_MODULES)\n",
    "    \n",
    "try:\n",
    "    # Load all models\n",
    "    comparator.load_all_models(BASE_MODEL, LORA_MODEL, GRIT_MODEL)\n",
    "        \n",
    "    # Create unified comparison plot\n",
    "    fig = comparator.create_unified_comparison_plot(points_per_module=1000)\n",
    "        \n",
    "    if fig:\n",
    "        # print(\"\\nðŸŽ¨ Displaying unified comparison plot...\")\n",
    "        # fig.show()\n",
    "            \n",
    "        # save the plot\n",
    "        fig.write_html(\"model_comparison.html\")\n",
    "        print(\"ðŸ’¾ Plot saved as model_comparison.html\")\n",
    "        \n",
    "    print(\"\\nâœ… Unified comparison complete!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during comparison: {e}\")\n",
    "        \n",
    "finally:\n",
    "    comparator.cleanup()\n",
    "\n",
    "# Additional convenience function for quick comparisons\n",
    "def compare_models_with_modules(base_model, lora_model, grit_model, target_modules, points_per_module=1000):\n",
    "    \"\"\"\n",
    "    Quick comparison function with custom target modules\n",
    "    \n",
    "    Args:\n",
    "        base_model: Base model path\n",
    "        lora_model: LoRA model path  \n",
    "        grit_model: GRIT model path\n",
    "        target_modules: List of target modules to analyze\n",
    "        points_per_module: Number of points to sample per module\n",
    "    \"\"\"\n",
    "    print(f\"ðŸŽ¯ Comparing models on modules: {', '.join(target_modules)}\")\n",
    "    \n",
    "    comparator = UnifiedModelComparator(target_modules=target_modules)\n",
    "    \n",
    "    try:\n",
    "        comparator.load_all_models(base_model, lora_model, grit_model)\n",
    "        fig = comparator.create_unified_comparison_plot(points_per_module=points_per_module)\n",
    "        \n",
    "        if fig:\n",
    "            fig.show()\n",
    "            return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        comparator.cleanup()\n",
    "\n",
    "# Example usage patterns:\n",
    "def example_usage():\n",
    "    \"\"\"Examples of how to use the tool with different target modules\"\"\"\n",
    "    \n",
    "    BASE_MODEL = \"meta-llama/Llama-3.2-3B\"\n",
    "    LORA_MODEL = \"te4bag/LoRA_alpaca_Llama3.2_3B\"\n",
    "    GRIT_MODEL = \"te4bag/grit-lora-Llama-3.2-3B-bnb-4bit-alpaca\"\n",
    "    \n",
    "    print(\"Example 1: Attention modules only\")\n",
    "    compare_models_with_modules(\n",
    "        BASE_MODEL, LORA_MODEL, GRIT_MODEL,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
