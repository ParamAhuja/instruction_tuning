{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:35:13.860384Z","iopub.execute_input":"2025-08-08T11:35:13.860616Z","iopub.status.idle":"2025-08-08T11:35:15.192909Z","shell.execute_reply.started":"2025-08-08T11:35:13.860589Z","shell.execute_reply":"2025-08-08T11:35:15.191946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set CUDA_VISIBLE_DEVICES to use only the first GPU (index 0)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:35:15.194270Z","iopub.execute_input":"2025-08-08T11:35:15.194711Z","iopub.status.idle":"2025-08-08T11:35:15.199368Z","shell.execute_reply.started":"2025-08-08T11:35:15.194677Z","shell.execute_reply":"2025-08-08T11:35:15.198448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Install Necessary Libraries\n!pip install -q transformers datasets peft accelerate evaluate scikit-learn torch\n\n# For Kaggle environments, you might also need to install the specific version of `bitsandbytes` if using QLoRA\n!pip install -q bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:35:15.200001Z","iopub.execute_input":"2025-08-08T11:35:15.200277Z","iopub.status.idle":"2025-08-08T11:36:43.297677Z","shell.execute_reply.started":"2025-08-08T11:35:15.200251Z","shell.execute_reply":"2025-08-08T11:36:43.296728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Import Libraries and Log in to Hugging Face\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom peft import PeftModel, PeftConfig # Make sure PeftModel is imported\nimport evaluate\nimport numpy as np\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nimport os\n\n# Optional: Log in to Hugging Face if the adapter repository is private.\n# You can set up your HF_TOKEN as a Kaggle Secret or run `huggingface_hub.login()`\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nfrom huggingface_hub import login\n\n# hf_token = os.environ.get(\"HF_TOKEN\")\nif hf_token:\n    login(token=hf_token)\nelse:\n    print(\"Hugging Face token not found. If the adapter is private, please add it as a Kaggle Secret (HF_TOKEN) and re-run.\")\n\n# Ensure a stable environment, if needed, you might want to set a seed\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:36:43.298657Z","iopub.execute_input":"2025-08-08T11:36:43.298891Z","iopub.status.idle":"2025-08-08T11:37:11.637904Z","shell.execute_reply.started":"2025-08-08T11:36:43.298869Z","shell.execute_reply":"2025-08-08T11:37:11.637121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Define Model and Adapter Paths, and Load Tokenizer\nbase_model_id = \"meta-llama/Llama-3.2-3B\"\nadaptor_repo_id = \"te4bag/GRIT-Full-BoolQ-llama-3.2-3B-Energy-0.9\"\nmax_length = 2048 # As per your training notebook\n\n# Load the tokenizer from the base model\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\n\n# --- CRITICAL FIX: Ensure padding token is defined ---\n# Llama models often don't have a padding token by default.\n# We explicitly set it to the EOS token for batching.\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id # Also set pad_token_id for explicit clarity\n\n# Verify that pad_token is now set\nprint(f\"Tokenizer loaded. Pad token: '{tokenizer.pad_token}', Pad token ID: {tokenizer.pad_token_id}\")\nif tokenizer.pad_token is None:\n    raise ValueError(\"Padding token is still not set after attempting to assign it. Please check tokenizer configuration.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:37:11.639565Z","iopub.execute_input":"2025-08-08T11:37:11.640061Z","iopub.status.idle":"2025-08-08T11:37:12.921715Z","shell.execute_reply.started":"2025-08-08T11:37:11.640041Z","shell.execute_reply":"2025-08-08T11:37:12.921047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Load Base Model and PEFT Adapter (MODIFIED TO FIX PAD_TOKEN_ID)\n# Check for GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Quantization configuration for loading the base model (as done in your training script)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16, # Assuming bf16 as in your GRITConfig\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load the base model for sequence classification\nprint(f\"Loading base model: {base_model_id}...\")\nif device == \"cuda\":\n    model = AutoModelForSequenceClassification.from_pretrained(\n        base_model_id,\n        num_labels=2,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\"\n    )\nelse:\n    model = AutoModelForSequenceClassification.from_pretrained(\n        base_model_id,\n        num_labels=2,\n        device_map=\"auto\"\n    )\n\n# --- CRITICAL FIX: Set the model's pad_token_id from the tokenizer's pad_token_id ---\n# This ensures the model's internal configuration knows about the padding token.\nmodel.config.pad_token_id = tokenizer.pad_token_id\nprint(f\"Model's pad_token_id set to: {model.config.pad_token_id}\")\n\n# Load the PEFT adapter weights on top of the base model\nprint(f\"Loading PEFT adapter from: {adaptor_repo_id}...\")\nmodel = PeftModel.from_pretrained(model, adaptor_repo_id)\n\n# Set the model to evaluation mode\nmodel.eval()\n\nprint(\"Base model and PEFT adapter loaded and merged successfully. Model set to eval mode.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:37:12.922359Z","iopub.execute_input":"2025-08-08T11:37:12.922551Z","iopub.status.idle":"2025-08-08T11:38:20.075469Z","shell.execute_reply.started":"2025-08-08T11:37:12.922536Z","shell.execute_reply":"2025-08-08T11:38:20.074887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Load and Preprocess the Dataset (QNLI Test Split)\nprint(f\"Loading dataset: nyu-mll/glue - QNLI test split...\")\n# Load 'validation' split as test split have no fucking labels \ntest_dataset = load_dataset(\"google/boolq\", split=\"validation\")\n\nprint(f\"Original test dataset size: {len(test_dataset)}\")\n\n# Define the preprocessing function\ndef preprocess_function(examples):\n    # QNLI uses 'question' and 'sentence' as text inputs\n    return tokenizer(examples[\"question\"], examples[\"passage\"], truncation=True, max_length=max_length)\n\n# Apply preprocessing to the test dataset\nprint(\"Tokenizing test dataset...\")\n# Using batched=True for faster processing\nprocessed_test_dataset = test_dataset.map(preprocess_function, batched=True)\n\n# Remove original text columns and ensure 'labels' are present and in PyTorch format\nprocessed_test_dataset = processed_test_dataset.remove_columns([\"question\", \"passage\"])\nprocessed_test_dataset.set_format(\"torch\", columns=[\"answer\", \"input_ids\", \"attention_mask\"])\n\nprint(\"Dataset preprocessing complete for QNLI test split.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:38:20.076248Z","iopub.execute_input":"2025-08-08T11:38:20.076547Z","iopub.status.idle":"2025-08-08T11:38:25.986314Z","shell.execute_reply.started":"2025-08-08T11:38:20.076521Z","shell.execute_reply":"2025-08-08T11:38:25.985408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Define Data Collator and Dataloader\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorWithPadding\n\n# Data collator will dynamically pad sequences to the longest length in a batch\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n\n# Create DataLoader for the test set\n# Using a small batch size for evaluation to minimize memory\ntest_dataloader = DataLoader(processed_test_dataset, batch_size=8, collate_fn=data_collator) # Adjust batch_size as needed\nprint(\"DataLoader created for the QNLI test set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:40:40.743663Z","iopub.execute_input":"2025-08-08T11:40:40.743949Z","iopub.status.idle":"2025-08-08T11:40:40.749425Z","shell.execute_reply.started":"2025-08-08T11:40:40.743929Z","shell.execute_reply":"2025-08-08T11:40:40.748650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: Perform Inference and Collect Predictions\nprint(\"Starting inference on the QNLI test set...\")\npredictions = []\nreferences = []\n\nmodel.to(device) # Ensure model is on the correct device\n\nfor batch in test_dataloader:\n    # Move batch to the same device as the model\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"answer\"].to(device) \n    \n    with torch.no_grad(): # Essential for inference to disable gradient calculations\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    # Get predicted class IDs (0 or 1 for QNLI)\n    preds = torch.argmax(logits, dim=-1).cpu().numpy()\n\n    # Collect true labels\n    references.extend(labels.cpu().numpy())\n    predictions.extend(preds)\n    # REMEMBER TO REMOVE `break` FOR FULL EVALUATION!\n    # break\n\nprint(\"Inference complete. Collected predictions and true labels.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:43:34.655830Z","iopub.execute_input":"2025-08-08T11:43:34.656551Z","iopub.status.idle":"2025-08-08T11:43:40.514543Z","shell.execute_reply.started":"2025-08-08T11:43:34.656526Z","shell.execute_reply":"2025-08-08T11:43:40.513788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"references, predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:43:53.942365Z","iopub.execute_input":"2025-08-08T11:43:53.942928Z","iopub.status.idle":"2025-08-08T11:43:53.947399Z","shell.execute_reply.started":"2025-08-08T11:43:53.942907Z","shell.execute_reply":"2025-08-08T11:43:53.946715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Compute and Display Metrics (Revised for clean labels from custom collator)\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\nimport numpy as np\n\n# If CustomDataCollatorWithPadding worked, references should only contain 0s and 1s.\nprint(f\"Unique labels in references: {np.unique(references)}\")\nprint(f\"Unique predictions: {np.unique(predictions)}\")\n\n# Calculate metrics using scikit-learn\n# No filtering needed if custom collator correctly extracted and passed 0/1 labels.\nif len(references) == 0:\n    print(\"\\nWARNING: No samples available for metric computation.\")\nelse:\n    acc = accuracy_score(references, predictions)\n    f1 = f1_score(references, predictions, average='binary', pos_label=1)\n    precision = precision_score(references, predictions, average='binary', pos_label=1)\n    recall = recall_score(references, predictions, average='binary', pos_label=1)\n\n    print(\"\\n--- Evaluation Results for GLUE QNLI (Test Split) ---\")\n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"F1 Score (Binary for class 1): {f1:.4f}\")\n    print(f\"Precision (Binary for class 1): {precision:.4f}\")\n    print(f\"Recall (Binary for class 1): {recall:.4f}\")\n\n    print(\"\\nNote: QNLI, unlike MNLI, does not have separate 'matched' and 'mismatched' test splits. The evaluation is performed on the single standard 'test' split.\")\n\n    print(\"\\n--- Detailed Classification Report ---\")\n    print(classification_report(references, predictions, target_names=[\"entailment\", \"not_entailment\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:43:40.520510Z","iopub.execute_input":"2025-08-08T11:43:40.520767Z","iopub.status.idle":"2025-08-08T11:43:40.551920Z","shell.execute_reply.started":"2025-08-08T11:43:40.520743Z","shell.execute_reply":"2025-08-08T11:43:40.551315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}