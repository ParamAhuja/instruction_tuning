{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install huggingface huggingface_hub transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:55:51.624891Z","iopub.execute_input":"2025-07-30T09:55:51.625302Z","iopub.status.idle":"2025-07-30T09:55:57.704763Z","shell.execute_reply.started":"2025-07-30T09:55:51.625273Z","shell.execute_reply":"2025-07-30T09:55:57.703843Z"}},"outputs":[{"name":"stdout","text":"Collecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:55:57.705924Z","iopub.execute_input":"2025-07-30T09:55:57.706217Z","iopub.status.idle":"2025-07-30T09:57:24.750995Z","shell.execute_reply.started":"2025-07-30T09:55:57.706191Z","shell.execute_reply":"2025-07-30T09:57:24.750322Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install datasets accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:57:24.752635Z","iopub.execute_input":"2025-07-30T09:57:24.752835Z","iopub.status.idle":"2025-07-30T09:57:31.171434Z","shell.execute_reply.started":"2025-07-30T09:57:24.752815Z","shell.execute_reply":"2025-07-30T09:57:31.170711Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:57:31.172271Z","iopub.execute_input":"2025-07-30T09:57:31.172481Z","iopub.status.idle":"2025-07-30T09:57:34.390142Z","shell.execute_reply.started":"2025-07-30T09:57:31.172460Z","shell.execute_reply":"2025-07-30T09:57:34.389488Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.52.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.8.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.33.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install torchao torchtune","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:57:34.391154Z","iopub.execute_input":"2025-07-30T09:57:34.391470Z","iopub.status.idle":"2025-07-30T09:57:37.793521Z","shell.execute_reply.started":"2025-07-30T09:57:34.391436Z","shell.execute_reply":"2025-07-30T09:57:37.792748Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (0.10.0)\nRequirement already satisfied: torchtune in /usr/local/lib/python3.11/dist-packages (0.6.1)\nRequirement already satisfied: torchdata==0.11.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.11.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.6.0)\nRequirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.3)\nRequirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.12)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.9.0)\nRequirement already satisfied: blobfile>=2 in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.0.0)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.21.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (7.0.0)\nRequirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (11.2.1)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.5.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.32.4)\nRequirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.6.0+cu124)\nRequirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.23.0)\nRequirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.4.0)\nRequirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (2025.3.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchtune) (2.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (1.1.5)\nRequirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->torchtune) (4.9.3)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (3.12.13)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (2025.6.15)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchdata==0.11.0->torchtune) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtune) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtune) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchtune) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchtune) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.20.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchtune) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.11.0->torchtune) (3.0.2)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:57:37.794413Z","iopub.execute_input":"2025-07-30T09:57:37.794642Z","iopub.status.idle":"2025-07-30T09:57:40.806125Z","shell.execute_reply.started":"2025-07-30T09:57:37.794618Z","shell.execute_reply":"2025-07-30T09:57:40.805228Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install --upgrade wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:57:40.807126Z","iopub.execute_input":"2025-07-30T09:57:40.807422Z","iopub.status.idle":"2025-07-30T09:58:18.602104Z","shell.execute_reply.started":"2025-07-30T09:57:40.807386Z","shell.execute_reply":"2025-07-30T09:58:18.601268Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\nCollecting wandb\n  Downloading wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nDownloading wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.20.1\n    Uninstalling wandb-0.20.1:\n      Successfully uninstalled wandb-0.20.1\nSuccessfully installed wandb-0.21.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\n# Disable the tokenizer parallelism warning to avoid spam\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.cuda.amp import GradScaler\nfrom tqdm import tqdm\nimport os\nimport gc\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    get_scheduler,\n    BitsAndBytesConfig,\n    EarlyStoppingCallback\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom peft.tuners.lora import LoraLayer\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport wandb\nimport bitsandbytes as bnb\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom huggingface_hub import login, HfApi, create_repo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:58:18.603349Z","iopub.execute_input":"2025-07-30T09:58:18.603601Z","iopub.status.idle":"2025-07-30T09:58:59.154003Z","shell.execute_reply.started":"2025-07-30T09:58:18.603576Z","shell.execute_reply":"2025-07-30T09:58:59.153447Z"}},"outputs":[{"name":"stderr","text":"2025-07-30 09:58:36.802369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753869517.163685      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753869517.269620      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:10:08.439527Z","iopub.execute_input":"2025-07-30T10:10:08.439867Z","iopub.status.idle":"2025-07-30T10:10:08.456835Z","shell.execute_reply.started":"2025-07-30T10:10:08.439834Z","shell.execute_reply":"2025-07-30T10:10:08.456077Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39db7d7185f04979916502221c1ec7bd"}},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"class KFACAutogradFunction(torch.autograd.Function):\n    \"\"\"Custom autograd function to capture activations and gradients for K-FAC.\"\"\"\n    \n    @staticmethod\n    def forward(ctx, module, output, input):\n        ctx.module = module\n        # We only need the input for the backward pass to compute activation stats\n        ctx.save_for_backward(input.detach())\n        # We pass the output through, it's what the next layer will see\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_wrt_output):\n        # This grad is the gradient w.r.t the output of the LoraLayer\n        module = ctx.module\n        input, = ctx.saved_tensors # This is the original input to the LoraLayer\n        manager = module.grit_manager\n        manager.backward_step += 1\n\n        if not module.training:\n            # Pass gradients through without modification if not training\n            return None, grad_wrt_output, None\n            \n        # --- Run covariance updates periodically to avoid bottlenecking ---\n        if manager.backward_step % manager.config.grit_cov_update_freq != 0:\n            return None, grad_wrt_output, None\n\n        with torch.no_grad():\n            # --- 1. Project activations into r-space and update covariance ---\n            # a has shape (batch*seq, in_features)\n            a = input.reshape(-1, input.shape[-1])\n            if a.shape[0] > 0 and 'default' in module.lora_A:\n                # A has shape (r, in_features), so A.T has (in_features, r)\n                # Cast weight to the same dtype as activation `a` to prevent mismatch\n                lora_A_T = module.lora_A['default'].weight.data.T.to(device=a.device, dtype=a.dtype, non_blocking=True)\n                # projected_a has shape (batch*seq, r)\n                projected_a = a @ lora_A_T\n                a_cov_sample = projected_a.T @ projected_a\n                \n                # Online update for a_covs\n                current_cov = manager.a_covs[module]\n                n = manager.num_samples_a[module]\n                new_n = n + projected_a.shape[0]\n                if new_n > 0:\n                    updated_cov = (current_cov.float() * n + a_cov_sample.cpu().float()) / new_n\n                    manager.a_covs[module].copy_(updated_cov)\n                    manager.num_samples_a[module] = new_n\n\n            # --- 2. Project gradients into r-space and update covariance ---\n            # g has shape (batch*seq, out_features)\n            g = grad_wrt_output.reshape(-1, grad_wrt_output.shape[-1])\n            if g.shape[0] > 0 and 'default' in module.lora_B:\n                # B has shape (out_features, r)\n                # Cast weight to the same dtype as gradient `g` to prevent mismatch\n                lora_B = module.lora_B['default'].weight.data.to(device=g.device, dtype=g.dtype, non_blocking=True)\n                # projected_g has shape (batch*seq, r)\n                projected_g = g @ lora_B\n                g_cov_sample = projected_g.T @ projected_g\n\n                # Online update for g_covs\n                current_cov = manager.g_covs[module]\n                n = manager.num_samples_g[module]\n                new_n = n + projected_g.shape[0]\n                if new_n > 0:\n                    updated_cov = (current_cov.float() * n + g_cov_sample.cpu().float()) / new_n\n                    manager.g_covs[module].copy_(updated_cov)\n                    manager.num_samples_g[module] = new_n\n\n        # We return gradients for the inputs of the `forward` method:\n        # (module, output, input)\n        # 1. module: Not a tensor, so None\n        # 2. output: The gradient is `grad_wrt_output`, pass it back to the LoraLayer\n        # 3. input: This function does not depend on `input` for its output's value,\n        #    so its gradient contribution w.r.t. `input` is zero.\n        return None, grad_wrt_output, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:59:47.107764Z","iopub.execute_input":"2025-07-30T09:59:47.108414Z","iopub.status.idle":"2025-07-30T09:59:47.118098Z","shell.execute_reply.started":"2025-07-30T09:59:47.108387Z","shell.execute_reply":"2025-07-30T09:59:47.117587Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Place this function outside your GRITManager class.\n# It only knows how to work with Tensors.\ndef jit_invert_tensor_pair(a_cov: torch.Tensor, g_cov: torch.Tensor, kfac_damping: float):\n    \"\"\"\n    JIT-compatible function to invert a SINGLE pair of covariance tensors.\n    \"\"\"\n    with torch.no_grad():\n        damping = max(kfac_damping, 1e-6)\n        \n        a_cov_damped = a_cov.float() + damping * torch.eye(a_cov.shape[0], device='cpu')\n        g_cov_damped = g_cov.float() + damping * torch.eye(g_cov.shape[0], device='cpu')\n        \n        L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n        L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n        \n        if info_a == 0 and info_g == 0:\n            a_inv = torch.cholesky_inverse(L_a).half()\n            g_inv = torch.cholesky_inverse(L_g).half()\n            return a_inv, g_inv\n        else:\n            # Return empty tensors on failure, which we can check for later\n            return torch.empty(0), torch.empty(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:59:47.119272Z","iopub.execute_input":"2025-07-30T09:59:47.119519Z","iopub.status.idle":"2025-07-30T09:59:47.140659Z","shell.execute_reply.started":"2025-07-30T09:59:47.119504Z","shell.execute_reply":"2025-07-30T09:59:47.139974Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class GRITConfig:\n    \"\"\"Configuration class for GRIT with Unsloth support.\"\"\"\n    def __init__(self):\n        # Using a 3B model, optimized for a powerful A100 GPU\n        self.model_id = \"meta-llama/Llama-3.2-3B\"  # Using Unsloth's pre-optimized model\n        \n        # -------- Training Configuration (Now Optimized for Kaggle -> 16GB VRAM) --------\n        self.batch_size = 8  # Lowered from 8 for memory constraints\n        self.gradient_accumulation_steps = 4 # Increased from 1 to maintain effective batch size of 8\n        self.num_epochs = 1\n        self.learning_rate = 2e-5\n        self.precision = \"bf16\" # bf16 is still optimal for modern GPUs\n        self.max_length = 8192\n\n        # LoRA configuration (optimized for memory)\n        self.lora_rank = 16   # Start with a higher rank to allow for pruning\n        self.lora_alpha = 32 # Adjusted for new rank (2 * rank)\n        self.lora_dropout = 0.0\n        # 5W heuristic for layer selection: include attention and key MLP layers\n        self.lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\n        # GRIT parameters (tuned for speed vs. quality)\n        self.kfac_update_freq = 50 # Invert less often\n        self.kfac_damping = 0.001\n        self.reprojection_freq = 50\n        self.reprojection_k = 8\n        self.grit_cov_update_freq = 15 # Update covs a bit more often with more budget\n        \n        # --- Rank-Adaptive LoRA Configuration ---\n        self.enable_rank_adaptation = True\n        self.rank_adaptation_threshold = 0.90  # Cumulative energy threshold\n        self.min_lora_rank = 4                 # Minimum rank to prevent collapse\n\n        # --- Convergence Control ---\n        self.enable_early_stopping = True\n        self.early_stopping_patience = 3 # Stop after 3 evaluations with no improvement\n\n        # Unsloth-specific parameters\n        self.use_unsloth = False\n        self.unsloth_max_seq_length = self.max_length\n        self.unsloth_dtype = torch.bfloat16 if self.precision == \"bf16\" else torch.float16\n        self.unsloth_load_in_4bit = True  # Use 4-bit quantization\n        \n        # Data loading configuration\n        self.dataset_name = \"ParamDev/ICD10CM_HCC\" # Default dataset\n        self.num_workers = 2 # Reduced from 8 for Kaggle's lower CPU/RAM resources\n        self.pin_memory = False # This is generally fine\n        self.drop_last = True\n\nconfig = GRITConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:59:47.141552Z","iopub.execute_input":"2025-07-30T09:59:47.141805Z","iopub.status.idle":"2025-07-30T09:59:47.166576Z","shell.execute_reply.started":"2025-07-30T09:59:47.141783Z","shell.execute_reply":"2025-07-30T09:59:47.165878Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class GRITManager:\n    \"\"\"\n    Memory-efficient GRIT Manager with CPU-based K-FAC storage and selective optimization.\n    This addresses both performance and memory issues.\n    \"\"\"\n    \n    def __init__(self, model, config, device):\n        self.model = model\n        self.config = config\n        self.device = device\n        self.global_step = 0\n        self.backward_step = 0\n        \n        # Adaptive frequency state\n        self.loss_history = []\n        self.loss_history_capacity = 20  # Corresponds to window size in _get_adaptive_freq\n        self.last_kfac_update_step = 0\n        self.last_reprojection_step = 0\n        \n        # Memory-efficient K-FAC state storage (CPU-based)\n        self.a_covs = {}  # Input activation covariances (stored on CPU)\n        self.g_covs = {}  # Output gradient covariances (stored on CPU)\n        self.a_invs = {}  # Inverted input covariances (CPU)\n        self.g_invs = {}  # Inverted output covariances (CPU)\n        \n        # Online estimation counters\n        self.num_samples_a = {}\n        self.num_samples_g = {}\n\n        # Track which modules we're optimizing (subset for memory efficiency)\n        self.optimized_modules = []\n        \n        self.factors_are_ready = False\n        self._instrument_model()\n        \n        # Debug: Check how many LoRA layers we found\n        total_modules = len(self.optimized_modules)\n        print(f\"GRITManager: Initialization complete.\")\n        print(f\"🔍 Optimizing {total_modules} key LoRA modules.\")\n        print(f\"💾 K-FAC matrices stored on CPU for memory efficiency.\")\n\n    def _instrument_model(self):\n        \"\"\"\n        Replaces forward passes with a version that includes our autograd function.\n        Optimized to only instrument q_proj modules in the last 8 layers and use r-dim\n        covariance matrices to significantly reduce CPU RAM usage.\n        \"\"\"\n        attention_modules = 0\n        \n        for name, module in self.model.named_modules():\n            if isinstance(module, LoraLayer) and module.r['default'] > 0:\n                # Check if this module is in the last 8 layers\n                is_last8 = any(f'model.layers.{i}' in name for i in range(0,32)) #(for llama 3b 20-27)\n\n                if not is_last8:\n                    continue\n\n                # All LoRA modules in last 8 layers get GRIT optimization\n                module.lora_name = name # Store name for logging\n                module.grit_manager = self \n                self.num_samples_a[module] = 0\n                self.num_samples_g[module] = 0\n                \n                # Store original forward method\n                module.original_forward = module.forward\n                \n                # Replace forward method to apply the hook *after* the original forward\n                def new_forward(self, x):\n                    y = self.original_forward(x)\n                    return KFACAutogradFunction.apply(self, y, x)\n\n                module.forward = new_forward.__get__(module, LoraLayer)\n\n                # --- store covariances in r-dim space (MASSIVE memory saving) ---\n                r = module.r['default']\n                self.a_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n                self.g_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n                \n                self.optimized_modules.append(module)\n                attention_modules += 1\n        \n        print(f\"🎯 Instrumented {attention_modules} modules for GRIT optimization with custom autograd:\")\n        print(f\"   • {attention_modules} attention modules\")\n        print(f\"🚀 Using r-dim ({self.config.lora_rank}x{self.config.lora_rank}) covariances for maximum memory efficiency.\")\n\n    def _get_adaptive_freq(self, base_freq, min_freq=1, max_freq=1000, window=20):\n        \"\"\"Calculates adaptive frequency based on loss stability.\"\"\"\n        if len(self.loss_history) < window:\n            return base_freq\n        \n        # Check loss trend over the last `window` steps\n        recent_losses = self.loss_history[-window:]\n        first_half = sum(recent_losses[:window//2]) / (window//2)\n        second_half = sum(recent_losses[window//2:]) / (window//2)\n        \n        # If loss is decreasing (learning is stable), decrease frequency (run less often)\n        if second_half < first_half * 0.99:\n            new_freq = int(base_freq * 1.5)\n        # If loss is fluctuating or increasing, increase frequency (run more often)\n        else:\n            new_freq = int(base_freq * 0.75)\n            \n        return max(min_freq, min(new_freq, max_freq))\n\n    def step(self, loss=None):\n        \"\"\"Called after each optimizer step to manage periodic updates\"\"\"\n        self.global_step += 1\n        if loss is not None:\n            self.loss_history.append(loss)\n            if len(self.loss_history) > self.loss_history_capacity:\n                self.loss_history = self.loss_history[-self.loss_history_capacity:]\n\n        # --- Second-order damping schedule ---\n        if len(self.loss_history) > 10: # Need enough history to compute variance\n            loss_variance = torch.tensor(self.loss_history).var().item()\n            # Scale damping with variance, with min/max caps\n            self.config.kfac_damping = max(1e-6, min(0.01, 0.001 + math.sqrt(loss_variance)))\n            if self.global_step % 100 == 0: # Log periodically\n                 wandb.log({\"adaptive_kfac_damping\": self.config.kfac_damping})          \n\n        # Adaptive K-FAC update frequency\n        kfac_freq = self._get_adaptive_freq(self.config.kfac_update_freq)\n        if self.global_step - self.last_kfac_update_step >= kfac_freq:\n            self.update_and_invert_factors()\n            self.last_kfac_update_step = self.global_step\n        \n        # Adaptive neural reprojection frequency\n        reproj_freq = self._get_adaptive_freq(self.config.reprojection_freq, min_freq=1, max_freq=2000)\n        if self.global_step - self.last_reprojection_step >= reproj_freq:\n            self.neural_reprojection()\n            self.last_reprojection_step = self.global_step\n\n    # Inside your GRITManager class\n    def update_and_invert_factors(self):\n        print(f\"\\nGRITManager: Inverting K-FAC factors at step {self.global_step}...\")\n\n        # JIT script the new, simpler function ONCE outside the loop\n        scripted_invert_fn = torch.jit.script(jit_invert_tensor_pair)\n\n        # The loop over modules stays in regular Python, where module keys are OK\n        for module in self.optimized_modules:\n            a_cov = self.a_covs[module]\n            g_cov = self.g_covs[module]\n\n            # Call the JIT function with TENSORS ONLY\n            a_inv, g_inv = scripted_invert_fn(\n                a_cov=a_cov,\n                g_cov=g_cov,\n                kfac_damping=self.config.kfac_damping\n            )\n\n            # Check if the inversion was successful and update the dictionaries\n            if a_inv.numel() > 0 and g_inv.numel() > 0:\n                self.a_invs[module] = a_inv\n                self.g_invs[module] = g_inv\n            else:\n                print(f\"K-FAC inversion failed for a module. Skipping.\")\n        \n        self.factors_are_ready = True\n\n    def _invert_factors_fn(self):\n        with torch.no_grad():\n            for module in self.optimized_modules:\n                # Ensure damping is non-negative\n                damping = max(self.config.kfac_damping, 1e-6)\n                \n                # --- Invert Activation Covariance (a_cov) ---\n                a_cov_damped = self.a_covs[module].float() + damping * torch.eye(\n                    self.a_covs[module].shape[0], device='cpu'\n                )\n                \n                # Use cholesky_ex to check for positive-definiteness (a proxy for invertibility here)\n                # L_a is the Cholesky factor, info_a is 0 on success\n                L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n                \n                # --- Invert Gradient Covariance (g_cov) ---\n                g_cov_damped = self.g_covs[module].float() + damping * torch.eye(\n                    self.g_covs[module].shape[0], device='cpu'\n                )\n                L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n\n                # Check if both decompositions succeeded\n                if info_a == 0 and info_g == 0:\n                    # If successful, compute inverse from Cholesky factor (more stable)\n                    self.a_invs[module] = torch.cholesky_inverse(L_a).half()\n                    self.g_invs[module] = torch.cholesky_inverse(L_g).half()\n                else:\n                    # This block runs if inversion would have failed\n                    print(f\"K-FAC inversion check failed for a module. Skipping update.\")\n                    # You can still log this event if needed, but wandb calls might not be JIT-friendly.\n                    # It's better to handle logging outside the JIT-compiled function.\n                    continue\n\n    def precondition_gradients(self):\n        \"\"\"Apply K-FAC preconditioning with efficient CPU-GPU transfers\"\"\"\n        if not self.factors_are_ready:\n            return\n        if self.global_step % self.config.kfac_update_freq == 0:\n            print(f\"\\nGRITManager: Applying Natural Gradient preconditioner at step {self.global_step} to {self.global_step + 50}...\")\n            \n        with torch.no_grad():\n            # Determine correct dtype based on config\n            dtype = torch.float16 if self.config.precision == \"fp16\" else torch.bfloat16\n\n            for module in self.optimized_modules:\n                if module not in self.a_invs or module not in self.g_invs:\n                    continue\n                    \n                lora_a = module.lora_A['default']\n                lora_b = module.lora_B['default']\n                \n                if (lora_a is None or lora_b is None or \n                    lora_a.weight.grad is None or lora_b.weight.grad is None):\n                    continue\n\n                try:\n                    # Move inverse matrices to GPU only when needed, using correct dtype\n                    a_inv = self.a_invs[module].to(self.device, dtype=dtype)\n                    g_inv = self.g_invs[module].to(self.device, dtype=dtype)\n                    \n                    # --- True Natural Gradient Computation: grad' = G_inv @ grad @ A_inv ---\n                    \n                    # Precondition LoRA B gradient (the \"easy\" part)\n                    # Original grad_b has shape (out, r)\n                    grad_b = lora_b.weight.grad.to(dtype)\n                    # Corrected multiplication: (out, r) @ (r, r) -> (out, r)\n                    preconditioned_b_grad = grad_b @ g_inv\n                    \n                    # Precondition LoRA A gradient (the \"hard\" part)\n                    # Original grad_a has shape (r, in)\n                    grad_a = lora_a.weight.grad.to(dtype)\n                    \n                    # Safety reshape for larger ranks\n                    r = module.r['default']\n                    if r > 0:\n                        grad_a = grad_a.view(r, -1)\n                    \n                    # Instead, we apply the preconditioning to each matrix's gradient.\n                    # grad_a' = A_inv @ grad_a\n                    # Corrected multiplication: (r, r) @ (r, in) -> (r, in)\n                    preconditioned_a_grad = a_inv @ grad_a\n\n                    # Copy back the preconditioned gradients\n                    lora_a.weight.grad.copy_(preconditioned_a_grad.to(lora_a.weight.grad.dtype))\n                    lora_b.weight.grad.copy_(preconditioned_b_grad.to(lora_b.weight.grad.dtype))\n                    \n                    # Clean up GPU tensors immediately\n                    del a_inv, g_inv, grad_a, grad_b, preconditioned_a_grad, preconditioned_b_grad\n                    \n                except Exception as e:\n                    print(f\"Gradient preconditioning failed: {e}\")\n                    wandb.log({\"preconditioning_error\": 1})\n                    continue\n\n    def neural_reprojection(self):\n        \"\"\"Perform neural reprojection and log the parameter reduction.\"\"\"\n        print(f\"\\nGRITManager: Neural reprojection at step {self.global_step}...\")\n        \n        initial_params = 0\n        final_params = 0\n        log_dict = {}\n\n        with torch.no_grad():\n            # First, calculate initial effective parameters for all GRIT-optimized modules\n            for module in self.optimized_modules:\n                if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n                    # Effective parameters in LoRA = r * (in_features + out_features)\n                    initial_params += module.r['default'] * (module.in_features + module.out_features)\n\n            for module in self.optimized_modules:\n                try:\n                    lora_a = module.lora_A['default']\n                    lora_b = module.lora_B['default']\n                    \n                    if lora_a is None or lora_b is None:\n                        continue\n                        \n                    A = lora_a.weight.data.float()\n                    B = lora_b.weight.data.float()\n                    \n                    r = A.shape[0]  # Current LoRA rank\n                    k = r # Initialize k to the current rank as a fallback\n                    \n                    # Form the r x r covariance matrix M = A @ A.T\n                    M = A @ A.T\n                    \n                    # --- Numerical Stability Check ---\n                    if torch.isnan(M).any() or torch.isinf(M).any():\n                        print(f\"WARNING: Covariance matrix M for {module.lora_name} contains NaN/Inf. Skipping reprojection for this module.\")\n                        log_dict[f\"reprojection_errors/nan_inf/{module.lora_name}\"] = 1\n                        # If skipped, the rank does not change.\n                        final_params += r * (module.in_features + module.out_features)\n                        continue\n\n                    # --- Rank Adaptation & Pruning Logic ---\n                    if self.config.enable_rank_adaptation and r > self.config.min_lora_rank:\n                        # --- Full Eigendecomposition for Adaptive Rank ---\n                        try:\n                            eigenvals, V = torch.linalg.eigh(M)\n                        except Exception as e:\n                            print(f\"Eigendecomposition failed for module: {e}\", \"error\")\n                            log_dict[\"reprojection_errors/eigh_error\"] = log_dict.get(\"reprojection_errors/eigh_error\", 0) + 1\n                            final_params += r * (module.in_features + module.out_features)\n                            continue\n                        \n                        sorted_indices = torch.argsort(eigenvals, descending=True)\n                        sorted_eigenvals = eigenvals[sorted_indices]\n                        V = V[:, sorted_indices]\n                        \n                        # Log the sorted eigenvalue distribution to show energy decay\n                        # --- Enhanced WandB Logging for Eigen-spectra ---\n                        # --- Log Data Tables for Manual Plotting ---\n                        if self.global_step % self.config.reprojection_freq == 0:\n                            try:\n                                # 1. Log Eigen value spectra\n                                eigen_spectra_list = sorted_eigenvals.cpu().numpy().tolist()\n                                data_for_table = [[s] for s in eigen_spectra_list]\n                                table = wandb.Table(data=data_for_table, columns=[\"eigenvalue\"])\n                                hist_plot = wandb.plot.histogram(table, \"eigenvalue\", title=f\"Eigenvalue Spectrum - {module.lora_name}\")\n                                log_dict[f\"Charts/Eigenvalue Spectrum/{module.lora_name}\"] = hist_plot\n                                \n\n                                # 2. Log Eigenvector data as a Table for the heatmap\n                                V_k_T = V[:, :k].T.cpu().numpy()\n                                heatmap_data = []\n                                for x, eigenvector in enumerate(V_k_T):\n                                    for y, component in enumerate(eigenvector):\n                                        heatmap_data.append([x, y, component])\n\n                                heatmap_table = wandb.Table(\n                                    data=heatmap_data,\n                                    columns=[\"eigenvector_idx\", \"component_idx\", \"value\"]\n                                )\n                                log_dict[f\"Data_Tables/Eigenvectors/{module.lora_name}\"] = heatmap_table\n\n                            except Exception as e:\n                                print(f\"Data logging for {module.lora_name} failed: {e}\")\n\n                        total_energy = torch.sum(sorted_eigenvals)\n                        if total_energy > 1e-6:\n                            cumulative_energy = torch.cumsum(sorted_eigenvals, dim=0) / total_energy\n                            k = (cumulative_energy < self.config.rank_adaptation_threshold).sum().item() + 1\n                        \n                        k = max(k, self.config.min_lora_rank) # Enforce minimum rank\n                        rank_reduction_percent = (1 - k / r) * 100 if r > 0 else 0\n                        \n                        log_dict[f\"GRIT/Effective Rank (k)/{module.lora_name}\"] = k\n                        log_dict[f\"GRIT/Rank Reduction (%)/{module.lora_name}\"] = rank_reduction_percent\n                        \n                        V_k = V[:, :k]\n                    else:\n                        # --- Fixed Rank Reprojection ---\n                        k = min(self.config.reprojection_k, r)\n                        try:\n                            if k < r and k > 0:\n                                _, V_k = torch.lobpcg(lambda v: M @ v, X=torch.randn(r, k, device=M.device, dtype=M.dtype), k=k, largest=True)\n                            else:\n                                _, V = torch.linalg.eigh(M)\n                                eigenvals, _ = torch.linalg.eigh(M)\n                                V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n                        except Exception as e:\n                            print(f\"LOBPCG failed for module, falling back to eigh: {e}\")\n                            _, V = torch.linalg.eigh(M)\n                            eigenvals, _ = torch.linalg.eigh(M)\n                            V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n                    \n                    # Accumulate the new effective parameter count for this module\n                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n                        final_params += k * (module.in_features + module.out_features)\n\n                    # --- Project onto the top-k eigenvectors and prune via zeroing ---\n                    A_proj = V_k.T @ A\n                    B_proj = B @ V_k\n\n                    A_new, B_new = torch.zeros_like(A), torch.zeros_like(B)\n                    A_new[:k, :], B_new[:, :k] = A_proj, B_proj\n\n                    lora_a.weight.data.copy_(A_new.to(A.dtype))\n                    lora_b.weight.data.copy_(B_new.to(B.dtype))\n                        \n                except Exception as e:\n                    print(f\"Neural reprojection failed for module {module.lora_name}: {e}\")\n                    log_dict[\"reprojection_errors/general_error\"] = log_dict.get(\"reprojection_errors/general_error\", 0) + 1\n                    # On failure, assume rank does not change for this module\n                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n                        final_params += module.r['default'] * (module.in_features + module.out_features)\n                    continue\n\n        # --- Log the final results ---\n        if initial_params > 0:\n            param_reduction = initial_params - final_params\n            reduction_percent = (param_reduction / initial_params) * 100\n            print(f\"✅ Neural reprojection completed. Effective parameter count reduced.\")\n            print(f\"   - Initial GRIT params: {initial_params:,}\")\n            print(f\"   - Final GRIT params:   {final_params:,}\")\n            print(f\"   - Reduction:           {param_reduction:,} ({reduction_percent:.2f}%)\")\n            \n            log_dict.update({\n                \"Parameters/GRIT Initial Params\": initial_params,\n                \"Parameters/GRIT Final Params\": final_params,\n                \"Parameters/GRIT Param Reduction (%)\": reduction_percent,\n            })\n        else:\n            print(f\"✅ Neural reprojection completed for {len(self.optimized_modules)} modules.\")\n        \n        if log_dict:\n            wandb.log(log_dict, step=self.global_step)\n        \n        # Strategic memory cleanup after expensive operations\n        gc.collect()\n        torch.cuda.synchronize()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:59:47.263115Z","iopub.execute_input":"2025-07-30T09:59:47.263647Z","iopub.status.idle":"2025-07-30T09:59:47.298778Z","shell.execute_reply.started":"2025-07-30T09:59:47.263622Z","shell.execute_reply":"2025-07-30T09:59:47.298183Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from torch.optim import Optimizer\n\nclass GritOptimizer(Optimizer):\n    \"\"\"\n    A wrapper for a PyTorch optimizer that applies GRIT's K-FAC gradient\n    preconditioning before the actual optimization step. This ensures that the\n    optimizer uses the natural gradient instead of the standard gradient.\n    \"\"\"\n    def __init__(self, optimizer: Optimizer, grit_manager: 'GRITManager'):\n        self.optimizer = optimizer\n        self.grit_manager = grit_manager\n\n    @property\n    def state(self):\n        return self.optimizer.state\n\n    @property\n    def param_groups(self):\n        return self.optimizer.param_groups\n\n    @param_groups.setter\n    def param_groups(self, value):\n        self.optimizer.param_groups = value\n\n    def step(self, closure=None):\n        \"\"\"\n        Performs a single optimization step.\n\n        1.  Applies K-FAC preconditioning to the accumulated gradients.\n        2.  Calls the underlying optimizer's step function.\n        \"\"\"\n        # Apply K-FAC preconditioning to the accumulated gradients\n        if self.grit_manager.factors_are_ready:\n            self.grit_manager.precondition_gradients()\n\n        # Call the underlying optimizer's step function\n        self.optimizer.step(closure)\n\n    def zero_grad(self, set_to_none: bool = False):\n        self.optimizer.zero_grad(set_to_none=set_to_none)\n\n    def add_param_group(self, param_group: dict):\n        self.optimizer.add_param_group(param_group)\n\n    def state_dict(self):\n        return self.optimizer.state_dict()\n\n    def load_state_dict(self, state_dict: dict):\n        self.optimizer.load_state_dict(state_dict)\n\n    def __repr__(self):\n        return f\"GritOptimizer({self.optimizer.__repr__()})\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:59:47.300017Z","iopub.execute_input":"2025-07-30T09:59:47.300436Z","iopub.status.idle":"2025-07-30T09:59:47.329374Z","shell.execute_reply.started":"2025-07-30T09:59:47.300414Z","shell.execute_reply":"2025-07-30T09:59:47.328822Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import TrainerCallback\n\nclass GritCallback(TrainerCallback):\n    \"\"\"\n    This callback injects GRIT's logic into the training loop.\n    \"\"\"\n    def __init__(self, grit_manager):\n        self.grit_manager = grit_manager\n\n    def on_step_end(self, args, state, control, **kwargs):\n        \"\"\"\n        Triggered at the end of each training step.\n        \"\"\"\n        last_loss = state.log_history[-1].get(\"loss\") if state.log_history else None\n        self.grit_manager.step(loss=last_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:59:47.329883Z","iopub.execute_input":"2025-07-30T09:59:47.330048Z","iopub.status.idle":"2025-07-30T09:59:47.355779Z","shell.execute_reply.started":"2025-07-30T09:59:47.330035Z","shell.execute_reply":"2025-07-30T09:59:47.355233Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class GritTrainer(Seq2SeqTrainer):\n    \"\"\"\n    Optimized GRIT Trainer that addresses the core performance issues and\n    correctly applies gradient preconditioning by wrapping the optimizer.\n    \"\"\"\n\n    def __init__(self, grit_manager, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.grit_manager = grit_manager\n        print(\"GritTrainer: Initialized with GRIT implementation.\")\n\n    def create_optimizer_and_scheduler(self, num_training_steps: int):\n        \"\"\"\n        Overrides the base method to wrap the created optimizer with our\n        GritOptimizer, which handles gradient preconditioning.\n        \"\"\"\n        super().create_optimizer_and_scheduler(num_training_steps)\n\n        if self.optimizer is not None:\n            print(\"🎁 Wrapping the optimizer with GRIT preconditioning logic.\")\n            self.optimizer = GritOptimizer(self.optimizer, self.grit_manager)\n\n    # --- THIS IS THE CORRECTED METHOD ---\n    def training_step(self, model, inputs, num_items_in_batch=None):\n        \"\"\"\n        The training step's signature is now aligned with the parent class.\n        \"\"\"\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        with self.compute_loss_context_manager():\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.n_gpu > 1:\n            loss = loss.mean()\n\n        self.accelerator.backward(loss)\n\n        return loss.detach() / self.args.gradient_accumulation_steps\n        \n    def evaluate(self, *args, **kwargs):\n        \"\"\"Overrides the default evaluate method to add aggressive memory cleanup.\"\"\"\n        print(\"\\n🧹 Clearing VRAM before evaluation...\")\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        return super().evaluate(*args, **kwargs)\n\n    def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n\n        has_labels = \"labels\" in inputs\n        if has_labels:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n\n        _, generated_tokens, _ = super().prediction_step(\n            model, inputs, prediction_loss_only, ignore_keys\n        )\n\n        if has_labels:\n            inputs[\"labels\"] = labels\n\n        loss = None\n        if has_labels:\n            with torch.no_grad():\n                loss = self.compute_loss(model, inputs.copy()).detach()\n\n        if generated_tokens is not None and type(generated_tokens).__name__ == 'EmptyLogits':\n            batch_size = inputs[\"input_ids\"].shape[0]\n            seq_length = labels.shape[1] if labels is not None else inputs[\"input_ids\"].shape[1]\n            vocab_size = self.model.config.vocab_size\n\n            generated_tokens = torch.zeros(\n                (batch_size, seq_length, vocab_size),\n                device=self.accelerator.device,\n                dtype=config.unsloth_dtype\n            )\n\n        if labels is not None:\n            if len(labels.shape) == 3:\n                labels = torch.argmax(labels, dim=-1)\n            elif len(labels.shape) == 1:\n                batch_size = inputs[\"input_ids\"].shape[0]\n                labels = labels.view(batch_size, -1)\n\n        return loss, generated_tokens, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:59:47.408693Z","iopub.execute_input":"2025-07-30T09:59:47.409193Z","iopub.status.idle":"2025-07-30T09:59:47.419398Z","shell.execute_reply.started":"2025-07-30T09:59:47.409166Z","shell.execute_reply":"2025-07-30T09:59:47.418786Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"if config.use_unsloth:\n    from unsloth import FastLanguageModel\n    \n    # Load 4bit prequantized model from Unsloth\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = config.model_id,\n        max_seq_length = config.unsloth_max_seq_length,\n        dtype = config.unsloth_dtype,\n        load_in_4bit = config.unsloth_load_in_4bit,\n    )\n    \n    # --- Dynamically select target modules for the proposed experiment ---\n    # This ensures LoRA is applied ONLY to the layers we will optimize with GRIT,\n    # directly reducing the total number of trainable parameters.\n    # The Llama-3.2-3B model has 32 layers. We target the last 8.\n    # Build target modules list for ONLY the last 8 layers (layers 24-31)\n    target_modules = []\n    for i in range(0, 32):  # Last 8 layers only(for llama 3b 20-27)\n        for module in config.lora_target_modules:\n            if \"proj\" in module:  # attention modules\n                target_modules.append(f\"model.layers.{i}.self_attn.{module}\")\n            else:  # MLP modules  \n                target_modules.append(f\"model.layers.{i}.mlp.{module}\")\n    \n    # Prepare model for LoRA training with Unsloth's optimized implementation\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r = config.lora_rank,\n        target_modules = target_modules, # Use the dynamically generated list\n        lora_alpha = config.lora_alpha,\n        lora_dropout = config.lora_dropout,\n        bias = \"none\",\n        use_gradient_checkpointing = True,\n        random_state = 42,\n        use_rslora = True,\n    )\nelse:\n    # Your original QLoRA loading code\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16 if config.precision == \"fp16\" else torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(config.model_id, use_fast=False)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    model = AutoModelForCausalLM.from_pretrained(\n        config.model_id,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n    )\n\n    target_modules = []\n    for i in range(0, 32):  # Last 8 layers only(for llama 3b 20-27)\n        for module in config.lora_target_modules:\n            if \"proj\" in module:  # attention modules\n                target_modules.append(f\"model.layers.{i}.self_attn.{module}\")\n            else:  # MLP modules  \n                target_modules.append(f\"model.layers.{i}.mlp.{module}\")\n                \n    model = prepare_model_for_kbit_training(model)\n    lora_config = LoraConfig(\n        r=config.lora_rank,\n        lora_alpha=config.lora_alpha,\n        target_modules=target_modules,\n        lora_dropout=config.lora_dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        inference_mode=False,\n        use_rslora=True,\n    )\n    model = get_peft_model(model, lora_config)\n\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T09:59:47.420714Z","iopub.execute_input":"2025-07-30T09:59:47.420954Z","iopub.status.idle":"2025-07-30T10:00:47.844977Z","shell.execute_reply.started":"2025-07-30T09:59:47.420938Z","shell.execute_reply":"2025-07-30T10:00:47.844218Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11fd88bfa7764e7880221fe077542653"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f02b39f0c974c88a1794125426fef8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f27cbde176be48e2b3444309994136da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5983e08c768542efbd71c2f4beb8c8de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81cd0582a5934b06b5d6548a2ff12fb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d6c8a2d6b5b48cdb222fcb1cae763e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75dc73dbbd7a42e8a379f8afc5a1fdc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734ce358f36c4626aa252f041e811310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa159a1f640345998c92e6bee8e78f8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ea55cd581f6456abc95d7d1931a0326"}},"metadata":{}},{"name":"stdout","text":"trainable params: 9,175,040 || all params: 3,221,924,864 || trainable%: 0.2848\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ---------------- Dataset Preparation ----------------\n\nfull_dataset = load_dataset(config.dataset_name, split=\"train\")\n\n# Use only 50% of the total data\n#reduced_dataset = full_dataset.select(range(len(full_dataset) // 2))\n\ndataset = full_dataset.train_test_split(test_size=0.1, seed=42)\n\ntrain_dataset = dataset[\"train\"]\nval_dataset = dataset[\"test\"]\n\n# Reduce the validation set to a small, representative sample for faster evaluation\n#val_dataset = val_dataset.select(range(64))\n#print(f\"📉 Using a reduced validation set of {len(val_dataset)} samples for speed.\")\n\ndef tokenize(example):\n    instr = example['instruction'].strip()\n    inp = example['input'].strip()\n    out = example['output'].strip()\n    if inp:\n        prompt = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"\n    else:\n        prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{out}\"\n    # This is a placeholder to mention that for the clinical ICD-10 task,\n    # a different dataset and preprocessing would be required.\n    # e.g., load_dataset(\"some/clinical_dataset\") and format accordingly.\n    tok = tokenizer(\n        prompt,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=config.max_length,\n        return_tensors=None,\n    )\n    labels = tok[\"input_ids\"][:]\n    tok[\"labels\"] = labels\n    return tok\n\ntokenized_train_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\ntokenized_val_dataset = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)\n\n# Compute total training steps\ntrain_len = len(tokenized_train_dataset)\neff_batch = config.batch_size * config.gradient_accumulation_steps\ntotal_steps = math.ceil(train_len / eff_batch) * config.num_epochs\nprint(f\"🏁 Total training steps: {total_steps}\")\n\nrun_name = f\"grit-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}\"\n\nimport wandb\n\n# Start a new wandb run to track this script.\nrun = wandb.init(\n    # Set the wandb project where this run will be logged.\n    entity=\"RAAPID\",\n    project=\"GRIT-Final\",\n    name=run_name,\n    job_type=\"training\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:00:47.845649Z","iopub.execute_input":"2025-07-30T10:00:47.845896Z","iopub.status.idle":"2025-07-30T10:03:27.703036Z","shell.execute_reply.started":"2025-07-30T10:00:47.845874Z","shell.execute_reply":"2025-07-30T10:03:27.701773Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be6858695274d918e2802dcaafcd5ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ICD10CM_HCC.zip:   0%|          | 0.00/11.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30a9f1c1a4c4e28b38bc6e9165221dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9960 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"595c5af8637444ea9fa1eebd288ab976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8964 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b321b0ee994e9897c566d4747aa706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/996 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f07e9f7d8743108c4c85db4ad2e04c"}},"metadata":{}},{"name":"stdout","text":"🏁 Total training steps: 281\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2747481094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Start a new wandb run to track this script.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m run = wandb.init(\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Set the wandb project where this run will be logged.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RAAPID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \u001b[0;31m# Need to build delay into this sentry capture because our exit hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0;31m# mess with sentry's ability to send out errors before the program ends.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# this will messily add this \"reraise\" function to the stack trace,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# but hopefully it's not too bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_safe_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1604\u001b[0m                 \u001b[0minit_telemetry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_side_derived_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_printer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, settings, config, run_printer)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mProtobufErrorHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCommError\u001b[0m: Error uploading run: returned error 403: {\"data\":{\"upsertBucket\":null},\"errors\":[{\"message\":\"permission denied\",\"path\":[\"upsertBucket\"],\"extensions\":{\"code\":\"PERMISSION_ERROR\"}}]}"],"ename":"CommError","evalue":"Error uploading run: returned error 403: {\"data\":{\"upsertBucket\":null},\"errors\":[{\"message\":\"permission denied\",\"path\":[\"upsertBucket\"],\"extensions\":{\"code\":\"PERMISSION_ERROR\"}}]}","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"# ---------------- Training Arguments ----------------\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    run_name=run_name,\n\n    per_device_train_batch_size=config.batch_size,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=config.gradient_accumulation_steps,\n    num_train_epochs=config.num_epochs,\n    learning_rate=config.learning_rate,\n\n    #eval_strategy=\"steps\",\n    #eval_steps=100,                 # Evaluate less often\n    logging_steps=50,\n    eval_accumulation_steps=1,\n    #max_steps=200,\n\n    save_strategy=\"epoch\",\n    #save_steps=250,                 # Match eval_steps for early stopping\n    save_total_limit=2,\n\n    #fp16=config.precision == \"fp16\",\n    bf16=config.precision == \"bf16\",\n    gradient_checkpointing=True,\n    dataloader_num_workers=config.num_workers,\n    dataloader_pin_memory=config.pin_memory,\n    remove_unused_columns=False,\n\n    report_to=\"wandb\",\n    #metric_for_best_model=\"bleu\",\n    #greater_is_better=True,\n    predict_with_generate=True,\n    \n    # --- Generation settings for faster evaluation ---\n    generation_max_length=config.max_length + 128,\n    generation_num_beams=1,\n\n    # --- Early Stopping ---\n    #load_best_model_at_end=config.enable_early_stopping,\n\n    optim=\"paged_adamw_8bit\"\n)\n\ntorch.cuda.empty_cache()\ntorch.cuda.synchronize()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"🚀 Initializing GRITManager on device:\", device)\ngrit_manager = GRITManager(model, config, device)\n\n# Instantiate the new callback\ngrit_callback = GritCallback(grit_manager)\n\n# Let the Trainer use its default high-performance optimizer\n# The custom 8-bit optimizer is not needed when memory is abundant","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:07:39.843107Z","iopub.execute_input":"2025-07-30T10:07:39.843417Z","iopub.status.idle":"2025-07-30T10:07:39.970601Z","shell.execute_reply.started":"2025-07-30T10:07:39.843394Z","shell.execute_reply":"2025-07-30T10:07:39.969896Z"}},"outputs":[{"name":"stdout","text":"🚀 Initializing GRITManager on device: cuda\n🎯 Instrumented 112 modules for GRIT optimization with custom autograd:\n   • 112 attention modules\n🚀 Using r-dim (16x16) covariances for maximum memory efficiency.\nGRITManager: Initialization complete.\n🔍 Optimizing 112 key LoRA modules.\n💾 K-FAC matrices stored on CPU for memory efficiency.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def count_parameters(model):\n    \"\"\"Count and display parameter statistics\"\"\"\n    total_params = 0\n    trainable_params = 0\n    lora_params = 0\n    \n    layer_params = {}  # Track params by layer\n    \n    for name, param in model.named_parameters():\n        total_params += param.numel()\n        \n        if param.requires_grad:\n            trainable_params += param.numel()\n            if \"lora_\" in name:\n                lora_params += param.numel()\n                \n                # Extract layer number for statistics\n                for i in range(32):\n                    if f'layers.{i}.' in name:\n                        if i not in layer_params:\n                            layer_params[i] = 0\n                        layer_params[i] += param.numel()\n                        break\n    \n    print(f\"\\n📊 Parameter Statistics:\")\n    print(f\"🔢 Total parameters: {total_params:,}\")\n    print(f\"🔥 Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.3f}%)\")\n    print(f\"📐 LoRA parameters: {lora_params:,}\")\n    \n    print(f\"\\n📍 Layer-wise LoRA Distribution:\")\n    active_layers = sorted(layer_params.keys())\n    for layer_id in active_layers:\n        print(f\"   Layer {layer_id}: {layer_params[layer_id]:,} params\")\n    \n    if active_layers:\n        print(f\"\\n🎯 Strategy: LoRA + GRIT applied ONLY to layers {min(active_layers)}-{max(active_layers)}\")\n        print(f\"🚫 Layers 0-{min(active_layers)-1}: Completely untouched (frozen by default)\")\n\n    return total_params, trainable_params\n\n# Call this after model setup\ntotal_params, trainable_params = count_parameters(model)\nif wandb.run:\n    wandb.config.update({\n        \"total_model_params\": total_params,\n        \"lora_trainable_params\": trainable_params,\n        \"initial_lora_rank_r\": config.lora_rank,\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:07:50.399944Z","iopub.execute_input":"2025-07-30T10:07:50.400732Z","iopub.status.idle":"2025-07-30T10:07:50.429058Z","shell.execute_reply.started":"2025-07-30T10:07:50.400701Z","shell.execute_reply":"2025-07-30T10:07:50.428456Z"}},"outputs":[{"name":"stdout","text":"\n📊 Parameter Statistics:\n🔢 Total parameters: 1,812,638,720\n🔥 Trainable parameters: 9,175,040 (0.506%)\n📐 LoRA parameters: 9,175,040\n\n📍 Layer-wise LoRA Distribution:\n   Layer 0: 327,680 params\n   Layer 1: 327,680 params\n   Layer 2: 327,680 params\n   Layer 3: 327,680 params\n   Layer 4: 327,680 params\n   Layer 5: 327,680 params\n   Layer 6: 327,680 params\n   Layer 7: 327,680 params\n   Layer 8: 327,680 params\n   Layer 9: 327,680 params\n   Layer 10: 327,680 params\n   Layer 11: 327,680 params\n   Layer 12: 327,680 params\n   Layer 13: 327,680 params\n   Layer 14: 327,680 params\n   Layer 15: 327,680 params\n   Layer 16: 327,680 params\n   Layer 17: 327,680 params\n   Layer 18: 327,680 params\n   Layer 19: 327,680 params\n   Layer 20: 327,680 params\n   Layer 21: 327,680 params\n   Layer 22: 327,680 params\n   Layer 23: 327,680 params\n   Layer 24: 327,680 params\n   Layer 25: 327,680 params\n   Layer 26: 327,680 params\n   Layer 27: 327,680 params\n\n🎯 Strategy: LoRA + GRIT applied ONLY to layers 0-27\n🚫 Layers 0--1: Completely untouched (frozen by default)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Trainer uses GRITManager for K-FAC steps\ntrainer = GritTrainer(\n    grit_manager=grit_manager,\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    tokenizer=tokenizer,\n    callbacks=[grit_callback],\n    #compute_metrics=compute_metrics,\n    #callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)] if config.enable_early_stopping else [],\n)\n\n# Final memory cleanup\nimport gc\nfor _ in range(3):\n    gc.collect()\n    torch.cuda.empty_cache()\ntorch.cuda.synchronize()\n\nprint(f\"🎯 Effective batch (per step): {config.batch_size * config.gradient_accumulation_steps}\")\nprint(\"⚡ Mixed precision:\", config.precision)\nprint(\"📏 Max sequence length:\", config.max_length)\nprint(f\"💾 Checkpoints will be saved every {training_args.save_steps} steps.\")\nprint(\"🔧 LoRA rank:\", config.lora_rank)\nprint(\"🚀 Starting training now...\")\n\ntrainer.train()\nwandb.finish()\n\nprint(\"🎉 Training completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:07:50.430105Z","iopub.execute_input":"2025-07-30T10:07:50.430532Z","iopub.status.idle":"2025-07-30T10:07:55.604697Z","shell.execute_reply.started":"2025-07-30T10:07:50.430515Z","shell.execute_reply":"2025-07-30T10:07:55.602750Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1386230625.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `GritTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"GritTrainer: Initialized with GRIT implementation.\n🎯 Effective batch (per step): 32\n⚡ Mixed precision: bf16\n📏 Max sequence length: 8192\n💾 Checkpoints will be saved every 500 steps.\n🔧 LoRA rank: 16\n🚀 Starting training now...\n🎁 Wrapping the optimizer with GRIT preconditioning logic.\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/273651177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Starting training now...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n","\u001b[0;32m/tmp/ipykernel_36/1386230625.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1758\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    689\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 300.12 MiB is free. Process 2938 has 14.45 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 69.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 300.12 MiB is free. Process 2938 has 14.45 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 69.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"# === Hugging Face Upload ===\nfrom huggingface_hub import create_repo, upload_folder, HfApi\n\nprint(\"\\n🚀 Uploading fine-tuned model to Hugging Face Hub...\")\n\n# === Hugging Face Upload ===\nprint(\"\\n🚀 Uploading fine-tuned model to Hugging Face Hub...\")\n\nHF_MODEL_NAME = f\"grit-lora-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}\"\nHF_USERNAME = \"Rockerleo\" # Replace with your Hugging Face username\nFULL_MODEL_NAME = f\"{HF_USERNAME}/GRIT-Full-MIMIC-llama-3.2-3B-Energy-0.9\"\n\ntry:\n    # Save model and tokenizer locally first\n    output_dir = \"./grit_trained_model\"\n    model.save_pretrained(output_dir, safe_serialization=True)\n    tokenizer.save_pretrained(output_dir)\n\n    # Create/update model card (with hardware and hyperparams)\n    model_card_content = f\"\"\"---\ntags:\n- llama\n- alpaca\n- grit\n- lora\n- qlora\n- unsloth\n- instruction-tuning\n- fine-tuned\nbase_model: {config.model_id}\nlibrary_name: peft\nlicense: apache-2.0\ndatasets:\n- {config.dataset_name}\nlanguage:\n- en\npipeline_tag: text-generation\n---\n\n# {config.model_id} Fine-tuned with GRIT and QLoRA (Unsloth)\n\nThis model is a fine-tuned version of [{config.model_id}](https://huggingface.co/{config.model_id}) using the **GRIT** (Geometric Reprojection Instruction Tuning) algorithm and **QLoRA** on the [{config.dataset_name} dataset](https://huggingface.co/datasets/{config.dataset_name}).\n\nThe base model is quantized to 4-bit (NF4) and optimized with [Unsloth](https://github.com/unslothai/unsloth) to enable efficient fine-tuning.\n\n## 🚀 Training Details\n\n### GRIT Algorithm\n- **K-FAC Updates**: Every {config.kfac_update_freq} steps (adaptive) for second-order preconditioning.\n- **Neural Reprojection**: Every {config.reprojection_freq} steps (adaptive) for rank optimization.\n- **Rank Adaptation**: {'Enabled' if config.enable_rank_adaptation else 'Disabled'} (Threshold: {config.rank_adaptation_threshold}, Min Rank: {config.min_lora_rank}).\n- **Optimized LoRA Modules**: {config.lora_target_modules}\n\n### Fine-tuning Configuration\n- **Base Model**: {config.model_id}\n- **Quantization**: 4-bit (NF4) with {config.precision} compute.\n- **LoRA Rank**: {config.lora_rank}\n- **LoRA Alpha**: {config.lora_alpha}\n- **Batch Size**: {config.batch_size} (per device)\n- **Gradient Accumulation**: {config.gradient_accumulation_steps} (Effective batch = {config.batch_size * config.gradient_accumulation_steps})\n- **Learning Rate**: {config.learning_rate:.1e}\n- **Precision**: {config.precision} mixed precision\n- **Sequence Length**: {config.max_length} tokens\n- **Gradient Checkpointing**: Enabled\n\n### Performance Improvements\n- ✅ **Faster Convergence**: K-FAC preconditioning aligns updates with curvature.\n- ✅ **Memory-Efficient**: 4-bit quantization (QLoRA) and gradient checkpointing used.\n- ✅ **Adaptive Rank**: Dynamically prunes LoRA rank to improve parameter efficiency.\n\n## 📊 Training Metrics\n- **Total Steps**: {trainer.state.global_step if 'trainer' in locals() else 'N/A'}\n- **Final Loss**: {trainer.state.log_history[-1].get('train_loss', 'N/A') if 'trainer' in locals() and trainer.state.log_history else 'N/A'}\n- **Trainable Params**: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\n\n## 📝 Algorithm Details\n- **K-FAC Preconditioning** (Natural Gradient) and **Neural Reprojection** as per GRIT method.\n- **Memory Efficient**: Covariance matrices on CPU to reduce GPU load.\n\n## 🏆 Results\nIn benchmark comparisons, GRIT has shown **faster convergence and better stability** than standard LoRA or fine-tuning, making it well-suited for efficient single-epoch training. The use of Unsloth further accelerates this process.\n\n## 📝 Citation\nIf you use this model, please cite the original GRIT paper and:\n```bibtex\n@misc{{grit-lora-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}}},\n  title={{ {config.model_id} Fine-tuned with GRIT on {config.dataset_name} }},\n  author={{{HF_USERNAME}}},\n  year={{2024}},\n  publisher={{Hugging Face}},\n  url={{https://huggingface.co/{FULL_MODEL_NAME}}}\n}}\n```\n\n## ⚖️ License\nThis model inherits the Apache 2.0 license.\n\"\"\"\n    with open(f\"{output_dir}/README.md\", \"w\") as f:\n        f.write(model_card_content)\n    print(\"📝 Model card created with training details.\")\n\n    # Create repository on the Hub and upload the folder\n    print(f\"🌐 Uploading model to https://huggingface.co/{FULL_MODEL_NAME} ...\")\n    create_repo(FULL_MODEL_NAME, exist_ok=True)\n    api = HfApi()\n    api.upload_folder(\n        folder_path=output_dir,\n        repo_id=FULL_MODEL_NAME,\n        commit_message=f\"GRIT fine-tuned {config.model_id.split('/')[-1]} on {config.dataset_name.split('/')[-1]}\"\n    )\n    print(f\"✅ Model successfully uploaded: https://huggingface.co/{FULL_MODEL_NAME}\")\n\nexcept Exception as e:\n    print(f\"❌ Upload failed: {e}\\\\n(Model saved locally in {output_dir})\")\n\nprint(\"🎉 GRIT fine-tuning script completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:07:55.605825Z","iopub.status.idle":"2025-07-30T10:07:55.606077Z","shell.execute_reply.started":"2025-07-30T10:07:55.605971Z","shell.execute_reply":"2025-07-30T10:07:55.605980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}