{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchao torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable the tokenizer parallelism warning to avoid spam\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    get_scheduler,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import wandb\n",
    "import bitsandbytes as bnb\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from huggingface_hub import login, HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a43c54efcd4123a1169726cc63521f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFACAutogradFunction(torch.autograd.Function):\n",
    "    \"\"\"Custom autograd function to capture activations and gradients for K-FAC.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, module, output, input):\n",
    "        ctx.module = module\n",
    "        # We only need the input for the backward pass to compute activation stats\n",
    "        ctx.save_for_backward(input.detach())\n",
    "        # We pass the output through, it's what the next layer will see\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_wrt_output):\n",
    "        # This grad is the gradient w.r.t the output of the LoraLayer\n",
    "        module = ctx.module\n",
    "        input, = ctx.saved_tensors # This is the original input to the LoraLayer\n",
    "        manager = module.grit_manager\n",
    "        manager.backward_step += 1\n",
    "\n",
    "        if not module.training:\n",
    "            # Pass gradients through without modification if not training\n",
    "            return None, grad_wrt_output, None\n",
    "            \n",
    "        # --- Run covariance updates periodically to avoid bottlenecking ---\n",
    "        if manager.backward_step % manager.config.grit_cov_update_freq != 0:\n",
    "            return None, grad_wrt_output, None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # --- 1. Project activations into r-space and update covariance ---\n",
    "            # a has shape (batch*seq, in_features)\n",
    "            a = input.reshape(-1, input.shape[-1])\n",
    "            if a.shape[0] > 0 and 'default' in module.lora_A:\n",
    "                # A has shape (r, in_features), so A.T has (in_features, r)\n",
    "                # Cast weight to the same dtype as activation `a` to prevent mismatch\n",
    "                lora_A_T = module.lora_A['default'].weight.data.T.to(device=a.device, dtype=a.dtype, non_blocking=True)\n",
    "                # projected_a has shape (batch*seq, r)\n",
    "                projected_a = a @ lora_A_T\n",
    "                a_cov_sample = projected_a.T @ projected_a\n",
    "                \n",
    "                # Online update for a_covs\n",
    "                current_cov = manager.a_covs[module]\n",
    "                n = manager.num_samples_a[module]\n",
    "                new_n = n + projected_a.shape[0]\n",
    "                if new_n > 0:\n",
    "                    updated_cov = (current_cov.float() * n + a_cov_sample.cpu().float()) / new_n\n",
    "                    manager.a_covs[module].copy_(updated_cov)\n",
    "                    manager.num_samples_a[module] = new_n\n",
    "\n",
    "            # --- 2. Project gradients into r-space and update covariance ---\n",
    "            # g has shape (batch*seq, out_features)\n",
    "            g = grad_wrt_output.reshape(-1, grad_wrt_output.shape[-1])\n",
    "            if g.shape[0] > 0 and 'default' in module.lora_B:\n",
    "                # B has shape (out_features, r)\n",
    "                # Cast weight to the same dtype as gradient `g` to prevent mismatch\n",
    "                lora_B = module.lora_B['default'].weight.data.to(device=g.device, dtype=g.dtype, non_blocking=True)\n",
    "                # projected_g has shape (batch*seq, r)\n",
    "                projected_g = g @ lora_B\n",
    "                g_cov_sample = projected_g.T @ projected_g\n",
    "\n",
    "                # Online update for g_covs\n",
    "                current_cov = manager.g_covs[module]\n",
    "                n = manager.num_samples_g[module]\n",
    "                new_n = n + projected_g.shape[0]\n",
    "                if new_n > 0:\n",
    "                    updated_cov = (current_cov.float() * n + g_cov_sample.cpu().float()) / new_n\n",
    "                    manager.g_covs[module].copy_(updated_cov)\n",
    "                    manager.num_samples_g[module] = new_n\n",
    "\n",
    "        # We return gradients for the inputs of the `forward` method:\n",
    "        # (module, output, input)\n",
    "        # 1. module: Not a tensor, so None\n",
    "        # 2. output: The gradient is `grad_wrt_output`, pass it back to the LoraLayer\n",
    "        # 3. input: This function does not depend on `input` for its output's value,\n",
    "        #    so its gradient contribution w.r.t. `input` is zero.\n",
    "        return None, grad_wrt_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this function outside your GRITManager class.\n",
    "# It only knows how to work with Tensors.\n",
    "def jit_invert_tensor_pair(a_cov: torch.Tensor, g_cov: torch.Tensor, kfac_damping: float):\n",
    "    \"\"\"\n",
    "    JIT-compatible function to invert a SINGLE pair of covariance tensors.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        damping = max(kfac_damping, 1e-6)\n",
    "        \n",
    "        a_cov_damped = a_cov.float() + damping * torch.eye(a_cov.shape[0], device='cpu')\n",
    "        g_cov_damped = g_cov.float() + damping * torch.eye(g_cov.shape[0], device='cpu')\n",
    "        \n",
    "        L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n",
    "        L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n",
    "        \n",
    "        if info_a == 0 and info_g == 0:\n",
    "            a_inv = torch.cholesky_inverse(L_a).half()\n",
    "            g_inv = torch.cholesky_inverse(L_g).half()\n",
    "            return a_inv, g_inv\n",
    "        else:\n",
    "            # Return empty tensors on failure, which we can check for later\n",
    "            return torch.empty(0), torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRITConfig:\n",
    "    \"\"\"Configuration class for GRIT with Unsloth support.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Using a 3B model, optimized for a powerful A100 GPU\n",
    "        self.model_id = \"meta-llama/Llama-3.2-3B\"  # Using Unsloth's pre-optimized model\n",
    "        \n",
    "        # -------- Training Configuration (Now Optimized for Kaggle -> 16GB VRAM) --------\n",
    "        self.batch_size = 8  # Lowered from 8 for memory constraints\n",
    "        self.gradient_accumulation_steps = 4 # Increased from 1 to maintain effective batch size of 8\n",
    "        self.num_epochs = 1\n",
    "        self.learning_rate = 2e-5\n",
    "        self.precision = \"bf16\" # bf16 is still optimal for modern GPUs\n",
    "        self.max_length = 1024\n",
    "\n",
    "        # LoRA configuration (optimized for memory)\n",
    "        self.lora_rank = 16   # Start with a higher rank to allow for pruning\n",
    "        self.lora_alpha = 32 # Adjusted for new rank (2 * rank)\n",
    "        self.lora_dropout = 0.0\n",
    "        # 5W heuristic for layer selection: include attention and key MLP layers\n",
    "        self.lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "        # GRIT parameters (tuned for speed vs. quality)\n",
    "        self.kfac_update_freq = 50 # Invert less often\n",
    "        self.kfac_damping = 0.001\n",
    "        self.reprojection_freq = 50\n",
    "        self.reprojection_k = 8\n",
    "        self.grit_cov_update_freq = 15 # Update covs a bit more often with more budget\n",
    "        \n",
    "        # --- Rank-Adaptive LoRA Configuration ---\n",
    "        self.enable_rank_adaptation = True\n",
    "        self.rank_adaptation_threshold = 0.90  # Cumulative energy threshold\n",
    "        self.min_lora_rank = 4                 # Minimum rank to prevent collapse\n",
    "\n",
    "        # --- Convergence Control ---\n",
    "        self.enable_early_stopping = True\n",
    "        self.early_stopping_patience = 3 # Stop after 3 evaluations with no improvement\n",
    "\n",
    "        # Unsloth-specific parameters\n",
    "        self.use_unsloth = False\n",
    "        self.unsloth_max_seq_length = self.max_length\n",
    "        self.unsloth_dtype = torch.bfloat16 if self.precision == \"bf16\" else torch.float16\n",
    "        self.unsloth_load_in_4bit = True  # Use 4-bit quantization\n",
    "        \n",
    "        # Data loading configuration\n",
    "        self.dataset_name = \"tatsu-lab/alpaca\" # Default dataset\n",
    "        self.num_workers = 2 # Reduced from 8 for Kaggle's lower CPU/RAM resources\n",
    "        self.pin_memory = False # This is generally fine\n",
    "        self.drop_last = True\n",
    "\n",
    "config = GRITConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRITManager:\n",
    "    \"\"\"\n",
    "    Memory-efficient GRIT Manager with CPU-based K-FAC storage and selective optimization.\n",
    "    This addresses both performance and memory issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.global_step = 0\n",
    "        self.backward_step = 0\n",
    "        \n",
    "        # Adaptive frequency state\n",
    "        self.loss_history = []\n",
    "        self.loss_history_capacity = 20  # Corresponds to window size in _get_adaptive_freq\n",
    "        self.last_kfac_update_step = 0\n",
    "        self.last_reprojection_step = 0\n",
    "        \n",
    "        # Memory-efficient K-FAC state storage (CPU-based)\n",
    "        self.a_covs = {}  # Input activation covariances (stored on CPU)\n",
    "        self.g_covs = {}  # Output gradient covariances (stored on CPU)\n",
    "        self.a_invs = {}  # Inverted input covariances (CPU)\n",
    "        self.g_invs = {}  # Inverted output covariances (CPU)\n",
    "        \n",
    "        # Online estimation counters\n",
    "        self.num_samples_a = {}\n",
    "        self.num_samples_g = {}\n",
    "\n",
    "        # Track which modules we're optimizing (subset for memory efficiency)\n",
    "        self.optimized_modules = []\n",
    "        \n",
    "        self.factors_are_ready = False\n",
    "        self._instrument_model()\n",
    "        \n",
    "        # Debug: Check how many LoRA layers we found\n",
    "        total_modules = len(self.optimized_modules)\n",
    "        print(f\"GRITManager: Initialization complete.\")\n",
    "        print(f\"🔍 Optimizing {total_modules} key LoRA modules.\")\n",
    "        print(f\"💾 K-FAC matrices stored on CPU for memory efficiency.\")\n",
    "\n",
    "    def _instrument_model(self):\n",
    "        \"\"\"\n",
    "        Replaces forward passes with a version that includes our autograd function.\n",
    "        Optimized to only instrument q_proj modules in the last 8 layers and use r-dim\n",
    "        covariance matrices to significantly reduce CPU RAM usage.\n",
    "        \"\"\"\n",
    "        attention_modules = 0\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, LoraLayer) and module.r['default'] > 0:\n",
    "                # Check if this module is in the last 8 layers\n",
    "                is_last8 = any(f'model.layers.{i}' in name for i in range(0,32)) #(for llama 3b 20-27)\n",
    "\n",
    "                if not is_last8:\n",
    "                    continue\n",
    "\n",
    "                # All LoRA modules in last 8 layers get GRIT optimization\n",
    "                module.lora_name = name # Store name for logging\n",
    "                module.grit_manager = self \n",
    "                self.num_samples_a[module] = 0\n",
    "                self.num_samples_g[module] = 0\n",
    "                \n",
    "                # Store original forward method\n",
    "                module.original_forward = module.forward\n",
    "                \n",
    "                # Replace forward method to apply the hook *after* the original forward\n",
    "                def new_forward(self, x):\n",
    "                    y = self.original_forward(x)\n",
    "                    return KFACAutogradFunction.apply(self, y, x)\n",
    "\n",
    "                module.forward = new_forward.__get__(module, LoraLayer)\n",
    "\n",
    "                # --- store covariances in r-dim space (MASSIVE memory saving) ---\n",
    "                r = module.r['default']\n",
    "                self.a_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n",
    "                self.g_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n",
    "                \n",
    "                self.optimized_modules.append(module)\n",
    "                attention_modules += 1\n",
    "        \n",
    "        print(f\"🎯 Instrumented {attention_modules} modules for GRIT optimization with custom autograd:\")\n",
    "        print(f\"   • {attention_modules} attention modules\")\n",
    "        print(f\"🚀 Using r-dim ({self.config.lora_rank}x{self.config.lora_rank}) covariances for maximum memory efficiency.\")\n",
    "\n",
    "    def _get_adaptive_freq(self, base_freq, min_freq=1, max_freq=1000, window=20):\n",
    "        \"\"\"Calculates adaptive frequency based on loss stability.\"\"\"\n",
    "        if len(self.loss_history) < window:\n",
    "            return base_freq\n",
    "        \n",
    "        # Check loss trend over the last `window` steps\n",
    "        recent_losses = self.loss_history[-window:]\n",
    "        first_half = sum(recent_losses[:window//2]) / (window//2)\n",
    "        second_half = sum(recent_losses[window//2:]) / (window//2)\n",
    "        \n",
    "        # If loss is decreasing (learning is stable), decrease frequency (run less often)\n",
    "        if second_half < first_half * 0.99:\n",
    "            new_freq = int(base_freq * 1.5)\n",
    "        # If loss is fluctuating or increasing, increase frequency (run more often)\n",
    "        else:\n",
    "            new_freq = int(base_freq * 0.75)\n",
    "            \n",
    "        return max(min_freq, min(new_freq, max_freq))\n",
    "\n",
    "    def step(self, loss=None):\n",
    "        \"\"\"Called after each optimizer step to manage periodic updates\"\"\"\n",
    "        self.global_step += 1\n",
    "        if loss is not None:\n",
    "            self.loss_history.append(loss)\n",
    "            if len(self.loss_history) > self.loss_history_capacity:\n",
    "                self.loss_history = self.loss_history[-self.loss_history_capacity:]\n",
    "\n",
    "        # --- Second-order damping schedule ---\n",
    "        if len(self.loss_history) > 10: # Need enough history to compute variance\n",
    "            loss_variance = torch.tensor(self.loss_history).var().item()\n",
    "            # Scale damping with variance, with min/max caps\n",
    "            self.config.kfac_damping = max(1e-6, min(0.01, 0.001 + math.sqrt(loss_variance)))\n",
    "            if self.global_step % 100 == 0: # Log periodically\n",
    "                 wandb.log({\"adaptive_kfac_damping\": self.config.kfac_damping})\n",
    "\n",
    "        # Adaptive K-FAC update frequency\n",
    "        kfac_freq = self._get_adaptive_freq(self.config.kfac_update_freq)\n",
    "        if self.global_step - self.last_kfac_update_step >= kfac_freq:\n",
    "            self.update_and_invert_factors()\n",
    "            self.last_kfac_update_step = self.global_step\n",
    "        \n",
    "        # Adaptive neural reprojection frequency\n",
    "        reproj_freq = self._get_adaptive_freq(self.config.reprojection_freq, min_freq=1, max_freq=2000)\n",
    "        if self.global_step - self.last_reprojection_step >= reproj_freq:\n",
    "            self.neural_reprojection()\n",
    "            self.last_reprojection_step = self.global_step\n",
    "\n",
    "    # Inside your GRITManager class\n",
    "    def update_and_invert_factors(self):\n",
    "        print(f\"\\nGRITManager: Inverting K-FAC factors at step {self.global_step}...\")\n",
    "\n",
    "        # JIT script the new, simpler function ONCE outside the loop\n",
    "        scripted_invert_fn = torch.jit.script(jit_invert_tensor_pair)\n",
    "\n",
    "        # The loop over modules stays in regular Python, where module keys are OK\n",
    "        for module in self.optimized_modules:\n",
    "            a_cov = self.a_covs[module]\n",
    "            g_cov = self.g_covs[module]\n",
    "\n",
    "            # Call the JIT function with TENSORS ONLY\n",
    "            a_inv, g_inv = scripted_invert_fn(\n",
    "                a_cov=a_cov,\n",
    "                g_cov=g_cov,\n",
    "                kfac_damping=self.config.kfac_damping\n",
    "            )\n",
    "\n",
    "            # Check if the inversion was successful and update the dictionaries\n",
    "            if a_inv.numel() > 0 and g_inv.numel() > 0:\n",
    "                self.a_invs[module] = a_inv\n",
    "                self.g_invs[module] = g_inv\n",
    "            else:\n",
    "                print(f\"K-FAC inversion failed for a module. Skipping.\")\n",
    "        \n",
    "        self.factors_are_ready = True\n",
    "\n",
    "    def _invert_factors_fn(self):\n",
    "        with torch.no_grad():\n",
    "            for module in self.optimized_modules:\n",
    "                # Ensure damping is non-negative\n",
    "                damping = max(self.config.kfac_damping, 1e-6)\n",
    "                \n",
    "                # --- Invert Activation Covariance (a_cov) ---\n",
    "                a_cov_damped = self.a_covs[module].float() + damping * torch.eye(\n",
    "                    self.a_covs[module].shape[0], device='cpu'\n",
    "                )\n",
    "                \n",
    "                # Use cholesky_ex to check for positive-definiteness (a proxy for invertibility here)\n",
    "                # L_a is the Cholesky factor, info_a is 0 on success\n",
    "                L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n",
    "                \n",
    "                # --- Invert Gradient Covariance (g_cov) ---\n",
    "                g_cov_damped = self.g_covs[module].float() + damping * torch.eye(\n",
    "                    self.g_covs[module].shape[0], device='cpu'\n",
    "                )\n",
    "                L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n",
    "\n",
    "                # Check if both decompositions succeeded\n",
    "                if info_a == 0 and info_g == 0:\n",
    "                    # If successful, compute inverse from Cholesky factor (more stable)\n",
    "                    self.a_invs[module] = torch.cholesky_inverse(L_a).half()\n",
    "                    self.g_invs[module] = torch.cholesky_inverse(L_g).half()\n",
    "                else:\n",
    "                    # This block runs if inversion would have failed\n",
    "                    print(f\"K-FAC inversion check failed for a module. Skipping update.\")\n",
    "                    # You can still log this event if needed, but wandb calls might not be JIT-friendly.\n",
    "                    # It's better to handle logging outside the JIT-compiled function.\n",
    "                    continue\n",
    "\n",
    "    def precondition_gradients(self):\n",
    "        \"\"\"Apply K-FAC preconditioning with efficient CPU-GPU transfers\"\"\"\n",
    "        if not self.factors_are_ready:\n",
    "            return\n",
    "        if self.global_step % self.config.kfac_update_freq == 0:\n",
    "            print(f\"\\nGRITManager: Applying Natural Gradient preconditioner at step {self.global_step} to {self.global_step + 50}...\")\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # Determine correct dtype based on config\n",
    "            dtype = torch.float16 if self.config.precision == \"fp16\" else torch.bfloat16\n",
    "\n",
    "            for module in self.optimized_modules:\n",
    "                if module not in self.a_invs or module not in self.g_invs:\n",
    "                    continue\n",
    "                    \n",
    "                lora_a = module.lora_A['default']\n",
    "                lora_b = module.lora_B['default']\n",
    "                \n",
    "                if (lora_a is None or lora_b is None or \n",
    "                    lora_a.weight.grad is None or lora_b.weight.grad is None):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Move inverse matrices to GPU only when needed, using correct dtype\n",
    "                    a_inv = self.a_invs[module].to(self.device, dtype=dtype)\n",
    "                    g_inv = self.g_invs[module].to(self.device, dtype=dtype)\n",
    "                    \n",
    "                    # --- True Natural Gradient Computation: grad' = G_inv @ grad @ A_inv ---\n",
    "                    \n",
    "                    # Precondition LoRA B gradient (the \"easy\" part)\n",
    "                    # Original grad_b has shape (out, r)\n",
    "                    grad_b = lora_b.weight.grad.to(dtype)\n",
    "                    # Corrected multiplication: (out, r) @ (r, r) -> (out, r)\n",
    "                    preconditioned_b_grad = grad_b @ g_inv\n",
    "                    \n",
    "                    # Precondition LoRA A gradient (the \"hard\" part)\n",
    "                    # Original grad_a has shape (r, in)\n",
    "                    grad_a = lora_a.weight.grad.to(dtype)\n",
    "                    \n",
    "                    # Safety reshape for larger ranks\n",
    "                    r = module.r['default']\n",
    "                    if r > 0:\n",
    "                        grad_a = grad_a.view(r, -1)\n",
    "                    \n",
    "                    # Instead, we apply the preconditioning to each matrix's gradient.\n",
    "                    # grad_a' = A_inv @ grad_a\n",
    "                    # Corrected multiplication: (r, r) @ (r, in) -> (r, in)\n",
    "                    preconditioned_a_grad = a_inv @ grad_a\n",
    "\n",
    "                    # Copy back the preconditioned gradients\n",
    "                    lora_a.weight.grad.copy_(preconditioned_a_grad.to(lora_a.weight.grad.dtype))\n",
    "                    lora_b.weight.grad.copy_(preconditioned_b_grad.to(lora_b.weight.grad.dtype))\n",
    "                    \n",
    "                    # Clean up GPU tensors immediately\n",
    "                    del a_inv, g_inv, grad_a, grad_b, preconditioned_a_grad, preconditioned_b_grad\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Gradient preconditioning failed: {e}\")\n",
    "                    wandb.log({\"preconditioning_error\": 1})\n",
    "                    continue\n",
    "\n",
    "    def neural_reprojection(self):\n",
    "        \"\"\"Perform neural reprojection and log the parameter reduction.\"\"\"\n",
    "        print(f\"\\nGRITManager: Neural reprojection at step {self.global_step}...\")\n",
    "        \n",
    "        initial_params = 0\n",
    "        final_params = 0\n",
    "        log_dict = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # First, calculate initial effective parameters for all GRIT-optimized modules\n",
    "            for module in self.optimized_modules:\n",
    "                if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                    # Effective parameters in LoRA = r * (in_features + out_features)\n",
    "                    initial_params += module.r['default'] * (module.in_features + module.out_features)\n",
    "\n",
    "            for module in self.optimized_modules:\n",
    "                try:\n",
    "                    lora_a = module.lora_A['default']\n",
    "                    lora_b = module.lora_B['default']\n",
    "                    \n",
    "                    if lora_a is None or lora_b is None:\n",
    "                        continue\n",
    "                        \n",
    "                    A = lora_a.weight.data.float()\n",
    "                    B = lora_b.weight.data.float()\n",
    "                    \n",
    "                    r = A.shape[0]  # Current LoRA rank\n",
    "                    k = r # Initialize k to the current rank as a fallback\n",
    "                    \n",
    "                    # Form the r x r covariance matrix M = A @ A.T\n",
    "                    M = A @ A.T\n",
    "                    \n",
    "                    # --- Numerical Stability Check ---\n",
    "                    if torch.isnan(M).any() or torch.isinf(M).any():\n",
    "                        print(f\"WARNING: Covariance matrix M for {module.lora_name} contains NaN/Inf. Skipping reprojection for this module.\")\n",
    "                        log_dict[f\"reprojection_errors/nan_inf/{module.lora_name}\"] = 1\n",
    "                        # If skipped, the rank does not change.\n",
    "                        final_params += r * (module.in_features + module.out_features)\n",
    "                        continue\n",
    "\n",
    "                    # --- Rank Adaptation & Pruning Logic ---\n",
    "                    if self.config.enable_rank_adaptation and r > self.config.min_lora_rank:\n",
    "                        # --- Full Eigendecomposition for Adaptive Rank ---\n",
    "                        try:\n",
    "                            eigenvals, V = torch.linalg.eigh(M)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Eigendecomposition failed for module: {e}\", \"error\")\n",
    "                            log_dict[\"reprojection_errors/eigh_error\"] = log_dict.get(\"reprojection_errors/eigh_error\", 0) + 1\n",
    "                            final_params += r * (module.in_features + module.out_features)\n",
    "                            continue\n",
    "                        \n",
    "                        sorted_indices = torch.argsort(eigenvals, descending=True)\n",
    "                        sorted_eigenvals = eigenvals[sorted_indices]\n",
    "                        V = V[:, sorted_indices]\n",
    "                        \n",
    "                        # Log the sorted eigenvalue distribution to show energy decay\n",
    "                        # --- Enhanced WandB Logging for Eigen-spectra ---\n",
    "                        # --- Log Data Tables for Manual Plotting ---\n",
    "                        if self.global_step % self.config.reprojection_freq == 0:\n",
    "                            try:\n",
    "                                # 1. Log Eigen value spectra\n",
    "                                eigen_spectra_list = sorted_eigenvals.cpu().numpy().tolist()\n",
    "                                data_for_table = [[s] for s in eigen_spectra_list]\n",
    "                                table = wandb.Table(data=data_for_table, columns=[\"eigenvalue\"])\n",
    "                                hist_plot = wandb.plot.histogram(table, \"eigenvalue\", title=f\"Eigenvalue Spectrum - {module.lora_name}\")\n",
    "                                log_dict[f\"Charts/Eigenvalue Spectrum/{module.lora_name}\"] = hist_plot\n",
    "                                \n",
    "\n",
    "                                # 2. Log Eigenvector data as a Table for the heatmap\n",
    "                                V_k_T = V[:, :k].T.cpu().numpy()\n",
    "                                heatmap_data = []\n",
    "                                for x, eigenvector in enumerate(V_k_T):\n",
    "                                    for y, component in enumerate(eigenvector):\n",
    "                                        heatmap_data.append([x, y, component])\n",
    "\n",
    "                                heatmap_table = wandb.Table(\n",
    "                                    data=heatmap_data,\n",
    "                                    columns=[\"eigenvector_idx\", \"component_idx\", \"value\"]\n",
    "                                )\n",
    "                                log_dict[f\"Data_Tables/Eigenvectors/{module.lora_name}\"] = heatmap_table\n",
    "\n",
    "                            except Exception as e:\n",
    "                                print(f\"Data logging for {module.lora_name} failed: {e}\")\n",
    "\n",
    "                        total_energy = torch.sum(sorted_eigenvals)\n",
    "                        if total_energy > 1e-6:\n",
    "                            cumulative_energy = torch.cumsum(sorted_eigenvals, dim=0) / total_energy\n",
    "                            k = (cumulative_energy < self.config.rank_adaptation_threshold).sum().item() + 1\n",
    "                        \n",
    "                        k = max(k, self.config.min_lora_rank) # Enforce minimum rank\n",
    "                        rank_reduction_percent = (1 - k / r) * 100 if r > 0 else 0\n",
    "                        \n",
    "                        log_dict[f\"GRIT/Effective Rank (k)/{module.lora_name}\"] = k\n",
    "                        log_dict[f\"GRIT/Rank Reduction (%)/{module.lora_name}\"] = rank_reduction_percent\n",
    "                        \n",
    "                        V_k = V[:, :k]\n",
    "                    else:\n",
    "                        # --- Fixed Rank Reprojection ---\n",
    "                        k = min(self.config.reprojection_k, r)\n",
    "                        try:\n",
    "                            if k < r and k > 0:\n",
    "                                _, V_k = torch.lobpcg(lambda v: M @ v, X=torch.randn(r, k, device=M.device, dtype=M.dtype), k=k, largest=True)\n",
    "                            else:\n",
    "                                _, V = torch.linalg.eigh(M)\n",
    "                                eigenvals, _ = torch.linalg.eigh(M)\n",
    "                                V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n",
    "                        except Exception as e:\n",
    "                            print(f\"LOBPCG failed for module, falling back to eigh: {e}\")\n",
    "                            _, V = torch.linalg.eigh(M)\n",
    "                            eigenvals, _ = torch.linalg.eigh(M)\n",
    "                            V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n",
    "                    \n",
    "                    # Accumulate the new effective parameter count for this module\n",
    "                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                        final_params += k * (module.in_features + module.out_features)\n",
    "\n",
    "                    # --- Project onto the top-k eigenvectors and prune via zeroing ---\n",
    "                    A_proj = V_k.T @ A\n",
    "                    B_proj = B @ V_k\n",
    "\n",
    "                    A_new, B_new = torch.zeros_like(A), torch.zeros_like(B)\n",
    "                    A_new[:k, :], B_new[:, :k] = A_proj, B_proj\n",
    "\n",
    "                    lora_a.weight.data.copy_(A_new.to(A.dtype))\n",
    "                    lora_b.weight.data.copy_(B_new.to(B.dtype))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Neural reprojection failed for module {module.lora_name}: {e}\")\n",
    "                    log_dict[\"reprojection_errors/general_error\"] = log_dict.get(\"reprojection_errors/general_error\", 0) + 1\n",
    "                    # On failure, assume rank does not change for this module\n",
    "                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                        final_params += module.r['default'] * (module.in_features + module.out_features)\n",
    "                    continue\n",
    "\n",
    "        # --- Log the final results ---\n",
    "        if initial_params > 0:\n",
    "            param_reduction = initial_params - final_params\n",
    "            reduction_percent = (param_reduction / initial_params) * 100\n",
    "            print(f\"✅ Neural reprojection completed. Effective parameter count reduced.\")\n",
    "            print(f\"   - Initial GRIT params: {initial_params:,}\")\n",
    "            print(f\"   - Final GRIT params:   {final_params:,}\")\n",
    "            print(f\"   - Reduction:           {param_reduction:,} ({reduction_percent:.2f}%)\")\n",
    "            \n",
    "            log_dict.update({\n",
    "                \"Parameters/GRIT Initial Params\": initial_params,\n",
    "                \"Parameters/GRIT Final Params\": final_params,\n",
    "                \"Parameters/GRIT Param Reduction (%)\": reduction_percent,\n",
    "            })\n",
    "        else:\n",
    "            print(f\"✅ Neural reprojection completed for {len(self.optimized_modules)} modules.\")\n",
    "        \n",
    "        if log_dict:\n",
    "            wandb.log(log_dict, step=self.global_step)\n",
    "        \n",
    "        # Strategic memory cleanup after expensive operations\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class GritOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    A wrapper for a PyTorch optimizer that applies GRIT's K-FAC gradient\n",
    "    preconditioning before the actual optimization step. This ensures that the\n",
    "    optimizer uses the natural gradient instead of the standard gradient.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer: Optimizer, grit_manager: 'GRITManager'):\n",
    "        self.optimizer = optimizer\n",
    "        self.grit_manager = grit_manager\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.optimizer.state\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "\n",
    "    @param_groups.setter\n",
    "    def param_groups(self, value):\n",
    "        self.optimizer.param_groups = value\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        1.  Applies K-FAC preconditioning to the accumulated gradients.\n",
    "        2.  Calls the underlying optimizer's step function.\n",
    "        \"\"\"\n",
    "        # Apply K-FAC preconditioning to the accumulated gradients\n",
    "        if self.grit_manager.factors_are_ready:\n",
    "            self.grit_manager.precondition_gradients()\n",
    "\n",
    "        # Call the underlying optimizer's step function\n",
    "        self.optimizer.step(closure)\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        self.optimizer.zero_grad(set_to_none=set_to_none)\n",
    "\n",
    "    def add_param_group(self, param_group: dict):\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"GritOptimizer({self.optimizer.__repr__()})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GritCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    This callback injects GRIT's logic into the training loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, grit_manager):\n",
    "        self.grit_manager = grit_manager\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Triggered at the end of each training step.\n",
    "        \"\"\"\n",
    "        last_loss = state.log_history[-1].get(\"loss\") if state.log_history else None\n",
    "        self.grit_manager.step(loss=last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GritTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"\n",
    "    Optimized GRIT Trainer that addresses the core performance issues and\n",
    "    correctly applies gradient preconditioning by wrapping the optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grit_manager, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.grit_manager = grit_manager\n",
    "        print(\"GritTrainer: Initialized with GRIT implementation.\")\n",
    "\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        \"\"\"\n",
    "        Overrides the base method to wrap the created optimizer with our\n",
    "        GritOptimizer, which handles gradient preconditioning.\n",
    "        \"\"\"\n",
    "        super().create_optimizer_and_scheduler(num_training_steps)\n",
    "\n",
    "        if self.optimizer is not None:\n",
    "            print(\"🎁 Wrapping the optimizer with GRIT preconditioning logic.\")\n",
    "            self.optimizer = GritOptimizer(self.optimizer, self.grit_manager)\n",
    "\n",
    "    # --- THIS IS THE CORRECTED METHOD ---\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        The training step's signature is now aligned with the parent class.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "        \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        \"\"\"Overrides the default evaluate method to add aggressive memory cleanup.\"\"\"\n",
    "        print(\"\\n🧹 Clearing VRAM before evaluation...\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return super().evaluate(*args, **kwargs)\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "\n",
    "        has_labels = \"labels\" in inputs\n",
    "        if has_labels:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        _, generated_tokens, _ = super().prediction_step(\n",
    "            model, inputs, prediction_loss_only, ignore_keys\n",
    "        )\n",
    "\n",
    "        if has_labels:\n",
    "            inputs[\"labels\"] = labels\n",
    "\n",
    "        loss = None\n",
    "        if has_labels:\n",
    "            with torch.no_grad():\n",
    "                loss = self.compute_loss(model, inputs.copy()).detach()\n",
    "\n",
    "        if generated_tokens is not None and type(generated_tokens).__name__ == 'EmptyLogits':\n",
    "            batch_size = inputs[\"input_ids\"].shape[0]\n",
    "            seq_length = labels.shape[1] if labels is not None else inputs[\"input_ids\"].shape[1]\n",
    "            vocab_size = self.model.config.vocab_size\n",
    "\n",
    "            generated_tokens = torch.zeros(\n",
    "                (batch_size, seq_length, vocab_size),\n",
    "                device=self.accelerator.device,\n",
    "                dtype=config.unsloth_dtype\n",
    "            )\n",
    "\n",
    "        if labels is not None:\n",
    "            if len(labels.shape) == 3:\n",
    "                labels = torch.argmax(labels, dim=-1)\n",
    "            elif len(labels.shape) == 1:\n",
    "                batch_size = inputs[\"input_ids\"].shape[0]\n",
    "                labels = labels.view(batch_size, -1)\n",
    "\n",
    "        return loss, generated_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5510c6087c4ab38cdd6d4d7a851013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,175,040 || all params: 3,221,924,864 || trainable%: 0.2848\n"
     ]
    }
   ],
   "source": [
    "if config.use_unsloth:\n",
    "    from unsloth import FastLanguageModel\n",
    "    \n",
    "    # Load 4bit prequantized model from Unsloth\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = config.model_id,\n",
    "        max_seq_length = config.unsloth_max_seq_length,\n",
    "        dtype = config.unsloth_dtype,\n",
    "        load_in_4bit = config.unsloth_load_in_4bit,\n",
    "    )\n",
    "    \n",
    "    # --- Dynamically select target modules for the proposed experiment ---\n",
    "    # This ensures LoRA is applied ONLY to the layers we will optimize with GRIT,\n",
    "    # directly reducing the total number of trainable parameters.\n",
    "    # The Llama-3.2-3B model has 32 layers. We target the last 8.\n",
    "    # Build target modules list for ONLY the last 8 layers (layers 24-31)\n",
    "    target_modules = []\n",
    "    for i in range(0, 32):  # Last 8 layers only(for llama 3b 20-27)\n",
    "        for module in config.lora_target_modules:\n",
    "            if \"proj\" in module:  # attention modules\n",
    "                target_modules.append(f\"model.layers.{i}.self_attn.{module}\")\n",
    "            else:  # MLP modules  \n",
    "                target_modules.append(f\"model.layers.{i}.mlp.{module}\")\n",
    "    \n",
    "    # Prepare model for LoRA training with Unsloth's optimized implementation\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = config.lora_rank,\n",
    "        target_modules = target_modules, # Use the dynamically generated list\n",
    "        lora_alpha = config.lora_alpha,\n",
    "        lora_dropout = config.lora_dropout,\n",
    "        bias = \"none\",\n",
    "        use_gradient_checkpointing = True,\n",
    "        random_state = 42,\n",
    "        use_rslora = True,\n",
    "    )\n",
    "else:\n",
    "    # Your original QLoRA loading code\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16 if config.precision == \"fp16\" else torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_id, use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    target_modules = []\n",
    "    for i in range(0, 32):  # Last 8 layers only(for llama 3b 20-27)\n",
    "        for module in config.lora_target_modules:\n",
    "            if \"proj\" in module:  # attention modules\n",
    "                target_modules.append(f\"model.layers.{i}.self_attn.{module}\")\n",
    "            else:  # MLP modules  \n",
    "                target_modules.append(f\"model.layers.{i}.mlp.{module}\")\n",
    "                \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.lora_rank,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        inference_mode=False,\n",
    "        use_rslora=True,\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed603ce95f04f51b20eff9352b633f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏁 Total training steps: 1463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchandrav\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250724_173724-ctztw2jj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/RAAPID/GRIT-Final/runs/ctztw2jj' target=\"_blank\">grit-Llama-3.2-3B-alpaca</a></strong> to <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/RAAPID/GRIT-Final/runs/ctztw2jj' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final/runs/ctztw2jj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- Dataset Preparation ----------------\n",
    "\n",
    "full_dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "\n",
    "# Use only 50% of the total data\n",
    "#reduced_dataset = full_dataset.select(range(len(full_dataset) // 2))\n",
    "\n",
    "dataset = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "# Reduce the validation set to a small, representative sample for faster evaluation\n",
    "#val_dataset = val_dataset.select(range(64))\n",
    "#print(f\"📉 Using a reduced validation set of {len(val_dataset)} samples for speed.\")\n",
    "\n",
    "def tokenize(example):\n",
    "    instr = example['instruction'].strip()\n",
    "    inp = example['input'].strip()\n",
    "    out = example['output'].strip()\n",
    "    if inp:\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{out}\"\n",
    "    # This is a placeholder to mention that for the clinical ICD-10 task,\n",
    "    # a different dataset and preprocessing would be required.\n",
    "    # e.g., load_dataset(\"some/clinical_dataset\") and format accordingly.\n",
    "    tok = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=config.max_length,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    labels = tok[\"input_ids\"][:]\n",
    "    tok[\"labels\"] = labels\n",
    "    return tok\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)\n",
    "\n",
    "# Compute total training steps\n",
    "train_len = len(tokenized_train_dataset)\n",
    "eff_batch = config.batch_size * config.gradient_accumulation_steps\n",
    "total_steps = math.ceil(train_len / eff_batch) * config.num_epochs\n",
    "print(f\"🏁 Total training steps: {total_steps}\")\n",
    "\n",
    "run_name = f\"grit-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    entity=\"RAAPID\",\n",
    "    project=\"GRIT-Final\",\n",
    "    name=run_name,\n",
    "    job_type=\"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing GRITManager on device: cuda\n",
      "🎯 Instrumented 112 modules for GRIT optimization with custom autograd:\n",
      "   • 112 attention modules\n",
      "🚀 Using r-dim (16x16) covariances for maximum memory efficiency.\n",
      "GRITManager: Initialization complete.\n",
      "🔍 Optimizing 112 key LoRA modules.\n",
      "💾 K-FAC matrices stored on CPU for memory efficiency.\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Training Arguments ----------------\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    run_name=run_name,\n",
    "\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    learning_rate=config.learning_rate,\n",
    "\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=100,                 # Evaluate less often\n",
    "    logging_steps=50,\n",
    "    eval_accumulation_steps=1,\n",
    "    #max_steps=200,\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "    #save_steps=250,                 # Match eval_steps for early stopping\n",
    "    save_total_limit=2,\n",
    "\n",
    "    #fp16=config.precision == \"fp16\",\n",
    "    bf16=config.precision == \"bf16\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=config.num_workers,\n",
    "    dataloader_pin_memory=config.pin_memory,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    report_to=\"wandb\",\n",
    "    #metric_for_best_model=\"bleu\",\n",
    "    #greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    \n",
    "    # --- Generation settings for faster evaluation ---\n",
    "    generation_max_length=config.max_length + 128,\n",
    "    generation_num_beams=1,\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    #load_best_model_at_end=config.enable_early_stopping,\n",
    "\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"🚀 Initializing GRITManager on device:\", device)\n",
    "grit_manager = GRITManager(model, config, device)\n",
    "\n",
    "# Instantiate the new callback\n",
    "grit_callback = GritCallback(grit_manager)\n",
    "\n",
    "# Let the Trainer use its default high-performance optimizer\n",
    "# The custom 8-bit optimizer is not needed when memory is abundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Parameter Statistics:\n",
      "🔢 Total parameters: 1,812,638,720\n",
      "🔥 Trainable parameters: 9,175,040 (0.506%)\n",
      "📐 LoRA parameters: 9,175,040\n",
      "\n",
      "📍 Layer-wise LoRA Distribution:\n",
      "   Layer 0: 327,680 params\n",
      "   Layer 1: 327,680 params\n",
      "   Layer 2: 327,680 params\n",
      "   Layer 3: 327,680 params\n",
      "   Layer 4: 327,680 params\n",
      "   Layer 5: 327,680 params\n",
      "   Layer 6: 327,680 params\n",
      "   Layer 7: 327,680 params\n",
      "   Layer 8: 327,680 params\n",
      "   Layer 9: 327,680 params\n",
      "   Layer 10: 327,680 params\n",
      "   Layer 11: 327,680 params\n",
      "   Layer 12: 327,680 params\n",
      "   Layer 13: 327,680 params\n",
      "   Layer 14: 327,680 params\n",
      "   Layer 15: 327,680 params\n",
      "   Layer 16: 327,680 params\n",
      "   Layer 17: 327,680 params\n",
      "   Layer 18: 327,680 params\n",
      "   Layer 19: 327,680 params\n",
      "   Layer 20: 327,680 params\n",
      "   Layer 21: 327,680 params\n",
      "   Layer 22: 327,680 params\n",
      "   Layer 23: 327,680 params\n",
      "   Layer 24: 327,680 params\n",
      "   Layer 25: 327,680 params\n",
      "   Layer 26: 327,680 params\n",
      "   Layer 27: 327,680 params\n",
      "\n",
      "🎯 Strategy: LoRA + GRIT applied ONLY to layers 0-27\n",
      "🚫 Layers 0--1: Completely untouched (frozen by default)\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count and display parameter statistics\"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    lora_params = 0\n",
    "    \n",
    "    layer_params = {}  # Track params by layer\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            if \"lora_\" in name:\n",
    "                lora_params += param.numel()\n",
    "                \n",
    "                # Extract layer number for statistics\n",
    "                for i in range(32):\n",
    "                    if f'layers.{i}.' in name:\n",
    "                        if i not in layer_params:\n",
    "                            layer_params[i] = 0\n",
    "                        layer_params[i] += param.numel()\n",
    "                        break\n",
    "    \n",
    "    print(f\"\\n📊 Parameter Statistics:\")\n",
    "    print(f\"🔢 Total parameters: {total_params:,}\")\n",
    "    print(f\"🔥 Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.3f}%)\")\n",
    "    print(f\"📐 LoRA parameters: {lora_params:,}\")\n",
    "    \n",
    "    print(f\"\\n📍 Layer-wise LoRA Distribution:\")\n",
    "    active_layers = sorted(layer_params.keys())\n",
    "    for layer_id in active_layers:\n",
    "        print(f\"   Layer {layer_id}: {layer_params[layer_id]:,} params\")\n",
    "    \n",
    "    if active_layers:\n",
    "        print(f\"\\n🎯 Strategy: LoRA + GRIT applied ONLY to layers {min(active_layers)}-{max(active_layers)}\")\n",
    "        print(f\"🚫 Layers 0-{min(active_layers)-1}: Completely untouched (frozen by default)\")\n",
    "\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Call this after model setup\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "if wandb.run:\n",
    "    wandb.config.update({\n",
    "        \"total_model_params\": total_params,\n",
    "        \"lora_trainable_params\": trainable_params,\n",
    "        \"initial_lora_rank_r\": config.lora_rank,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8271/1386230625.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `GritTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GritTrainer: Initialized with GRIT implementation.\n",
      "🎯 Effective batch (per step): 32\n",
      "⚡ Mixed precision: bf16\n",
      "📏 Max sequence length: 1024\n",
      "💾 Checkpoints will be saved every 500 steps.\n",
      "🔧 LoRA rank: 16\n",
      "🚀 Starting training now...\n",
      "🎁 Wrapping the optimizer with GRIT preconditioning logic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1463' max='1463' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1463/1463 4:56:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.109600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRITManager: Inverting K-FAC factors at step 50...\n",
      "\n",
      "GRITManager: Neural reprojection at step 50...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   8,601,600\n",
      "   - Reduction:           573,440 (6.25%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 50 to 100...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 87...\n",
      "\n",
      "GRITManager: Neural reprojection at step 87...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   8,028,160\n",
      "   - Reduction:           1,146,880 (12.50%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 100 to 150...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 124...\n",
      "\n",
      "GRITManager: Neural reprojection at step 124...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   7,454,720\n",
      "   - Reduction:           1,720,320 (18.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 150 to 200...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 165...\n",
      "\n",
      "GRITManager: Neural reprojection at step 165...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   6,881,280\n",
      "   - Reduction:           2,293,760 (25.00%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 200 to 250...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 202...\n",
      "\n",
      "GRITManager: Neural reprojection at step 202...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   6,307,840\n",
      "   - Reduction:           2,867,200 (31.25%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 239...\n",
      "\n",
      "GRITManager: Neural reprojection at step 239...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,734,400\n",
      "   - Reduction:           3,440,640 (37.50%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 250 to 300...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 276...\n",
      "\n",
      "GRITManager: Neural reprojection at step 276...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 300 to 350...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 313...\n",
      "\n",
      "GRITManager: Neural reprojection at step 313...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 350...\n",
      "\n",
      "GRITManager: Neural reprojection at step 350...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,156,864\n",
      "   - Reduction:           4,018,176 (43.79%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 350 to 400...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 387...\n",
      "\n",
      "GRITManager: Neural reprojection at step 387...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,156,864\n",
      "   - Reduction:           4,018,176 (43.79%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 400 to 450...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 424...\n",
      "\n",
      "GRITManager: Neural reprojection at step 424...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 450 to 500...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 465...\n",
      "\n",
      "GRITManager: Neural reprojection at step 465...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 500 to 550...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 502...\n",
      "\n",
      "GRITManager: Neural reprojection at step 502...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 539...\n",
      "\n",
      "GRITManager: Neural reprojection at step 539...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 550 to 600...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 576...\n",
      "\n",
      "GRITManager: Neural reprojection at step 576...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 600 to 650...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 613...\n",
      "\n",
      "GRITManager: Neural reprojection at step 613...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 650...\n",
      "\n",
      "GRITManager: Neural reprojection at step 650...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 650 to 700...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 687...\n",
      "\n",
      "GRITManager: Neural reprojection at step 687...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 700 to 750...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 724...\n",
      "\n",
      "GRITManager: Neural reprojection at step 724...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 750 to 800...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 761...\n",
      "\n",
      "GRITManager: Neural reprojection at step 761...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 798...\n",
      "\n",
      "GRITManager: Neural reprojection at step 798...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 800 to 850...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 835...\n",
      "\n",
      "GRITManager: Neural reprojection at step 835...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 850 to 900...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 872...\n",
      "\n",
      "GRITManager: Neural reprojection at step 872...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 900 to 950...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 918...\n",
      "\n",
      "GRITManager: Neural reprojection at step 918...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 950 to 1000...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 955...\n",
      "\n",
      "GRITManager: Neural reprojection at step 955...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 992...\n",
      "\n",
      "GRITManager: Neural reprojection at step 992...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1000 to 1050...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1029...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1029...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1050 to 1100...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1066...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1066...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1100 to 1150...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1103...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1103...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1140...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1140...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1150 to 1200...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1177...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1177...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1200 to 1250...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1214...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1214...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1250 to 1300...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1251...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1251...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1288...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1288...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1300 to 1350...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1325...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1325...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1350 to 1400...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1366...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1366...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1400 to 1450...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1403...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1403...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1440...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1440...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,150,720\n",
      "   - Reduction:           4,024,320 (43.86%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1450 to 1500...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.o_proj</td><td>█▇▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.v_proj</td><td>█▇▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.o_proj</td><td>▁▂▃▄▅▆▇▇▇▇████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.v_proj</td><td>▁▂▃▄▅▆▇▇██████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.k_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.o_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.q_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.v_proj</td><td>▁▂▃▅▆▇████████████████████████████████</td></tr><tr><td>Parameters/GRIT Final Params</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters/GRIT Initial Params</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters/GRIT Param Reduction (%)</td><td>▁▂▃▄▆▇████████████████████████████████</td></tr><tr><td>adaptive_kfac_damping</td><td>▁█████▁█▁▁███▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄▅▇▅▁▅▄▅▃▃▆▅▅▆▆▅▇▆█▆▆▇▅▆▆█▇▇█</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.o_proj</td><td>8</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.v_proj</td><td>8</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.o_proj</td><td>50</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.v_proj</td><td>50</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.v_proj</td><td>43.75</td></tr><tr><td>Parameters/GRIT Final Params</td><td>5150720</td></tr><tr><td>Parameters/GRIT Initial Params</td><td>9175040</td></tr><tr><td>Parameters/GRIT Param Reduction (%)</td><td>43.86161</td></tr><tr><td>adaptive_kfac_damping</td><td>0.001</td></tr><tr><td>total_flos</td><td>8.131559285147566e+17</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>1463</td></tr><tr><td>train/grad_norm</td><td>0.59972</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1096</td></tr><tr><td>train_loss</td><td>0.15029</td></tr><tr><td>train_runtime</td><td>17779.5338</td></tr><tr><td>train_samples_per_second</td><td>2.632</td></tr><tr><td>train_steps_per_second</td><td>0.082</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grit-Llama-3.2-3B-alpaca</strong> at: <a href='https://wandb.ai/RAAPID/GRIT-Final/runs/ctztw2jj' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final/runs/ctztw2jj</a><br> View project at: <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final</a><br>Synced 5 W&B file(s), 672 media file(s), 1344 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Trainer uses GRITManager for K-FAC steps\n",
    "trainer = GritTrainer(\n",
    "    grit_manager=grit_manager,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[grit_callback],\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)] if config.enable_early_stopping else [],\n",
    ")\n",
    "\n",
    "# Final memory cleanup\n",
    "import gc\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(f\"🎯 Effective batch (per step): {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(\"⚡ Mixed precision:\", config.precision)\n",
    "print(\"📏 Max sequence length:\", config.max_length)\n",
    "print(f\"💾 Checkpoints will be saved every {training_args.save_steps} steps.\")\n",
    "print(\"🔧 LoRA rank:\", config.lora_rank)\n",
    "print(\"🚀 Starting training now...\")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "\n",
    "print(\"🎉 Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Uploading fine-tuned model to Hugging Face Hub...\n",
      "\n",
      "🚀 Uploading fine-tuned model to Hugging Face Hub...\n",
      "📝 Model card created with training details.\n",
      "🌐 Uploading model to https://huggingface.co/te4bag/GRIT-Full-alpaca-llama-3.2-3B-Energy-0.9 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6769aea0883f402bac6696b09222527c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/53.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model successfully uploaded: https://huggingface.co/te4bag/GRIT-Full-alpaca-llama-3.2-3B-Energy-0.9\n",
      "🎉 GRIT fine-tuning script completed!\n"
     ]
    }
   ],
   "source": [
    "# === Hugging Face Upload ===\n",
    "from huggingface_hub import create_repo, upload_folder, HfApi\n",
    "\n",
    "print(\"\\n🚀 Uploading fine-tuned model to Hugging Face Hub...\")\n",
    "\n",
    "# === Hugging Face Upload ===\n",
    "print(\"\\n🚀 Uploading fine-tuned model to Hugging Face Hub...\")\n",
    "\n",
    "HF_MODEL_NAME = f\"grit-lora-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}\"\n",
    "HF_USERNAME = \"te4bag\" # Replace with your Hugging Face username\n",
    "FULL_MODEL_NAME = f\"{HF_USERNAME}/GRIT-Full-alpaca-llama-3.2-3B-Energy-0.9\"\n",
    "\n",
    "try:\n",
    "    # Save model and tokenizer locally first\n",
    "    output_dir = \"./grit_trained_model\"\n",
    "    model.save_pretrained(output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Create/update model card (with hardware and hyperparams)\n",
    "    model_card_content = f\"\"\"---\n",
    "tags:\n",
    "- llama\n",
    "- alpaca\n",
    "- grit\n",
    "- lora\n",
    "- qlora\n",
    "- unsloth\n",
    "- instruction-tuning\n",
    "- fine-tuned\n",
    "base_model: {config.model_id}\n",
    "library_name: peft\n",
    "license: apache-2.0\n",
    "datasets:\n",
    "- {config.dataset_name}\n",
    "language:\n",
    "- en\n",
    "pipeline_tag: text-generation\n",
    "---\n",
    "\n",
    "# {config.model_id} Fine-tuned with GRIT and QLoRA (Unsloth)\n",
    "\n",
    "This model is a fine-tuned version of [{config.model_id}](https://huggingface.co/{config.model_id}) using the **GRIT** (Geometric Reprojection Instruction Tuning) algorithm and **QLoRA** on the [{config.dataset_name} dataset](https://huggingface.co/datasets/{config.dataset_name}).\n",
    "\n",
    "The base model is quantized to 4-bit (NF4) and optimized with [Unsloth](https://github.com/unslothai/unsloth) to enable efficient fine-tuning.\n",
    "\n",
    "## 🚀 Training Details\n",
    "\n",
    "### GRIT Algorithm\n",
    "- **K-FAC Updates**: Every {config.kfac_update_freq} steps (adaptive) for second-order preconditioning.\n",
    "- **Neural Reprojection**: Every {config.reprojection_freq} steps (adaptive) for rank optimization.\n",
    "- **Rank Adaptation**: {'Enabled' if config.enable_rank_adaptation else 'Disabled'} (Threshold: {config.rank_adaptation_threshold}, Min Rank: {config.min_lora_rank}).\n",
    "- **Optimized LoRA Modules**: {config.lora_target_modules}\n",
    "\n",
    "### Fine-tuning Configuration\n",
    "- **Base Model**: {config.model_id}\n",
    "- **Quantization**: 4-bit (NF4) with {config.precision} compute.\n",
    "- **LoRA Rank**: {config.lora_rank}\n",
    "- **LoRA Alpha**: {config.lora_alpha}\n",
    "- **Batch Size**: {config.batch_size} (per device)\n",
    "- **Gradient Accumulation**: {config.gradient_accumulation_steps} (Effective batch = {config.batch_size * config.gradient_accumulation_steps})\n",
    "- **Learning Rate**: {config.learning_rate:.1e}\n",
    "- **Precision**: {config.precision} mixed precision\n",
    "- **Sequence Length**: {config.max_length} tokens\n",
    "- **Gradient Checkpointing**: Enabled\n",
    "\n",
    "### Performance Improvements\n",
    "- ✅ **Faster Convergence**: K-FAC preconditioning aligns updates with curvature.\n",
    "- ✅ **Memory-Efficient**: 4-bit quantization (QLoRA) and gradient checkpointing used.\n",
    "- ✅ **Adaptive Rank**: Dynamically prunes LoRA rank to improve parameter efficiency.\n",
    "\n",
    "## 📊 Training Metrics\n",
    "- **Total Steps**: {trainer.state.global_step if 'trainer' in locals() else 'N/A'}\n",
    "- **Final Loss**: {trainer.state.log_history[-1].get('train_loss', 'N/A') if 'trainer' in locals() and trainer.state.log_history else 'N/A'}\n",
    "- **Trainable Params**: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\n",
    "\n",
    "## 📝 Algorithm Details\n",
    "- **K-FAC Preconditioning** (Natural Gradient) and **Neural Reprojection** as per GRIT method.\n",
    "- **Memory Efficient**: Covariance matrices on CPU to reduce GPU load.\n",
    "\n",
    "## 🏆 Results\n",
    "In benchmark comparisons, GRIT has shown **faster convergence and better stability** than standard LoRA or fine-tuning, making it well-suited for efficient single-epoch training. The use of Unsloth further accelerates this process.\n",
    "\n",
    "## 📝 Citation\n",
    "If you use this model, please cite the original GRIT paper and:\n",
    "```bibtex\n",
    "@misc{{grit-lora-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}}},\n",
    "  title={{ {config.model_id} Fine-tuned with GRIT on {config.dataset_name} }},\n",
    "  author={{{HF_USERNAME}}},\n",
    "  year={{2024}},\n",
    "  publisher={{Hugging Face}},\n",
    "  url={{https://huggingface.co/{FULL_MODEL_NAME}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## ⚖️ License\n",
    "This model inherits the Apache 2.0 license.\n",
    "\"\"\"\n",
    "    with open(f\"{output_dir}/README.md\", \"w\") as f:\n",
    "        f.write(model_card_content)\n",
    "    print(\"📝 Model card created with training details.\")\n",
    "\n",
    "    # Create repository on the Hub and upload the folder\n",
    "    print(f\"🌐 Uploading model to https://huggingface.co/{FULL_MODEL_NAME} ...\")\n",
    "    create_repo(FULL_MODEL_NAME, exist_ok=True)\n",
    "    api = HfApi()\n",
    "    api.upload_folder(\n",
    "        folder_path=output_dir,\n",
    "        repo_id=FULL_MODEL_NAME,\n",
    "        commit_message=f\"GRIT fine-tuned {config.model_id.split('/')[-1]} on {config.dataset_name.split('/')[-1]}\"\n",
    "    )\n",
    "    print(f\"✅ Model successfully uploaded: https://huggingface.co/{FULL_MODEL_NAME}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Upload failed: {e}\\\\n(Model saved locally in {output_dir})\")\n",
    "\n",
    "print(\"🎉 GRIT fine-tuning script completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
