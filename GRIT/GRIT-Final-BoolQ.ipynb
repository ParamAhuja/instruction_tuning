{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface\n",
      "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting filelock (from huggingface_hub)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.3.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.6.15)\n",
      "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.9/485.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: huggingface, tqdm, safetensors, regex, hf-xet, fsspec, filelock, huggingface_hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.7.0 hf-xet-1.1.7 huggingface-0.0.1 huggingface_hub-0.34.3 regex-2025.7.34 safetensors-0.6.1 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.55.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting torch<3,>=2.2 (from bitsandbytes)\n",
      "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch<3,>=2.2->bitsandbytes) (68.1.2)\n",
      "Collecting sympy>=1.13.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.7.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.46.1 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.8.0 triton-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.1)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch>=2.0.0->accelerate) (68.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.7/374.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.6.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, accelerate, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "Successfully installed accelerate-1.10.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.3 multiprocess-0.70.16 pandas-2.3.1 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.55.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.10.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.34.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.7)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch>=1.13.0->peft) (68.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.21.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.6.15)\n",
      "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchao\n",
      "  Downloading torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting torchtune\n",
      "  Downloading torchtune-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting torchdata==0.11.0 (from torchtune)\n",
      "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from torchtune) (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.12/dist-packages (from torchtune) (0.34.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from torchtune) (0.6.1)\n",
      "Collecting kagglehub (from torchtune)\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting sentencepiece (from torchtune)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from torchtune)\n",
      "  Downloading tiktoken-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting blobfile>=2 (from torchtune)\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from torchtune) (0.21.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtune) (2.3.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtune) (4.67.1)\n",
      "Collecting omegaconf (from torchtune)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from torchtune) (7.0.0)\n",
      "Collecting Pillow>=9.4.0 (from torchtune)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.11.0->torchtune) (2.5.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchdata==0.11.0->torchtune) (2.32.4)\n",
      "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.11.0->torchtune) (2.8.0)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile>=2->torchtune)\n",
      "  Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting lxml>=4.9 (from blobfile>=2->torchtune)\n",
      "  Downloading lxml-6.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.12/dist-packages (from blobfile>=2->torchtune) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->torchtune) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->torchtune) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->torchtune) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (1.1.7)\n",
      "Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]->torchtune)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->torchtune) (2025.7.34)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (3.12.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.11.0->torchtune) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (68.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2->torchdata==0.11.0->torchtune) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2->torchdata==0.11.0->torchtune) (3.0.2)\n",
      "Downloading torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchtune-0.6.1-py3-none-any.whl (910 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m910.7/910.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml-6.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=bcc7e8de4469a50653a2f4f8260aafff84b193403b31e4bcfcc5b4b8f4bcc2f5\n",
      "  Stored in directory: /root/.cache/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: torchao, sentencepiece, antlr4-python3-runtime, pycryptodomex, Pillow, omegaconf, lxml, hf-transfer, tiktoken, kagglehub, blobfile, torchdata, torchtune\n",
      "Successfully installed Pillow-11.3.0 antlr4-python3-runtime-4.9.3 blobfile-3.0.0 hf-transfer-0.1.9 kagglehub-0.3.12 lxml-6.0.0 omegaconf-2.3.0 pycryptodomex-3.23.0 sentencepiece-0.2.0 tiktoken-0.10.0 torchao-0.12.0 torchdata-0.11.0 torchtune-0.6.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchao torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (6.31.1)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.34.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.14.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Using cached wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached sentry_sdk-2.34.1-py2.py3-none-any.whl (357 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: typing-inspection, smmap, sentry-sdk, pydantic-core, click, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 click-8.2.1 gitdb-4.0.12 gitpython-3.1.45 pydantic-2.11.7 pydantic-core-2.33.2 sentry-sdk-2.34.1 smmap-5.0.2 typing-inspection-0.4.1 wandb-0.21.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable the tokenizer parallelism warning to avoid spam\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_scheduler,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import bitsandbytes as bnb\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from huggingface_hub import login, HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f4c1e93ad940ce982d2eaa7e8bddd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFACAutogradFunction(torch.autograd.Function):\n",
    "    \"\"\"Custom autograd function to capture activations and gradients for K-FAC.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, module, output, input):\n",
    "        ctx.module = module\n",
    "        # We only need the input for the backward pass to compute activation stats\n",
    "        ctx.save_for_backward(input.detach())\n",
    "        # We pass the output through, it's what the next layer will see\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_wrt_output):\n",
    "        # This grad is the gradient w.r.t the output of the LoraLayer\n",
    "        module = ctx.module\n",
    "        input, = ctx.saved_tensors # This is the original input to the LoraLayer\n",
    "        manager = module.grit_manager\n",
    "        manager.backward_step += 1\n",
    "\n",
    "        if not module.training:\n",
    "            # Pass gradients through without modification if not training\n",
    "            return None, grad_wrt_output, None\n",
    "            \n",
    "        # --- Run covariance updates periodically to avoid bottlenecking ---\n",
    "        if manager.backward_step % manager.config.grit_cov_update_freq != 0:\n",
    "            return None, grad_wrt_output, None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # --- 1. Project activations into r-space and update covariance ---\n",
    "            # a has shape (batch*seq, in_features)\n",
    "            a = input.reshape(-1, input.shape[-1])\n",
    "            if a.shape[0] > 0 and 'default' in module.lora_A:\n",
    "                # A has shape (r, in_features), so A.T has (in_features, r)\n",
    "                # Cast weight to the same dtype as activation `a` to prevent mismatch\n",
    "                lora_A_T = module.lora_A['default'].weight.data.T.to(device=a.device, dtype=a.dtype, non_blocking=True)\n",
    "                # projected_a has shape (batch*seq, r)\n",
    "                projected_a = a @ lora_A_T\n",
    "                a_cov_sample = projected_a.T @ projected_a\n",
    "                \n",
    "                # Online update for a_covs\n",
    "                current_cov = manager.a_covs[module]\n",
    "                n = manager.num_samples_a[module]\n",
    "                new_n = n + projected_a.shape[0]\n",
    "                if new_n > 0:\n",
    "                    updated_cov = (current_cov.float() * n + a_cov_sample.cpu().float()) / new_n\n",
    "                    manager.a_covs[module].copy_(updated_cov)\n",
    "                    manager.num_samples_a[module] = new_n\n",
    "\n",
    "            # --- 2. Project gradients into r-space and update covariance ---\n",
    "            # g has shape (batch*seq, out_features)\n",
    "            g = grad_wrt_output.reshape(-1, grad_wrt_output.shape[-1])\n",
    "            if g.shape[0] > 0 and 'default' in module.lora_B:\n",
    "                # B has shape (out_features, r)\n",
    "                # Cast weight to the same dtype as gradient `g` to prevent mismatch\n",
    "                lora_B = module.lora_B['default'].weight.data.to(device=g.device, dtype=g.dtype, non_blocking=True)\n",
    "                # projected_g has shape (batch*seq, r)\n",
    "                projected_g = g @ lora_B\n",
    "                g_cov_sample = projected_g.T @ projected_g\n",
    "\n",
    "                # Online update for g_covs\n",
    "                current_cov = manager.g_covs[module]\n",
    "                n = manager.num_samples_g[module]\n",
    "                new_n = n + projected_g.shape[0]\n",
    "                if new_n > 0:\n",
    "                    updated_cov = (current_cov.float() * n + g_cov_sample.cpu().float()) / new_n\n",
    "                    manager.g_covs[module].copy_(updated_cov)\n",
    "                    manager.num_samples_g[module] = new_n\n",
    "\n",
    "        # We return gradients for the inputs of the `forward` method:\n",
    "        # (module, output, input)\n",
    "        # 1. module: Not a tensor, so None\n",
    "        # 2. output: The gradient is `grad_wrt_output`, pass it back to the LoraLayer\n",
    "        # 3. input: This function does not depend on `input` for its output's value,\n",
    "        #    so its gradient contribution w.r.t. `input` is zero.\n",
    "        return None, grad_wrt_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this function outside your GRITManager class.\n",
    "# It only knows how to work with Tensors.\n",
    "def jit_invert_tensor_pair(a_cov: torch.Tensor, g_cov: torch.Tensor, kfac_damping: float):\n",
    "    \"\"\"\n",
    "    JIT-compatible function to invert a SINGLE pair of covariance tensors.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        damping = max(kfac_damping, 1e-6)\n",
    "        \n",
    "        a_cov_damped = a_cov.float() + damping * torch.eye(a_cov.shape[0], device='cpu')\n",
    "        g_cov_damped = g_cov.float() + damping * torch.eye(g_cov.shape[0], device='cpu')\n",
    "        \n",
    "        L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n",
    "        L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n",
    "        \n",
    "        if info_a == 0 and info_g == 0:\n",
    "            a_inv = torch.cholesky_inverse(L_a).half()\n",
    "            g_inv = torch.cholesky_inverse(L_g).half()\n",
    "            return a_inv, g_inv\n",
    "        else:\n",
    "            # Return empty tensors on failure, which we can check for later\n",
    "            return torch.empty(0), torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRITConfig:\n",
    "    \"\"\"Configuration class for GRIT. \"\"\"\n",
    "    def __init__(self):\n",
    "        # Using a 3B model, optimized for a powerful A100 GPU\n",
    "        self.model_id = \"meta-llama/Llama-3.2-3B\"  # Using Unsloth's pre-optimized model\n",
    "        \n",
    "        # -------- Training Configuration (Now Optimized for Kaggle -> 16GB VRAM) --------\n",
    "        self.batch_size = 8  # Lowered from 8 for memory constraints\n",
    "        self.gradient_accumulation_steps = 2\n",
    "        self.num_epochs = 1\n",
    "        self.learning_rate = 2e-5\n",
    "        self.precision = \"bf16\" # bf16 is still optimal for modern GPUs\n",
    "        self.max_length = 2048\n",
    "\n",
    "        # LoRA configuration (optimized for memory)\n",
    "        self.lora_rank = 16   # Start with a higher rank to allow for pruning\n",
    "        self.lora_alpha = 32 # Adjusted for new rank (2 * rank)\n",
    "        self.lora_dropout = 0.0\n",
    "        # 5W heuristic for layer selection: include attention and key MLP layers\n",
    "        self.lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "        # GRIT parameters (tuned for speed vs. quality)\n",
    "        self.kfac_update_freq = 150 # Invert less often\n",
    "        self.kfac_damping = 0.001\n",
    "        self.reprojection_freq = 150\n",
    "        self.reprojection_k = 8\n",
    "        self.grit_cov_update_freq = 15 # Update covs a bit more often with more budget\n",
    "        \n",
    "        # --- Rank-Adaptive LoRA Configuration ---\n",
    "        self.enable_rank_adaptation = True\n",
    "        self.rank_adaptation_threshold = 0.9  # Cumulative energy threshold\n",
    "        self.min_lora_rank = 4                 # Minimum rank to prevent collapse\n",
    "\n",
    "        # --- Convergence Control ---\n",
    "        self.enable_early_stopping = True\n",
    "        self.early_stopping_patience = 3 # Stop after 3 evaluations with no improvement\n",
    "        \n",
    "        # Data loading configuration\n",
    "        self.dataset_name = \"google/boolq\" # Default dataset\n",
    "        self.num_workers = 2 # Reduced from 8 for Kaggle's lower CPU/RAM resources\n",
    "        self.pin_memory = False # This is generally fine\n",
    "        self.drop_last = True\n",
    "\n",
    "config = GRITConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRITManager:\n",
    "    \"\"\"\n",
    "    Memory-efficient GRIT Manager with CPU-based K-FAC storage and selective optimization.\n",
    "    This addresses both performance and memory issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.global_step = 0\n",
    "        self.backward_step = 0\n",
    "        \n",
    "        # Adaptive frequency state\n",
    "        self.loss_history = []\n",
    "        self.loss_history_capacity = 20  # Corresponds to window size in _get_adaptive_freq\n",
    "        self.last_kfac_update_step = 0\n",
    "        self.last_reprojection_step = 0\n",
    "        \n",
    "        # Memory-efficient K-FAC state storage (CPU-based)\n",
    "        self.a_covs = {}  # Input activation covariances (stored on CPU)\n",
    "        self.g_covs = {}  # Output gradient covariances (stored on CPU)\n",
    "        self.a_invs = {}  # Inverted input covariances (CPU)\n",
    "        self.g_invs = {}  # Inverted output covariances (CPU)\n",
    "        \n",
    "        # Online estimation counters\n",
    "        self.num_samples_a = {}\n",
    "        self.num_samples_g = {}\n",
    "\n",
    "        # Track which modules we're optimizing (subset for memory efficiency)\n",
    "        self.optimized_modules = []\n",
    "        \n",
    "        self.factors_are_ready = False\n",
    "        self._instrument_model()\n",
    "        \n",
    "        # Debug: Check how many LoRA layers we found\n",
    "        total_modules = len(self.optimized_modules)\n",
    "        print(f\"GRITManager: Initialization complete.\")\n",
    "        print(f\"🔍 Optimizing {total_modules} key LoRA modules.\")\n",
    "        print(f\"💾 K-FAC matrices stored on CPU for memory efficiency.\")\n",
    "\n",
    "    def _instrument_model(self):\n",
    "        \"\"\"\n",
    "        Replaces forward passes with a version that includes our autograd function.\n",
    "        Optimized to only instrument q_proj modules in the last 8 layers and use r-dim\n",
    "        covariance matrices to significantly reduce CPU RAM usage.\n",
    "        \"\"\"\n",
    "        attention_modules = 0\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, LoraLayer) and module.r['default'] > 0:\n",
    "                module.lora_name = name # Store name for logging\n",
    "                module.grit_manager = self \n",
    "                self.num_samples_a[module] = 0\n",
    "                self.num_samples_g[module] = 0\n",
    "                \n",
    "                # Store original forward method\n",
    "                module.original_forward = module.forward\n",
    "                \n",
    "                # Replace forward method to apply the hook *after* the original forward\n",
    "                def new_forward(self, x):\n",
    "                    y = self.original_forward(x)\n",
    "                    return KFACAutogradFunction.apply(self, y, x)\n",
    "\n",
    "                module.forward = new_forward.__get__(module, LoraLayer)\n",
    "\n",
    "                # --- store covariances in r-dim space (MASSIVE memory saving) ---\n",
    "                r = module.r['default']\n",
    "                self.a_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n",
    "                self.g_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n",
    "                \n",
    "                self.optimized_modules.append(module)\n",
    "                attention_modules += 1\n",
    "        \n",
    "        print(f\"🎯 Instrumented {attention_modules} modules for GRIT optimization with custom autograd:\")\n",
    "        print(f\"   • {attention_modules} attention modules\")\n",
    "        print(f\"🚀 Using r-dim ({self.config.lora_rank}x{self.config.lora_rank}) covariances for maximum memory efficiency.\")\n",
    "\n",
    "    def _get_adaptive_freq(self, base_freq, min_freq=1, max_freq=1000, window=20):\n",
    "        \"\"\"Calculates adaptive frequency based on loss stability.\"\"\"\n",
    "        if len(self.loss_history) < window:\n",
    "            return base_freq\n",
    "        \n",
    "        # Check loss trend over the last `window` steps\n",
    "        recent_losses = self.loss_history[-window:]\n",
    "        first_half = sum(recent_losses[:window//2]) / (window//2)\n",
    "        second_half = sum(recent_losses[window//2:]) / (window//2)\n",
    "        \n",
    "        # If loss is decreasing (learning is stable), decrease frequency (run less often)\n",
    "        if second_half < first_half * 0.99:\n",
    "            new_freq = int(base_freq * 1.5)\n",
    "        # If loss is fluctuating or increasing, increase frequency (run more often)\n",
    "        else:\n",
    "            new_freq = int(base_freq * 0.75)\n",
    "            \n",
    "        return max(min_freq, min(new_freq, max_freq))\n",
    "\n",
    "    def step(self, loss=None):\n",
    "        \"\"\"Called after each optimizer step to manage periodic updates\"\"\"\n",
    "        self.global_step += 1\n",
    "        if loss is not None:\n",
    "            self.loss_history.append(loss)\n",
    "            if len(self.loss_history) > self.loss_history_capacity:\n",
    "                self.loss_history = self.loss_history[-self.loss_history_capacity:]\n",
    "\n",
    "        # --- Second-order damping schedule ---\n",
    "        if len(self.loss_history) > 10: # Need enough history to compute variance\n",
    "            loss_variance = torch.tensor(self.loss_history).var().item()\n",
    "            # Scale damping with variance, with min/max caps\n",
    "            self.config.kfac_damping = max(1e-6, min(0.01, 0.001 + math.sqrt(loss_variance)))\n",
    "            if self.global_step % 100 == 0: # Log periodically\n",
    "                 wandb.log({\"adaptive_kfac_damping\": self.config.kfac_damping})\n",
    "\n",
    "        # Adaptive K-FAC update frequency\n",
    "        kfac_freq = self._get_adaptive_freq(self.config.kfac_update_freq)\n",
    "        if self.global_step - self.last_kfac_update_step >= kfac_freq:\n",
    "            self.update_and_invert_factors()\n",
    "            self.last_kfac_update_step = self.global_step\n",
    "        \n",
    "        # Adaptive neural reprojection frequency\n",
    "        reproj_freq = self._get_adaptive_freq(self.config.reprojection_freq, min_freq=1, max_freq=2000)\n",
    "        if self.global_step - self.last_reprojection_step >= reproj_freq:\n",
    "            self.neural_reprojection()\n",
    "            self.last_reprojection_step = self.global_step\n",
    "\n",
    "    # Inside your GRITManager class\n",
    "    def update_and_invert_factors(self):\n",
    "        print(f\"\\nGRITManager: Inverting K-FAC factors at step {self.global_step}...\")\n",
    "\n",
    "        # JIT script the new, simpler function ONCE outside the loop\n",
    "        scripted_invert_fn = torch.jit.script(jit_invert_tensor_pair)\n",
    "\n",
    "        # The loop over modules stays in regular Python, where module keys are OK\n",
    "        for module in self.optimized_modules:\n",
    "            a_cov = self.a_covs[module]\n",
    "            g_cov = self.g_covs[module]\n",
    "\n",
    "            # Call the JIT function with TENSORS ONLY\n",
    "            a_inv, g_inv = scripted_invert_fn(\n",
    "                a_cov=a_cov,\n",
    "                g_cov=g_cov,\n",
    "                kfac_damping=self.config.kfac_damping\n",
    "            )\n",
    "\n",
    "            # Check if the inversion was successful and update the dictionaries\n",
    "            if a_inv.numel() > 0 and g_inv.numel() > 0:\n",
    "                self.a_invs[module] = a_inv\n",
    "                self.g_invs[module] = g_inv\n",
    "            else:\n",
    "                print(f\"K-FAC inversion failed for a module. Skipping.\")\n",
    "        \n",
    "        self.factors_are_ready = True\n",
    "\n",
    "    def _invert_factors_fn(self):\n",
    "        with torch.no_grad():\n",
    "            for module in self.optimized_modules:\n",
    "                # Ensure damping is non-negative\n",
    "                damping = max(self.config.kfac_damping, 1e-6)\n",
    "                \n",
    "                # --- Invert Activation Covariance (a_cov) ---\n",
    "                a_cov_damped = self.a_covs[module].float() + damping * torch.eye(\n",
    "                    self.a_covs[module].shape[0], device='cpu'\n",
    "                )\n",
    "                \n",
    "                # Use cholesky_ex to check for positive-definiteness (a proxy for invertibility here)\n",
    "                # L_a is the Cholesky factor, info_a is 0 on success\n",
    "                L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n",
    "                \n",
    "                # --- Invert Gradient Covariance (g_cov) ---\n",
    "                g_cov_damped = self.g_covs[module].float() + damping * torch.eye(\n",
    "                    self.g_covs[module].shape[0], device='cpu'\n",
    "                )\n",
    "                L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n",
    "\n",
    "                # Check if both decompositions succeeded\n",
    "                if info_a == 0 and info_g == 0:\n",
    "                    # If successful, compute inverse from Cholesky factor (more stable)\n",
    "                    self.a_invs[module] = torch.cholesky_inverse(L_a).half()\n",
    "                    self.g_invs[module] = torch.cholesky_inverse(L_g).half()\n",
    "                else:\n",
    "                    # This block runs if inversion would have failed\n",
    "                    print(f\"K-FAC inversion check failed for a module. Skipping update.\")\n",
    "                    # You can still log this event if needed, but wandb calls might not be JIT-friendly.\n",
    "                    # It's better to handle logging outside the JIT-compiled function.\n",
    "                    continue\n",
    "\n",
    "    def precondition_gradients(self):\n",
    "        \"\"\"Apply K-FAC preconditioning with efficient CPU-GPU transfers\"\"\"\n",
    "        if not self.factors_are_ready:\n",
    "            return\n",
    "        if self.global_step % self.config.kfac_update_freq == 0:\n",
    "            print(f\"\\nGRITManager: Applying Natural Gradient preconditioner at step {self.global_step} to {self.global_step + 50}...\")\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # Determine correct dtype based on config\n",
    "            dtype = torch.float16 if self.config.precision == \"fp16\" else torch.bfloat16\n",
    "\n",
    "            for module in self.optimized_modules:\n",
    "                if module not in self.a_invs or module not in self.g_invs:\n",
    "                    continue\n",
    "                    \n",
    "                lora_a = module.lora_A['default']\n",
    "                lora_b = module.lora_B['default']\n",
    "                \n",
    "                if (lora_a is None or lora_b is None or \n",
    "                    lora_a.weight.grad is None or lora_b.weight.grad is None):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Move inverse matrices to GPU only when needed, using correct dtype\n",
    "                    a_inv = self.a_invs[module].to(self.device, dtype=dtype)\n",
    "                    g_inv = self.g_invs[module].to(self.device, dtype=dtype)\n",
    "                    \n",
    "                    # --- True Natural Gradient Computation: grad' = G_inv @ grad @ A_inv ---\n",
    "                    \n",
    "                    # Precondition LoRA B gradient (the \"easy\" part)\n",
    "                    # Original grad_b has shape (out, r)\n",
    "                    grad_b = lora_b.weight.grad.to(dtype)\n",
    "                    # Corrected multiplication: (out, r) @ (r, r) -> (out, r)\n",
    "                    preconditioned_b_grad = grad_b @ g_inv\n",
    "                    \n",
    "                    # Precondition LoRA A gradient (the \"hard\" part)\n",
    "                    # Original grad_a has shape (r, in)\n",
    "                    grad_a = lora_a.weight.grad.to(dtype)\n",
    "                    \n",
    "                    # Safety reshape for larger ranks\n",
    "                    r = module.r['default']\n",
    "                    if r > 0:\n",
    "                        grad_a = grad_a.view(r, -1)\n",
    "                    \n",
    "                    # Instead, we apply the preconditioning to each matrix's gradient.\n",
    "                    # grad_a' = A_inv @ grad_a\n",
    "                    # Corrected multiplication: (r, r) @ (r, in) -> (r, in)\n",
    "                    preconditioned_a_grad = a_inv @ grad_a\n",
    "\n",
    "                    # Copy back the preconditioned gradients\n",
    "                    lora_a.weight.grad.copy_(preconditioned_a_grad.to(lora_a.weight.grad.dtype))\n",
    "                    lora_b.weight.grad.copy_(preconditioned_b_grad.to(lora_b.weight.grad.dtype))\n",
    "                    \n",
    "                    # Clean up GPU tensors immediately\n",
    "                    del a_inv, g_inv, grad_a, grad_b, preconditioned_a_grad, preconditioned_b_grad\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Gradient preconditioning failed: {e}\")\n",
    "                    wandb.log({\"preconditioning_error\": 1})\n",
    "                    continue\n",
    "\n",
    "    def neural_reprojection(self):\n",
    "        \"\"\"Perform neural reprojection and log the parameter reduction.\"\"\"\n",
    "        print(f\"\\nGRITManager: Neural reprojection at step {self.global_step}...\")\n",
    "        \n",
    "        initial_params = 0\n",
    "        final_params = 0\n",
    "        log_dict = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # First, calculate initial effective parameters for all GRIT-optimized modules\n",
    "            for module in self.optimized_modules:\n",
    "                if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                    # Effective parameters in LoRA = r * (in_features + out_features)\n",
    "                    initial_params += module.r['default'] * (module.in_features + module.out_features)\n",
    "\n",
    "            for module in self.optimized_modules:\n",
    "                try:\n",
    "                    lora_a = module.lora_A['default']\n",
    "                    lora_b = module.lora_B['default']\n",
    "                    \n",
    "                    if lora_a is None or lora_b is None:\n",
    "                        continue\n",
    "                        \n",
    "                    A = lora_a.weight.data.float()\n",
    "                    B = lora_b.weight.data.float()\n",
    "                    \n",
    "                    r = A.shape[0]  # Current LoRA rank\n",
    "                    k = r # Initialize k to the current rank as a fallback\n",
    "                    \n",
    "                    # Form the r x r covariance matrix M = A @ A.T\n",
    "                    M = A @ A.T\n",
    "                    \n",
    "                    # --- Numerical Stability Check ---\n",
    "                    if torch.isnan(M).any() or torch.isinf(M).any():\n",
    "                        print(f\"WARNING: Covariance matrix M for {module.lora_name} contains NaN/Inf. Skipping reprojection for this module.\")\n",
    "                        log_dict[f\"reprojection_errors/nan_inf/{module.lora_name}\"] = 1\n",
    "                        # If skipped, the rank does not change.\n",
    "                        final_params += r * (module.in_features + module.out_features)\n",
    "                        continue\n",
    "\n",
    "                    # --- Rank Adaptation & Pruning Logic ---\n",
    "                    if self.config.enable_rank_adaptation and r > self.config.min_lora_rank:\n",
    "                        # --- Full Eigendecomposition for Adaptive Rank ---\n",
    "                        try:\n",
    "                            eigenvals, V = torch.linalg.eigh(M)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Eigendecomposition failed for module: {e}\", \"error\")\n",
    "                            log_dict[\"reprojection_errors/eigh_error\"] = log_dict.get(\"reprojection_errors/eigh_error\", 0) + 1\n",
    "                            final_params += r * (module.in_features + module.out_features)\n",
    "                            continue\n",
    "                        \n",
    "                        sorted_indices = torch.argsort(eigenvals, descending=True)\n",
    "                        sorted_eigenvals = eigenvals[sorted_indices]\n",
    "                        V = V[:, sorted_indices]\n",
    "\n",
    "                        total_energy = torch.sum(sorted_eigenvals)\n",
    "                        if total_energy > 1e-6:\n",
    "                            cumulative_energy = torch.cumsum(sorted_eigenvals, dim=0) / total_energy\n",
    "                            k = (cumulative_energy < self.config.rank_adaptation_threshold).sum().item() + 1\n",
    "                        \n",
    "                        k = max(k, self.config.min_lora_rank) # Enforce minimum rank\n",
    "                        rank_reduction_percent = (1 - k / r) * 100 if r > 0 else 0\n",
    "                        \n",
    "                        log_dict[f\"GRIT/Effective Rank (k)/{module.lora_name}\"] = k\n",
    "                        log_dict[f\"GRIT/Rank Reduction (%)/{module.lora_name}\"] = rank_reduction_percent\n",
    "\n",
    "                        # Log the sorted eigenvalue distribution to show energy decay\n",
    "                        # --- Enhanced WandB Logging for Eigen-spectra ---\n",
    "                        # --- Log Data Tables for Manual Plotting ---\n",
    "                        try:\n",
    "                            # Create a shorter name for logging\n",
    "                            short_lora_name = module.lora_name.replace(\"base_model.model.\", \"\")\n",
    "                        \n",
    "                            # 1. Log Eigen value spectra (This part is fine)\n",
    "                            eigen_spectra_list = sorted_eigenvals.cpu().numpy().tolist()\n",
    "                            data_for_table = [[s] for s in eigen_spectra_list]\n",
    "                            table = wandb.Table(data=data_for_table, columns=[\"eigenvalue\"])\n",
    "                            hist_plot = wandb.plot.histogram(table, \"eigenvalue\", title=f\"Eigenvalue Spectrum - {short_lora_name}\")\n",
    "                            log_dict[f\"Charts/Eigenvalue Spectrum/{short_lora_name}\"] = hist_plot\n",
    "                        \n",
    "                            # 2. Log ALL (Original) Eigenvectors\n",
    "                            V_all_T = V.T.cpu().numpy()  # <-- CORRECT: Use the full V matrix\n",
    "                            heatmap_data_all = []\n",
    "                            for x, eigenvector in enumerate(V_all_T):\n",
    "                                for y, component in enumerate(eigenvector):\n",
    "                                    heatmap_data_all.append([x, y, component])\n",
    "                            heatmap_table_all = wandb.Table(\n",
    "                                data=heatmap_data_all,\n",
    "                                columns=[\"eigenvector_idx\", \"component_idx\", \"value\"]\n",
    "                            )\n",
    "                            log_dict[f\"Data_Tables/Eigenvectors_All/{short_lora_name}\"] = heatmap_table_all\n",
    "                        \n",
    "                            # 3. Log the CHOSEN TOP-K eigenvectors\n",
    "                            V_k_T = V[:, :k].T.cpu().numpy() # This is correct for the chosen vectors\n",
    "                            heatmap_data_chosen = []\n",
    "                            for x, eigenvector in enumerate(V_k_T):\n",
    "                                for y, component in enumerate(eigenvector):\n",
    "                                    heatmap_data_chosen.append([x, y, component])\n",
    "                            heatmap_table_chosen = wandb.Table(\n",
    "                                data=heatmap_data_chosen,\n",
    "                                columns=[\"eigenvector_idx\", \"component_idx\", \"value\"]\n",
    "                            )\n",
    "                            log_dict[f\"Data_Tables/Eigenvectors_Chosen_TopK/{short_lora_name}\"] = heatmap_table_chosen\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"Data logging for {module.lora_name} failed: {e}\")\n",
    "                        \n",
    "                        V_k = V[:, :k]\n",
    "                    else:\n",
    "                        # --- Fixed Rank Reprojection ---\n",
    "                        k = min(self.config.reprojection_k, r)\n",
    "                        try:\n",
    "                            if k < r and k > 0:\n",
    "                                _, V_k = torch.lobpcg(lambda v: M @ v, X=torch.randn(r, k, device=M.device, dtype=M.dtype), k=k, largest=True)\n",
    "                            else:\n",
    "                                _, V = torch.linalg.eigh(M)\n",
    "                                eigenvals, _ = torch.linalg.eigh(M)\n",
    "                                V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n",
    "                        except Exception as e:\n",
    "                            print(f\"LOBPCG failed for module, falling back to eigh: {e}\")\n",
    "                            _, V = torch.linalg.eigh(M)\n",
    "                            eigenvals, _ = torch.linalg.eigh(M)\n",
    "                            V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n",
    "                    \n",
    "                    # Accumulate the new effective parameter count for this module\n",
    "                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                        final_params += k * (module.in_features + module.out_features)\n",
    "\n",
    "                    # --- Project onto the top-k eigenvectors and prune via zeroing ---\n",
    "                    A_proj = V_k.T @ A\n",
    "                    B_proj = B @ V_k\n",
    "\n",
    "                    A_new, B_new = torch.zeros_like(A), torch.zeros_like(B)\n",
    "                    A_new[:k, :], B_new[:, :k] = A_proj, B_proj\n",
    "\n",
    "                    lora_a.weight.data.copy_(A_new.to(A.dtype))\n",
    "                    lora_b.weight.data.copy_(B_new.to(B.dtype))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Neural reprojection failed for module {module.lora_name}: {e}\")\n",
    "                    log_dict[\"reprojection_errors/general_error\"] = log_dict.get(\"reprojection_errors/general_error\", 0) + 1\n",
    "                    # On failure, assume rank does not change for this module\n",
    "                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                        final_params += module.r['default'] * (module.in_features + module.out_features)\n",
    "                    continue\n",
    "\n",
    "        # --- Log the final results ---\n",
    "        if initial_params > 0:\n",
    "            param_reduction = initial_params - final_params\n",
    "            reduction_percent = (param_reduction / initial_params) * 100\n",
    "            print(f\"✅ Neural reprojection completed. Effective parameter count reduced.\")\n",
    "            print(f\"   - Initial GRIT params: {initial_params:,}\")\n",
    "            print(f\"   - Final GRIT params:   {final_params:,}\")\n",
    "            print(f\"   - Reduction:           {param_reduction:,} ({reduction_percent:.2f}%)\")\n",
    "            \n",
    "            log_dict.update({\n",
    "                \"Parameters/GRIT Initial Params\": initial_params,\n",
    "                \"Parameters/GRIT Final Params\": final_params,\n",
    "                \"Parameters/GRIT Param Reduction (%)\": reduction_percent,\n",
    "            })\n",
    "        else:\n",
    "            print(f\"✅ Neural reprojection completed for {len(self.optimized_modules)} modules.\")\n",
    "        \n",
    "        if log_dict:\n",
    "            wandb.log(log_dict, step=self.global_step)\n",
    "        \n",
    "        # Strategic memory cleanup after expensive operations\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class GritOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    A wrapper for a PyTorch optimizer that applies GRIT's K-FAC gradient\n",
    "    preconditioning before the actual optimization step. This ensures that the\n",
    "    optimizer uses the natural gradient instead of the standard gradient.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer: Optimizer, grit_manager: 'GRITManager'):\n",
    "        self.optimizer = optimizer\n",
    "        self.grit_manager = grit_manager\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.optimizer.state\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "\n",
    "    @param_groups.setter\n",
    "    def param_groups(self, value):\n",
    "        self.optimizer.param_groups = value\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        1.  Applies K-FAC preconditioning to the accumulated gradients.\n",
    "        2.  Calls the underlying optimizer's step function.\n",
    "        \"\"\"\n",
    "        # Apply K-FAC preconditioning to the accumulated gradients\n",
    "        if self.grit_manager.factors_are_ready:\n",
    "            self.grit_manager.precondition_gradients()\n",
    "\n",
    "        # Call the underlying optimizer's step function\n",
    "        self.optimizer.step(closure)\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        self.optimizer.zero_grad(set_to_none=set_to_none)\n",
    "\n",
    "    def add_param_group(self, param_group: dict):\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"GritOptimizer({self.optimizer.__repr__()})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GritCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    This callback injects GRIT's logic into the training loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, grit_manager):\n",
    "        self.grit_manager = grit_manager\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Triggered at the end of each training step.\n",
    "        \"\"\"\n",
    "        last_loss = state.log_history[-1].get(\"loss\") if state.log_history else None\n",
    "        self.grit_manager.step(loss=last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GritTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Optimized GRIT Trainer that addresses the core performance issues and\n",
    "    correctly applies gradient preconditioning by wrapping the optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grit_manager, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.grit_manager = grit_manager\n",
    "        print(\"GritTrainer: Initialized with GRIT implementation.\")\n",
    "\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        \"\"\"\n",
    "        Overrides the base method to wrap the created optimizer with our\n",
    "        GritOptimizer, which handles gradient preconditioning.\n",
    "        \"\"\"\n",
    "        super().create_optimizer_and_scheduler(num_training_steps)\n",
    "\n",
    "        if self.optimizer is not None:\n",
    "            print(\"🎁 Wrapping the optimizer with GRIT preconditioning logic.\")\n",
    "            self.optimizer = GritOptimizer(self.optimizer, self.grit_manager)\n",
    "\n",
    "    # --- THIS IS THE CORRECTED METHOD ---\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        The training step's signature is now aligned with the parent class.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "        \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        \"\"\"Overrides the default evaluate method to add aggressive memory cleanup.\"\"\"\n",
    "        print(\"\\n🧹 Clearing VRAM before evaluation...\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return super().evaluate(*args, **kwargs)\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "\n",
    "        has_labels = \"labels\" in inputs\n",
    "        if has_labels:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        _, generated_tokens, _ = super().prediction_step(\n",
    "            model, inputs, prediction_loss_only, ignore_keys\n",
    "        )\n",
    "\n",
    "        if has_labels:\n",
    "            inputs[\"labels\"] = labels\n",
    "\n",
    "        loss = None\n",
    "        if has_labels:\n",
    "            with torch.no_grad():\n",
    "                loss = self.compute_loss(model, inputs.copy()).detach()\n",
    "\n",
    "        if generated_tokens is not None and type(generated_tokens).__name__ == 'EmptyLogits':\n",
    "            batch_size = inputs[\"input_ids\"].shape[0]\n",
    "            seq_length = labels.shape[1] if labels is not None else inputs[\"input_ids\"].shape[1]\n",
    "            vocab_size = self.model.config.vocab_size\n",
    "\n",
    "            generated_tokens = torch.zeros(\n",
    "                (batch_size, seq_length, vocab_size),\n",
    "                device=self.accelerator.device,\n",
    "                dtype=config.unsloth_dtype\n",
    "            )\n",
    "\n",
    "        if labels is not None:\n",
    "            if len(labels.shape) == 3:\n",
    "                labels = torch.argmax(labels, dim=-1)\n",
    "            elif len(labels.shape) == 1:\n",
    "                batch_size = inputs[\"input_ids\"].shape[0]\n",
    "                labels = labels.view(batch_size, -1)\n",
    "\n",
    "        return loss, generated_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4597f06e0c9b4268b6dd9fe5d879c6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,320,000 || all params: 3,237,075,968 || trainable%: 0.7513\n"
     ]
    }
   ],
   "source": [
    "# Your original QLoRA loading code\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16 if config.precision == \"fp16\" else torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_id, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=2,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_rank,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.lora_target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average token length for ...\n",
      "📊 Average token length in training set: 134.06\n",
      "👉 Recommendation: Set config.max_length to a value like 256 (e.g., 256, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98631e7a77c34aee91e6db05b6c4205b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/9427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2265d19d424779b06ac0b37833c1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47396d5e16c4fcd9883bbfc52533ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c77aefbe4a94fca8a4426a1ef48bda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final features in training set: {'labels': ClassLabel(names=['no', 'yes']), 'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8'))}\n",
      "🏁 Total training steps: 590\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Dataset Preparation (for google/boolq) ----------------\n",
    "from datasets import load_dataset, Features, ClassLabel\n",
    "# Load the dataset\n",
    "train_dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "val_dataset = load_dataset(config.dataset_name, split=\"validation\")\n",
    "\n",
    "# --- Code to calculate average token length ---\n",
    "print(\"Calculating average token length for ...\")\n",
    "\n",
    "def get_token_length(example):\n",
    "    \"\"\"Simple function to tokenize and return length for dataset.\"\"\"\n",
    "    # Updated to use 'passage' instead of 'sentence'\n",
    "    return {\"length\": len(tokenizer(example['question'], example['passage']).input_ids)}\n",
    "\n",
    "# Calculate lengths on a subset for speed, or the full dataset for accuracy\n",
    "lengths_dataset = train_dataset.map(get_token_length, num_proc=4)\n",
    "\n",
    "average_length = np.mean(lengths_dataset['length'])\n",
    "print(f\"📊 Average token length in training set: {average_length:.2f}\")\n",
    "\n",
    "print(f\"👉 Recommendation: Set config.max_length to a value like {2**int(math.log2(average_length) + 1)} (e.g., 256, 512)\")\n",
    "\n",
    "# 2. Manually rename the 'answer' column to 'labels'\n",
    "train_dataset = train_dataset.rename_column(\"answer\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"answer\", \"labels\")\n",
    "\n",
    "# 3. Define the features and explicitly cast 'labels' to a ClassLabel (integer type)\n",
    "# THIS IS THE CRUCIAL FIX\n",
    "new_features = train_dataset.features.copy()\n",
    "new_features[\"labels\"] = ClassLabel(names=['no', 'yes']) # Corresponds to 0 and 1\n",
    "\n",
    "train_dataset = train_dataset.cast(new_features)\n",
    "val_dataset = val_dataset.cast(new_features)\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    \"\"\"Tokenizer for BoolQ, using 'question' and 'passage' columns.\"\"\"\n",
    "    return tokenizer(\n",
    "        example['question'],\n",
    "        example['passage'],\n",
    "        truncation=True,\n",
    "        max_length=config.max_length # Use a config object for max_length\n",
    "    )\n",
    "\n",
    "# 4. Apply tokenization and remove original text columns\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, remove_columns=['question', 'passage'])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, remove_columns=['question', 'passage'])\n",
    "\n",
    "\n",
    "# Verify the fix by checking the features\n",
    "print(\"✅ Final features in training set:\", tokenized_train_dataset.features)\n",
    "\n",
    "\n",
    "# Compute total training steps\n",
    "train_len = len(tokenized_train_dataset)\n",
    "eff_batch = config.batch_size * config.gradient_accumulation_steps\n",
    "total_steps = math.ceil(train_len / eff_batch) * config.num_epochs\n",
    "print(f\"🏁 Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchandrav\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250807_134620-84q51ebr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/RAAPID/GRIT-Final/runs/84q51ebr' target=\"_blank\">grit-Llama-3.2-3B-boolq</a></strong> to <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/RAAPID/GRIT-Final/runs/84q51ebr' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final/runs/84q51ebr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"grit-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    entity=\"RAAPID\",\n",
    "    project=\"GRIT-Final\",\n",
    "    name=run_name,\n",
    "    job_type=\"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.3.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.34.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the accuracy, F1, and precision metrics from the library\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy, F1, and precision for a binary classification task.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # Convert logits to class predictions\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute each metric\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    # For binary tasks, specify average=\"binary\" to get the score for the positive class (1)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"binary\")\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"binary\")\n",
    "\n",
    "    # Return a dictionary with all metric scores\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"precision\": precision[\"precision\"], # 3. Add precision to the returned dictionary\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing GRITManager on device: cuda\n",
      "🎯 Instrumented 196 modules for GRIT optimization with custom autograd:\n",
      "   • 196 attention modules\n",
      "🚀 Using r-dim (16x16) covariances for maximum memory efficiency.\n",
      "GRITManager: Initialization complete.\n",
      "🔍 Optimizing 196 key LoRA modules.\n",
      "💾 K-FAC matrices stored on CPU for memory efficiency.\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Training Arguments ----------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    run_name=run_name,\n",
    "\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    learning_rate=config.learning_rate,\n",
    "\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=1000,                 # Evaluate less often\n",
    "    logging_steps=50,\n",
    "    #eval_accumulation_steps=1,\n",
    "    #max_steps=200,\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "    #save_steps=250,                 # Match eval_steps for early stopping\n",
    "    save_total_limit=2,\n",
    "\n",
    "    fp16=config.precision == \"fp16\",\n",
    "    bf16=config.precision == \"bf16\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=config.num_workers,\n",
    "    dataloader_pin_memory=config.pin_memory,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"wandb\",\n",
    "    #metric_for_best_model=\"bleu\",\n",
    "    #greater_is_better=True,\n",
    "    #predict_with_generate=True,\n",
    "    \n",
    "    # --- Generation settings for faster evaluation ---\n",
    "    # generation_max_length=config.max_length + 128,\n",
    "    # generation_num_beams=1,\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    #load_best_model_at_end=config.enable_early_stopping,\n",
    "\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"🚀 Initializing GRITManager on device:\", device)\n",
    "grit_manager = GRITManager(model, config, device)\n",
    "\n",
    "# Instantiate the new callback\n",
    "grit_callback = GritCallback(grit_manager)\n",
    "\n",
    "# Let the Trainer use its default high-performance optimizer\n",
    "# The custom 8-bit optimizer is not needed when memory is abundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Parameter Statistics:\n",
      "🔢 Total parameters: 1,827,789,824\n",
      "🔥 Trainable parameters: 24,320,000 (1.331%)\n",
      "📐 LoRA parameters: 24,313,856\n",
      "\n",
      "📍 Layer-wise LoRA Distribution:\n",
      "   Layer 0: 868,352 params\n",
      "   Layer 1: 868,352 params\n",
      "   Layer 2: 868,352 params\n",
      "   Layer 3: 868,352 params\n",
      "   Layer 4: 868,352 params\n",
      "   Layer 5: 868,352 params\n",
      "   Layer 6: 868,352 params\n",
      "   Layer 7: 868,352 params\n",
      "   Layer 8: 868,352 params\n",
      "   Layer 9: 868,352 params\n",
      "   Layer 10: 868,352 params\n",
      "   Layer 11: 868,352 params\n",
      "   Layer 12: 868,352 params\n",
      "   Layer 13: 868,352 params\n",
      "   Layer 14: 868,352 params\n",
      "   Layer 15: 868,352 params\n",
      "   Layer 16: 868,352 params\n",
      "   Layer 17: 868,352 params\n",
      "   Layer 18: 868,352 params\n",
      "   Layer 19: 868,352 params\n",
      "   Layer 20: 868,352 params\n",
      "   Layer 21: 868,352 params\n",
      "   Layer 22: 868,352 params\n",
      "   Layer 23: 868,352 params\n",
      "   Layer 24: 868,352 params\n",
      "   Layer 25: 868,352 params\n",
      "   Layer 26: 868,352 params\n",
      "   Layer 27: 868,352 params\n",
      "\n",
      "🎯 Strategy: LoRA + GRIT applied to layers 0-27\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count and display parameter statistics\"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    lora_params = 0\n",
    "    \n",
    "    layer_params = {}  # Track params by layer\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            if \"lora_\" in name:\n",
    "                lora_params += param.numel()\n",
    "                \n",
    "                # Extract layer number for statistics\n",
    "                for i in range(32):\n",
    "                    if f'layers.{i}.' in name:\n",
    "                        if i not in layer_params:\n",
    "                            layer_params[i] = 0\n",
    "                        layer_params[i] += param.numel()\n",
    "                        break\n",
    "    \n",
    "    print(f\"\\n📊 Parameter Statistics:\")\n",
    "    print(f\"🔢 Total parameters: {total_params:,}\")\n",
    "    print(f\"🔥 Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.3f}%)\")\n",
    "    print(f\"📐 LoRA parameters: {lora_params:,}\")\n",
    "    \n",
    "    print(f\"\\n📍 Layer-wise LoRA Distribution:\")\n",
    "    active_layers = sorted(layer_params.keys())\n",
    "    for layer_id in active_layers:\n",
    "        print(f\"   Layer {layer_id}: {layer_params[layer_id]:,} params\")\n",
    "    \n",
    "    if active_layers:\n",
    "        print(f\"\\n🎯 Strategy: LoRA + GRIT applied to layers {min(active_layers)}-{max(active_layers)}\")\n",
    "\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Call this after model setup\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "if wandb.run:\n",
    "    wandb.config.update({\n",
    "        \"total_model_params\": total_params,\n",
    "        \"lora_trainable_params\": trainable_params,\n",
    "        \"initial_lora_rank_r\": config.lora_rank,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1094/508026746.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `GritTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GritTrainer: Initialized with GRIT implementation.\n",
      "🎯 Effective batch (per step): 16\n",
      "⚡ Mixed precision: bf16\n",
      "📏 Max sequence length: 2048\n",
      "🔧 LoRA rank: 16\n",
      "🚀 Starting training now...\n",
      "🎁 Wrapping the optimizer with GRIT preconditioning logic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='590' max='590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [590/590 29:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.602400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.515200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.525900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRITManager: Inverting K-FAC factors at step 120...\n",
      "\n",
      "GRITManager: Neural reprojection at step 120...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 24,313,856\n",
      "   - Final GRIT params:   22,794,240\n",
      "   - Reduction:           1,519,616 (6.25%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 150 to 200...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 232...\n",
      "\n",
      "GRITManager: Neural reprojection at step 232...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 24,313,856\n",
      "   - Final GRIT params:   21,252,096\n",
      "   - Reduction:           3,061,760 (12.59%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 300 to 350...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 344...\n",
      "\n",
      "GRITManager: Neural reprojection at step 344...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 24,313,856\n",
      "   - Final GRIT params:   19,721,216\n",
      "   - Reduction:           4,592,640 (18.89%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 450 to 500...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 469...\n",
      "\n",
      "GRITManager: Neural reprojection at step 469...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 24,313,856\n",
      "   - Final GRIT params:   18,179,072\n",
      "   - Reduction:           6,134,784 (25.23%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 581...\n",
      "\n",
      "GRITManager: Neural reprojection at step 581...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 24,313,856\n",
      "   - Final GRIT params:   16,648,192\n",
      "   - Reduction:           7,665,664 (31.53%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-6894b511-71821419222353c220464f67;c4b4c0d1-f1a1-4952-b114-5ecbd0c4df8a)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-3B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B to ask for access. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B.\n",
      "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B - will assume that the vocabulary was not modified.\n",
      "  del peft_model_state_dict[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Trainer uses GRITManager for K-FAC steps\n",
    "trainer = GritTrainer(\n",
    "    grit_manager=grit_manager,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[grit_callback],\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)] if config.enable_early_stopping else [],\n",
    ")\n",
    "\n",
    "# Final memory cleanup\n",
    "import gc\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(f\"🎯 Effective batch (per step): {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(\"⚡ Mixed precision:\", config.precision)\n",
    "print(\"📏 Max sequence length:\", config.max_length)\n",
    "print(\"🔧 LoRA rank:\", config.lora_rank)\n",
    "print(\"🚀 Starting training now...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"🎉 Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧹 Clearing VRAM before evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 15:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.mlp.down_proj</td><td>█▅▄▂▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.mlp.down_proj</td><td>█▇▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.mlp.down_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.mlp.gate_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.mlp.up_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.k_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.o_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.q_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.v_proj</td><td>█▆▅▃▁</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.mlp.down_proj</td><td>▁▄▅▇█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.mlp.down_proj</td><td>▁▂▄▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.mlp.down_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.mlp.gate_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.mlp.up_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.k_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.o_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.q_proj</td><td>▁▃▅▆█</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.v_proj</td><td>▁▃▅▆█</td></tr><tr><td>Parameters/GRIT Final Params</td><td>█▆▄▃▁</td></tr><tr><td>Parameters/GRIT Initial Params</td><td>▁▁▁▁▁</td></tr><tr><td>Parameters/GRIT Param Reduction (%)</td><td>▁▃▅▆█</td></tr><tr><td>adaptive_kfac_damping</td><td>▁▁▁▁▁</td></tr><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▄▄▅▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▄▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>▃▃▄▁▃▁▇▂██▂</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▅▅▄▄▃▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.mlp.down_proj</td><td>8</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.mlp.down_proj</td><td>8</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.mlp.down_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.mlp.gate_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.mlp.up_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.k_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.o_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.q_proj</td><td>11</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.v_proj</td><td>11</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.mlp.down_proj</td><td>50</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.mlp.down_proj</td><td>50</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.v_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.mlp.down_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.mlp.gate_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.mlp.up_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.k_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.o_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.q_proj</td><td>31.25</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.v_proj</td><td>31.25</td></tr><tr><td>Parameters/GRIT Final Params</td><td>16648192</td></tr><tr><td>Parameters/GRIT Initial Params</td><td>24313856</td></tr><tr><td>Parameters/GRIT Param Reduction (%)</td><td>31.52796</td></tr><tr><td>adaptive_kfac_damping</td><td>0.001</td></tr><tr><td>eval/accuracy</td><td>0.7104</td></tr><tr><td>eval/f1</td><td>0.788</td></tr><tr><td>eval/loss</td><td>0.5567</td></tr><tr><td>eval/precision</td><td>0.72309</td></tr><tr><td>eval/runtime</td><td>948.3822</td></tr><tr><td>eval/samples_per_second</td><td>3.448</td></tr><tr><td>eval/steps_per_second</td><td>3.448</td></tr><tr><td>total_flos</td><td>3.953551334270362e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>590</td></tr><tr><td>train/grad_norm</td><td>19.07712</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5259</td></tr><tr><td>train_loss</td><td>0.63602</td></tr><tr><td>train_runtime</td><td>1749.7162</td></tr><tr><td>train_samples_per_second</td><td>5.388</td></tr><tr><td>train_steps_per_second</td><td>0.337</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grit-Llama-3.2-3B-boolq</strong> at: <a href='https://wandb.ai/RAAPID/GRIT-Final/runs/84q51ebr' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final/runs/84q51ebr</a><br> View project at: <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final</a><br>Synced 5 W&B file(s), 2940 media file(s), 5880 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
