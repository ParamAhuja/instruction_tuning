{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchao torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable the tokenizer parallelism warning to avoid spam\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_scheduler,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import bitsandbytes as bnb\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from huggingface_hub import login, HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ceb0672d5d4f9b8d257e4fb5775df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFACAutogradFunction(torch.autograd.Function):\n",
    "    \"\"\"Custom autograd function to capture activations and gradients for K-FAC.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, module, output, input):\n",
    "        ctx.module = module\n",
    "        # We only need the input for the backward pass to compute activation stats\n",
    "        ctx.save_for_backward(input.detach())\n",
    "        # We pass the output through, it's what the next layer will see\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_wrt_output):\n",
    "        # This grad is the gradient w.r.t the output of the LoraLayer\n",
    "        module = ctx.module\n",
    "        input, = ctx.saved_tensors # This is the original input to the LoraLayer\n",
    "        manager = module.grit_manager\n",
    "        manager.backward_step += 1\n",
    "\n",
    "        if not module.training:\n",
    "            # Pass gradients through without modification if not training\n",
    "            return None, grad_wrt_output, None\n",
    "            \n",
    "        # --- Run covariance updates periodically to avoid bottlenecking ---\n",
    "        if manager.backward_step % manager.config.grit_cov_update_freq != 0:\n",
    "            return None, grad_wrt_output, None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # --- 1. Project activations into r-space and update covariance ---\n",
    "            # a has shape (batch*seq, in_features)\n",
    "            a = input.reshape(-1, input.shape[-1])\n",
    "            if a.shape[0] > 0 and 'default' in module.lora_A:\n",
    "                # A has shape (r, in_features), so A.T has (in_features, r)\n",
    "                # Cast weight to the same dtype as activation `a` to prevent mismatch\n",
    "                lora_A_T = module.lora_A['default'].weight.data.T.to(device=a.device, dtype=a.dtype, non_blocking=True)\n",
    "                # projected_a has shape (batch*seq, r)\n",
    "                projected_a = a @ lora_A_T\n",
    "                a_cov_sample = projected_a.T @ projected_a\n",
    "                \n",
    "                # Online update for a_covs\n",
    "                current_cov = manager.a_covs[module]\n",
    "                n = manager.num_samples_a[module]\n",
    "                new_n = n + projected_a.shape[0]\n",
    "                if new_n > 0:\n",
    "                    updated_cov = (current_cov.float() * n + a_cov_sample.cpu().float()) / new_n\n",
    "                    manager.a_covs[module].copy_(updated_cov)\n",
    "                    manager.num_samples_a[module] = new_n\n",
    "\n",
    "            # --- 2. Project gradients into r-space and update covariance ---\n",
    "            # g has shape (batch*seq, out_features)\n",
    "            g = grad_wrt_output.reshape(-1, grad_wrt_output.shape[-1])\n",
    "            if g.shape[0] > 0 and 'default' in module.lora_B:\n",
    "                # B has shape (out_features, r)\n",
    "                # Cast weight to the same dtype as gradient `g` to prevent mismatch\n",
    "                lora_B = module.lora_B['default'].weight.data.to(device=g.device, dtype=g.dtype, non_blocking=True)\n",
    "                # projected_g has shape (batch*seq, r)\n",
    "                projected_g = g @ lora_B\n",
    "                g_cov_sample = projected_g.T @ projected_g\n",
    "\n",
    "                # Online update for g_covs\n",
    "                current_cov = manager.g_covs[module]\n",
    "                n = manager.num_samples_g[module]\n",
    "                new_n = n + projected_g.shape[0]\n",
    "                if new_n > 0:\n",
    "                    updated_cov = (current_cov.float() * n + g_cov_sample.cpu().float()) / new_n\n",
    "                    manager.g_covs[module].copy_(updated_cov)\n",
    "                    manager.num_samples_g[module] = new_n\n",
    "\n",
    "        # We return gradients for the inputs of the `forward` method:\n",
    "        # (module, output, input)\n",
    "        # 1. module: Not a tensor, so None\n",
    "        # 2. output: The gradient is `grad_wrt_output`, pass it back to the LoraLayer\n",
    "        # 3. input: This function does not depend on `input` for its output's value,\n",
    "        #    so its gradient contribution w.r.t. `input` is zero.\n",
    "        return None, grad_wrt_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this function outside your GRITManager class.\n",
    "# It only knows how to work with Tensors.\n",
    "def jit_invert_tensor_pair(a_cov: torch.Tensor, g_cov: torch.Tensor, kfac_damping: float):\n",
    "    \"\"\"\n",
    "    JIT-compatible function to invert a SINGLE pair of covariance tensors.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        damping = max(kfac_damping, 1e-6)\n",
    "        \n",
    "        a_cov_damped = a_cov.float() + damping * torch.eye(a_cov.shape[0], device='cpu')\n",
    "        g_cov_damped = g_cov.float() + damping * torch.eye(g_cov.shape[0], device='cpu')\n",
    "        \n",
    "        L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n",
    "        L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n",
    "        \n",
    "        if info_a == 0 and info_g == 0:\n",
    "            a_inv = torch.cholesky_inverse(L_a).half()\n",
    "            g_inv = torch.cholesky_inverse(L_g).half()\n",
    "            return a_inv, g_inv\n",
    "        else:\n",
    "            # Return empty tensors on failure, which we can check for later\n",
    "            return torch.empty(0), torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRITConfig:\n",
    "    \"\"\"Configuration class for GRIT with Unsloth support.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Using a 3B model, optimized for a powerful A100 GPU\n",
    "        self.model_id = \"meta-llama/Llama-3.2-3B\"  # Using Unsloth's pre-optimized model\n",
    "        \n",
    "        # -------- Training Configuration (Now Optimized for Kaggle -> 16GB VRAM) --------\n",
    "        self.batch_size = 16  # Lowered from 8 for memory constraints\n",
    "        self.gradient_accumulation_steps = 4 # Increased from 1 to maintain effective batch size of 8\n",
    "        self.num_epochs = 1\n",
    "        self.learning_rate = 2e-5\n",
    "        self.precision = \"bf16\" # bf16 is still optimal for modern GPUs\n",
    "        self.max_length = 256\n",
    "\n",
    "        # LoRA configuration (optimized for memory)\n",
    "        self.lora_rank = 16   # Start with a higher rank to allow for pruning\n",
    "        self.lora_alpha = 32 # Adjusted for new rank (2 * rank)\n",
    "        self.lora_dropout = 0.0\n",
    "        # 5W heuristic for layer selection: include attention and key MLP layers\n",
    "        self.lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "        # GRIT parameters (tuned for speed vs. quality)\n",
    "        self.kfac_update_freq = 100 # Invert less often\n",
    "        self.kfac_damping = 0.001\n",
    "        self.reprojection_freq = 100\n",
    "        self.reprojection_k = 8\n",
    "        self.grit_cov_update_freq = 15 # Update covs a bit more often with more budget\n",
    "        \n",
    "        # --- Rank-Adaptive LoRA Configuration ---\n",
    "        self.enable_rank_adaptation = True\n",
    "        self.rank_adaptation_threshold = 0.9  # Cumulative energy threshold\n",
    "        self.min_lora_rank = 4                 # Minimum rank to prevent collapse\n",
    "\n",
    "        # --- Convergence Control ---\n",
    "        self.enable_early_stopping = True\n",
    "        self.early_stopping_patience = 3 # Stop after 3 evaluations with no improvement\n",
    "\n",
    "        # Unsloth-specific parameters\n",
    "        self.use_unsloth = False\n",
    "        self.unsloth_max_seq_length = self.max_length\n",
    "        self.unsloth_dtype = torch.bfloat16 if self.precision == \"bf16\" else torch.float16\n",
    "        self.unsloth_load_in_4bit = True  # Use 4-bit quantization\n",
    "        \n",
    "        # Data loading configuration\n",
    "        self.dataset_name = \"nyu-mll/glue\" # Default dataset\n",
    "        self.num_workers = 2 # Reduced from 8 for Kaggle's lower CPU/RAM resources\n",
    "        self.pin_memory = False # This is generally fine\n",
    "        self.drop_last = True\n",
    "\n",
    "config = GRITConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRITManager:\n",
    "    \"\"\"\n",
    "    Memory-efficient GRIT Manager with CPU-based K-FAC storage and selective optimization.\n",
    "    This addresses both performance and memory issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.global_step = 0\n",
    "        self.backward_step = 0\n",
    "        \n",
    "        # Adaptive frequency state\n",
    "        self.loss_history = []\n",
    "        self.loss_history_capacity = 20  # Corresponds to window size in _get_adaptive_freq\n",
    "        self.last_kfac_update_step = 0\n",
    "        self.last_reprojection_step = 0\n",
    "        \n",
    "        # Memory-efficient K-FAC state storage (CPU-based)\n",
    "        self.a_covs = {}  # Input activation covariances (stored on CPU)\n",
    "        self.g_covs = {}  # Output gradient covariances (stored on CPU)\n",
    "        self.a_invs = {}  # Inverted input covariances (CPU)\n",
    "        self.g_invs = {}  # Inverted output covariances (CPU)\n",
    "        \n",
    "        # Online estimation counters\n",
    "        self.num_samples_a = {}\n",
    "        self.num_samples_g = {}\n",
    "\n",
    "        # Track which modules we're optimizing (subset for memory efficiency)\n",
    "        self.optimized_modules = []\n",
    "        \n",
    "        self.factors_are_ready = False\n",
    "        self._instrument_model()\n",
    "        \n",
    "        # Debug: Check how many LoRA layers we found\n",
    "        total_modules = len(self.optimized_modules)\n",
    "        print(f\"GRITManager: Initialization complete.\")\n",
    "        print(f\"🔍 Optimizing {total_modules} key LoRA modules.\")\n",
    "        print(f\"💾 K-FAC matrices stored on CPU for memory efficiency.\")\n",
    "\n",
    "    def _instrument_model(self):\n",
    "        \"\"\"\n",
    "        Replaces forward passes with a version that includes our autograd function.\n",
    "        Optimized to only instrument q_proj modules in the last 8 layers and use r-dim\n",
    "        covariance matrices to significantly reduce CPU RAM usage.\n",
    "        \"\"\"\n",
    "        attention_modules = 0\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, LoraLayer) and module.r['default'] > 0:\n",
    "                module.lora_name = name # Store name for logging\n",
    "                module.grit_manager = self \n",
    "                self.num_samples_a[module] = 0\n",
    "                self.num_samples_g[module] = 0\n",
    "                \n",
    "                # Store original forward method\n",
    "                module.original_forward = module.forward\n",
    "                \n",
    "                # Replace forward method to apply the hook *after* the original forward\n",
    "                def new_forward(self, x):\n",
    "                    y = self.original_forward(x)\n",
    "                    return KFACAutogradFunction.apply(self, y, x)\n",
    "\n",
    "                module.forward = new_forward.__get__(module, LoraLayer)\n",
    "\n",
    "                # --- store covariances in r-dim space (MASSIVE memory saving) ---\n",
    "                r = module.r['default']\n",
    "                self.a_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n",
    "                self.g_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n",
    "                \n",
    "                self.optimized_modules.append(module)\n",
    "                attention_modules += 1\n",
    "        \n",
    "        print(f\"🎯 Instrumented {attention_modules} modules for GRIT optimization with custom autograd:\")\n",
    "        print(f\"   • {attention_modules} attention modules\")\n",
    "        print(f\"🚀 Using r-dim ({self.config.lora_rank}x{self.config.lora_rank}) covariances for maximum memory efficiency.\")\n",
    "\n",
    "    def _get_adaptive_freq(self, base_freq, min_freq=1, max_freq=1000, window=20):\n",
    "        \"\"\"Calculates adaptive frequency based on loss stability.\"\"\"\n",
    "        if len(self.loss_history) < window:\n",
    "            return base_freq\n",
    "        \n",
    "        # Check loss trend over the last `window` steps\n",
    "        recent_losses = self.loss_history[-window:]\n",
    "        first_half = sum(recent_losses[:window//2]) / (window//2)\n",
    "        second_half = sum(recent_losses[window//2:]) / (window//2)\n",
    "        \n",
    "        # If loss is decreasing (learning is stable), decrease frequency (run less often)\n",
    "        if second_half < first_half * 0.99:\n",
    "            new_freq = int(base_freq * 1.5)\n",
    "        # If loss is fluctuating or increasing, increase frequency (run more often)\n",
    "        else:\n",
    "            new_freq = int(base_freq * 0.75)\n",
    "            \n",
    "        return max(min_freq, min(new_freq, max_freq))\n",
    "\n",
    "    def step(self, loss=None):\n",
    "        \"\"\"Called after each optimizer step to manage periodic updates\"\"\"\n",
    "        self.global_step += 1\n",
    "        if loss is not None:\n",
    "            self.loss_history.append(loss)\n",
    "            if len(self.loss_history) > self.loss_history_capacity:\n",
    "                self.loss_history = self.loss_history[-self.loss_history_capacity:]\n",
    "\n",
    "        # --- Second-order damping schedule ---\n",
    "        if len(self.loss_history) > 10: # Need enough history to compute variance\n",
    "            loss_variance = torch.tensor(self.loss_history).var().item()\n",
    "            # Scale damping with variance, with min/max caps\n",
    "            self.config.kfac_damping = max(1e-6, min(0.01, 0.001 + math.sqrt(loss_variance)))\n",
    "            if self.global_step % 100 == 0: # Log periodically\n",
    "                 wandb.log({\"adaptive_kfac_damping\": self.config.kfac_damping})\n",
    "\n",
    "        # Adaptive K-FAC update frequency\n",
    "        kfac_freq = self._get_adaptive_freq(self.config.kfac_update_freq)\n",
    "        if self.global_step - self.last_kfac_update_step >= kfac_freq:\n",
    "            self.update_and_invert_factors()\n",
    "            self.last_kfac_update_step = self.global_step\n",
    "        \n",
    "        # Adaptive neural reprojection frequency\n",
    "        reproj_freq = self._get_adaptive_freq(self.config.reprojection_freq, min_freq=1, max_freq=2000)\n",
    "        if self.global_step - self.last_reprojection_step >= reproj_freq:\n",
    "            self.neural_reprojection()\n",
    "            self.last_reprojection_step = self.global_step\n",
    "\n",
    "    # Inside your GRITManager class\n",
    "    def update_and_invert_factors(self):\n",
    "        print(f\"\\nGRITManager: Inverting K-FAC factors at step {self.global_step}...\")\n",
    "\n",
    "        # JIT script the new, simpler function ONCE outside the loop\n",
    "        scripted_invert_fn = torch.jit.script(jit_invert_tensor_pair)\n",
    "\n",
    "        # The loop over modules stays in regular Python, where module keys are OK\n",
    "        for module in self.optimized_modules:\n",
    "            a_cov = self.a_covs[module]\n",
    "            g_cov = self.g_covs[module]\n",
    "\n",
    "            # Call the JIT function with TENSORS ONLY\n",
    "            a_inv, g_inv = scripted_invert_fn(\n",
    "                a_cov=a_cov,\n",
    "                g_cov=g_cov,\n",
    "                kfac_damping=self.config.kfac_damping\n",
    "            )\n",
    "\n",
    "            # Check if the inversion was successful and update the dictionaries\n",
    "            if a_inv.numel() > 0 and g_inv.numel() > 0:\n",
    "                self.a_invs[module] = a_inv\n",
    "                self.g_invs[module] = g_inv\n",
    "            else:\n",
    "                print(f\"K-FAC inversion failed for a module. Skipping.\")\n",
    "        \n",
    "        self.factors_are_ready = True\n",
    "\n",
    "    def _invert_factors_fn(self):\n",
    "        with torch.no_grad():\n",
    "            for module in self.optimized_modules:\n",
    "                # Ensure damping is non-negative\n",
    "                damping = max(self.config.kfac_damping, 1e-6)\n",
    "                \n",
    "                # --- Invert Activation Covariance (a_cov) ---\n",
    "                a_cov_damped = self.a_covs[module].float() + damping * torch.eye(\n",
    "                    self.a_covs[module].shape[0], device='cpu'\n",
    "                )\n",
    "                \n",
    "                # Use cholesky_ex to check for positive-definiteness (a proxy for invertibility here)\n",
    "                # L_a is the Cholesky factor, info_a is 0 on success\n",
    "                L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n",
    "                \n",
    "                # --- Invert Gradient Covariance (g_cov) ---\n",
    "                g_cov_damped = self.g_covs[module].float() + damping * torch.eye(\n",
    "                    self.g_covs[module].shape[0], device='cpu'\n",
    "                )\n",
    "                L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n",
    "\n",
    "                # Check if both decompositions succeeded\n",
    "                if info_a == 0 and info_g == 0:\n",
    "                    # If successful, compute inverse from Cholesky factor (more stable)\n",
    "                    self.a_invs[module] = torch.cholesky_inverse(L_a).half()\n",
    "                    self.g_invs[module] = torch.cholesky_inverse(L_g).half()\n",
    "                else:\n",
    "                    # This block runs if inversion would have failed\n",
    "                    print(f\"K-FAC inversion check failed for a module. Skipping update.\")\n",
    "                    # You can still log this event if needed, but wandb calls might not be JIT-friendly.\n",
    "                    # It's better to handle logging outside the JIT-compiled function.\n",
    "                    continue\n",
    "\n",
    "    def precondition_gradients(self):\n",
    "        \"\"\"Apply K-FAC preconditioning with efficient CPU-GPU transfers\"\"\"\n",
    "        if not self.factors_are_ready:\n",
    "            return\n",
    "        if self.global_step % self.config.kfac_update_freq == 0:\n",
    "            print(f\"\\nGRITManager: Applying Natural Gradient preconditioner at step {self.global_step} to {self.global_step + 50}...\")\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # Determine correct dtype based on config\n",
    "            dtype = torch.float16 if self.config.precision == \"fp16\" else torch.bfloat16\n",
    "\n",
    "            for module in self.optimized_modules:\n",
    "                if module not in self.a_invs or module not in self.g_invs:\n",
    "                    continue\n",
    "                    \n",
    "                lora_a = module.lora_A['default']\n",
    "                lora_b = module.lora_B['default']\n",
    "                \n",
    "                if (lora_a is None or lora_b is None or \n",
    "                    lora_a.weight.grad is None or lora_b.weight.grad is None):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Move inverse matrices to GPU only when needed, using correct dtype\n",
    "                    a_inv = self.a_invs[module].to(self.device, dtype=dtype)\n",
    "                    g_inv = self.g_invs[module].to(self.device, dtype=dtype)\n",
    "                    \n",
    "                    # --- True Natural Gradient Computation: grad' = G_inv @ grad @ A_inv ---\n",
    "                    \n",
    "                    # Precondition LoRA B gradient (the \"easy\" part)\n",
    "                    # Original grad_b has shape (out, r)\n",
    "                    grad_b = lora_b.weight.grad.to(dtype)\n",
    "                    # Corrected multiplication: (out, r) @ (r, r) -> (out, r)\n",
    "                    preconditioned_b_grad = grad_b @ g_inv\n",
    "                    \n",
    "                    # Precondition LoRA A gradient (the \"hard\" part)\n",
    "                    # Original grad_a has shape (r, in)\n",
    "                    grad_a = lora_a.weight.grad.to(dtype)\n",
    "                    \n",
    "                    # Safety reshape for larger ranks\n",
    "                    r = module.r['default']\n",
    "                    if r > 0:\n",
    "                        grad_a = grad_a.view(r, -1)\n",
    "                    \n",
    "                    # Instead, we apply the preconditioning to each matrix's gradient.\n",
    "                    # grad_a' = A_inv @ grad_a\n",
    "                    # Corrected multiplication: (r, r) @ (r, in) -> (r, in)\n",
    "                    preconditioned_a_grad = a_inv @ grad_a\n",
    "\n",
    "                    # Copy back the preconditioned gradients\n",
    "                    lora_a.weight.grad.copy_(preconditioned_a_grad.to(lora_a.weight.grad.dtype))\n",
    "                    lora_b.weight.grad.copy_(preconditioned_b_grad.to(lora_b.weight.grad.dtype))\n",
    "                    \n",
    "                    # Clean up GPU tensors immediately\n",
    "                    del a_inv, g_inv, grad_a, grad_b, preconditioned_a_grad, preconditioned_b_grad\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Gradient preconditioning failed: {e}\")\n",
    "                    wandb.log({\"preconditioning_error\": 1})\n",
    "                    continue\n",
    "\n",
    "    def neural_reprojection(self):\n",
    "        \"\"\"Perform neural reprojection and log the parameter reduction.\"\"\"\n",
    "        print(f\"\\nGRITManager: Neural reprojection at step {self.global_step}...\")\n",
    "        \n",
    "        initial_params = 0\n",
    "        final_params = 0\n",
    "        log_dict = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # First, calculate initial effective parameters for all GRIT-optimized modules\n",
    "            for module in self.optimized_modules:\n",
    "                if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                    # Effective parameters in LoRA = r * (in_features + out_features)\n",
    "                    initial_params += module.r['default'] * (module.in_features + module.out_features)\n",
    "\n",
    "            for module in self.optimized_modules:\n",
    "                try:\n",
    "                    lora_a = module.lora_A['default']\n",
    "                    lora_b = module.lora_B['default']\n",
    "                    \n",
    "                    if lora_a is None or lora_b is None:\n",
    "                        continue\n",
    "                        \n",
    "                    A = lora_a.weight.data.float()\n",
    "                    B = lora_b.weight.data.float()\n",
    "                    \n",
    "                    r = A.shape[0]  # Current LoRA rank\n",
    "                    k = r # Initialize k to the current rank as a fallback\n",
    "                    \n",
    "                    # Form the r x r covariance matrix M = A @ A.T\n",
    "                    M = A @ A.T\n",
    "                    \n",
    "                    # --- Numerical Stability Check ---\n",
    "                    if torch.isnan(M).any() or torch.isinf(M).any():\n",
    "                        print(f\"WARNING: Covariance matrix M for {module.lora_name} contains NaN/Inf. Skipping reprojection for this module.\")\n",
    "                        log_dict[f\"reprojection_errors/nan_inf/{module.lora_name}\"] = 1\n",
    "                        # If skipped, the rank does not change.\n",
    "                        final_params += r * (module.in_features + module.out_features)\n",
    "                        continue\n",
    "\n",
    "                    # --- Rank Adaptation & Pruning Logic ---\n",
    "                    if self.config.enable_rank_adaptation and r > self.config.min_lora_rank:\n",
    "                        # --- Full Eigendecomposition for Adaptive Rank ---\n",
    "                        try:\n",
    "                            eigenvals, V = torch.linalg.eigh(M)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Eigendecomposition failed for module: {e}\", \"error\")\n",
    "                            log_dict[\"reprojection_errors/eigh_error\"] = log_dict.get(\"reprojection_errors/eigh_error\", 0) + 1\n",
    "                            final_params += r * (module.in_features + module.out_features)\n",
    "                            continue\n",
    "                        \n",
    "                        sorted_indices = torch.argsort(eigenvals, descending=True)\n",
    "                        sorted_eigenvals = eigenvals[sorted_indices]\n",
    "                        V = V[:, sorted_indices]\n",
    "\n",
    "                        total_energy = torch.sum(sorted_eigenvals)\n",
    "                        if total_energy > 1e-6:\n",
    "                            cumulative_energy = torch.cumsum(sorted_eigenvals, dim=0) / total_energy\n",
    "                            k = (cumulative_energy < self.config.rank_adaptation_threshold).sum().item() + 1\n",
    "                        \n",
    "                        k = max(k, self.config.min_lora_rank) # Enforce minimum rank\n",
    "                        rank_reduction_percent = (1 - k / r) * 100 if r > 0 else 0\n",
    "                        \n",
    "                        log_dict[f\"GRIT/Effective Rank (k)/{module.lora_name}\"] = k\n",
    "                        log_dict[f\"GRIT/Rank Reduction (%)/{module.lora_name}\"] = rank_reduction_percent\n",
    "\n",
    "                        # Log the sorted eigenvalue distribution to show energy decay\n",
    "                        # --- Enhanced WandB Logging for Eigen-spectra ---\n",
    "                        # --- Log Data Tables for Manual Plotting ---\n",
    "                        try:\n",
    "                            # Create a shorter name for logging\n",
    "                            short_lora_name = module.lora_name.replace(\"base_model.model.\", \"\")\n",
    "                        \n",
    "                            # 1. Log Eigen value spectra (This part is fine)\n",
    "                            eigen_spectra_list = sorted_eigenvals.cpu().numpy().tolist()\n",
    "                            data_for_table = [[s] for s in eigen_spectra_list]\n",
    "                            table = wandb.Table(data=data_for_table, columns=[\"eigenvalue\"])\n",
    "                            hist_plot = wandb.plot.histogram(table, \"eigenvalue\", title=f\"Eigenvalue Spectrum - {short_lora_name}\")\n",
    "                            log_dict[f\"Charts/Eigenvalue Spectrum/{short_lora_name}\"] = hist_plot\n",
    "                        \n",
    "                            # 2. Log ALL (Original) Eigenvectors\n",
    "                            V_all_T = V.T.cpu().numpy()  # <-- CORRECT: Use the full V matrix\n",
    "                            heatmap_data_all = []\n",
    "                            for x, eigenvector in enumerate(V_all_T):\n",
    "                                for y, component in enumerate(eigenvector):\n",
    "                                    heatmap_data_all.append([x, y, component])\n",
    "                            heatmap_table_all = wandb.Table(\n",
    "                                data=heatmap_data_all,\n",
    "                                columns=[\"eigenvector_idx\", \"component_idx\", \"value\"]\n",
    "                            )\n",
    "                            log_dict[f\"Data_Tables/Eigenvectors_All/{short_lora_name}\"] = heatmap_table_all\n",
    "                        \n",
    "                            # 3. Log the CHOSEN TOP-K eigenvectors\n",
    "                            V_k_T = V[:, :k].T.cpu().numpy() # This is correct for the chosen vectors\n",
    "                            heatmap_data_chosen = []\n",
    "                            for x, eigenvector in enumerate(V_k_T):\n",
    "                                for y, component in enumerate(eigenvector):\n",
    "                                    heatmap_data_chosen.append([x, y, component])\n",
    "                            heatmap_table_chosen = wandb.Table(\n",
    "                                data=heatmap_data_chosen,\n",
    "                                columns=[\"eigenvector_idx\", \"component_idx\", \"value\"]\n",
    "                            )\n",
    "                            log_dict[f\"Data_Tables/Eigenvectors_Chosen_TopK/{short_lora_name}\"] = heatmap_table_chosen\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"Data logging for {module.lora_name} failed: {e}\")\n",
    "                        \n",
    "                        V_k = V[:, :k]\n",
    "                    else:\n",
    "                        # --- Fixed Rank Reprojection ---\n",
    "                        k = min(self.config.reprojection_k, r)\n",
    "                        try:\n",
    "                            if k < r and k > 0:\n",
    "                                _, V_k = torch.lobpcg(lambda v: M @ v, X=torch.randn(r, k, device=M.device, dtype=M.dtype), k=k, largest=True)\n",
    "                            else:\n",
    "                                _, V = torch.linalg.eigh(M)\n",
    "                                eigenvals, _ = torch.linalg.eigh(M)\n",
    "                                V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n",
    "                        except Exception as e:\n",
    "                            print(f\"LOBPCG failed for module, falling back to eigh: {e}\")\n",
    "                            _, V = torch.linalg.eigh(M)\n",
    "                            eigenvals, _ = torch.linalg.eigh(M)\n",
    "                            V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n",
    "                    \n",
    "                    # Accumulate the new effective parameter count for this module\n",
    "                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                        final_params += k * (module.in_features + module.out_features)\n",
    "\n",
    "                    # --- Project onto the top-k eigenvectors and prune via zeroing ---\n",
    "                    A_proj = V_k.T @ A\n",
    "                    B_proj = B @ V_k\n",
    "\n",
    "                    A_new, B_new = torch.zeros_like(A), torch.zeros_like(B)\n",
    "                    A_new[:k, :], B_new[:, :k] = A_proj, B_proj\n",
    "\n",
    "                    lora_a.weight.data.copy_(A_new.to(A.dtype))\n",
    "                    lora_b.weight.data.copy_(B_new.to(B.dtype))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Neural reprojection failed for module {module.lora_name}: {e}\")\n",
    "                    log_dict[\"reprojection_errors/general_error\"] = log_dict.get(\"reprojection_errors/general_error\", 0) + 1\n",
    "                    # On failure, assume rank does not change for this module\n",
    "                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                        final_params += module.r['default'] * (module.in_features + module.out_features)\n",
    "                    continue\n",
    "\n",
    "        # --- Log the final results ---\n",
    "        if initial_params > 0:\n",
    "            param_reduction = initial_params - final_params\n",
    "            reduction_percent = (param_reduction / initial_params) * 100\n",
    "            print(f\"✅ Neural reprojection completed. Effective parameter count reduced.\")\n",
    "            print(f\"   - Initial GRIT params: {initial_params:,}\")\n",
    "            print(f\"   - Final GRIT params:   {final_params:,}\")\n",
    "            print(f\"   - Reduction:           {param_reduction:,} ({reduction_percent:.2f}%)\")\n",
    "            \n",
    "            log_dict.update({\n",
    "                \"Parameters/GRIT Initial Params\": initial_params,\n",
    "                \"Parameters/GRIT Final Params\": final_params,\n",
    "                \"Parameters/GRIT Param Reduction (%)\": reduction_percent,\n",
    "            })\n",
    "        else:\n",
    "            print(f\"✅ Neural reprojection completed for {len(self.optimized_modules)} modules.\")\n",
    "        \n",
    "        if log_dict:\n",
    "            wandb.log(log_dict, step=self.global_step)\n",
    "        \n",
    "        # Strategic memory cleanup after expensive operations\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class GritOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    A wrapper for a PyTorch optimizer that applies GRIT's K-FAC gradient\n",
    "    preconditioning before the actual optimization step. This ensures that the\n",
    "    optimizer uses the natural gradient instead of the standard gradient.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer: Optimizer, grit_manager: 'GRITManager'):\n",
    "        self.optimizer = optimizer\n",
    "        self.grit_manager = grit_manager\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.optimizer.state\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "\n",
    "    @param_groups.setter\n",
    "    def param_groups(self, value):\n",
    "        self.optimizer.param_groups = value\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        1.  Applies K-FAC preconditioning to the accumulated gradients.\n",
    "        2.  Calls the underlying optimizer's step function.\n",
    "        \"\"\"\n",
    "        # Apply K-FAC preconditioning to the accumulated gradients\n",
    "        if self.grit_manager.factors_are_ready:\n",
    "            self.grit_manager.precondition_gradients()\n",
    "\n",
    "        # Call the underlying optimizer's step function\n",
    "        self.optimizer.step(closure)\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        self.optimizer.zero_grad(set_to_none=set_to_none)\n",
    "\n",
    "    def add_param_group(self, param_group: dict):\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"GritOptimizer({self.optimizer.__repr__()})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GritCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    This callback injects GRIT's logic into the training loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, grit_manager):\n",
    "        self.grit_manager = grit_manager\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Triggered at the end of each training step.\n",
    "        \"\"\"\n",
    "        last_loss = state.log_history[-1].get(\"loss\") if state.log_history else None\n",
    "        self.grit_manager.step(loss=last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GritTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Optimized GRIT Trainer that addresses the core performance issues and\n",
    "    correctly applies gradient preconditioning by wrapping the optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grit_manager, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.grit_manager = grit_manager\n",
    "        print(\"GritTrainer: Initialized with GRIT implementation.\")\n",
    "\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        \"\"\"\n",
    "        Overrides the base method to wrap the created optimizer with our\n",
    "        GritOptimizer, which handles gradient preconditioning.\n",
    "        \"\"\"\n",
    "        super().create_optimizer_and_scheduler(num_training_steps)\n",
    "\n",
    "        if self.optimizer is not None:\n",
    "            print(\"🎁 Wrapping the optimizer with GRIT preconditioning logic.\")\n",
    "            self.optimizer = GritOptimizer(self.optimizer, self.grit_manager)\n",
    "\n",
    "    # --- THIS IS THE CORRECTED METHOD ---\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        The training step's signature is now aligned with the parent class.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "        \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        \"\"\"Overrides the default evaluate method to add aggressive memory cleanup.\"\"\"\n",
    "        print(\"\\n🧹 Clearing VRAM before evaluation...\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return super().evaluate(*args, **kwargs)\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "\n",
    "        has_labels = \"labels\" in inputs\n",
    "        if has_labels:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        _, generated_tokens, _ = super().prediction_step(\n",
    "            model, inputs, prediction_loss_only, ignore_keys\n",
    "        )\n",
    "\n",
    "        if has_labels:\n",
    "            inputs[\"labels\"] = labels\n",
    "\n",
    "        loss = None\n",
    "        if has_labels:\n",
    "            with torch.no_grad():\n",
    "                loss = self.compute_loss(model, inputs.copy()).detach()\n",
    "\n",
    "        if generated_tokens is not None and type(generated_tokens).__name__ == 'EmptyLogits':\n",
    "            batch_size = inputs[\"input_ids\"].shape[0]\n",
    "            seq_length = labels.shape[1] if labels is not None else inputs[\"input_ids\"].shape[1]\n",
    "            vocab_size = self.model.config.vocab_size\n",
    "\n",
    "            generated_tokens = torch.zeros(\n",
    "                (batch_size, seq_length, vocab_size),\n",
    "                device=self.accelerator.device,\n",
    "                dtype=config.unsloth_dtype\n",
    "            )\n",
    "\n",
    "        if labels is not None:\n",
    "            if len(labels.shape) == 3:\n",
    "                labels = torch.argmax(labels, dim=-1)\n",
    "            elif len(labels.shape) == 1:\n",
    "                batch_size = inputs[\"input_ids\"].shape[0]\n",
    "                labels = labels.view(batch_size, -1)\n",
    "\n",
    "        return loss, generated_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6856b904d30346b0a7bd9dc98b449080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,181,184 || all params: 3,221,937,152 || trainable%: 0.2850\n"
     ]
    }
   ],
   "source": [
    "# Your original QLoRA loading code\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16 if config.precision == \"fp16\" else torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_id, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=2,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "target_modules = []\n",
    "for i in range(0, 32):\n",
    "    for module in config.lora_target_modules:\n",
    "        if \"proj\" in module:  # attention modules\n",
    "            target_modules.append(f\"model.layers.{i}.self_attn.{module}\")\n",
    "        else:  # MLP modules  \n",
    "            target_modules.append(f\"model.layers.{i}.mlp.{module}\")\n",
    "            \n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_rank,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average token length...\n",
      "📊 Average token length in training set: 49.58\n",
      "👉 Recommendation: Set config.max_length to a value like 64 (e.g., 128, 256)\n",
      "🏁 Total training steps: 1637\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Dataset Preparation ----------------\n",
    "\n",
    "train_dataset = load_dataset(config.dataset_name, 'qnli', split=\"train\")\n",
    "val_dataset = load_dataset(config.dataset_name, 'qnli', split=\"validation\")\n",
    "\n",
    "# --- Code to calculate average token length ---\n",
    "print(\"Calculating average token length...\")\n",
    "\n",
    "def get_token_length(example):\n",
    "    \"\"\"Simple function to tokenize and return length.\"\"\"\n",
    "    return {\"length\": len(tokenizer(example['question'], example['sentence']).input_ids)}\n",
    "\n",
    "# Calculate lengths on a subset for speed, or the full dataset for accuracy\n",
    "lengths_dataset = train_dataset.map(get_token_length, num_proc=4)\n",
    "\n",
    "average_length = np.mean(lengths_dataset['length'])\n",
    "print(f\"📊 Average token length in training set: {average_length:.2f}\")\n",
    "print(f\"👉 Recommendation: Set config.max_length to a value like {2**int(math.log2(average_length) + 1)} (e.g., 128, 256)\")\n",
    "# --- End of token length calculation ---\n",
    "\n",
    "def tokenize(example):\n",
    "    \"\"\" Tokenizer for QNLI, using 'question' and 'sentence' columns. \"\"\"\n",
    "    return tokenizer(\n",
    "        example['question'],\n",
    "        example['sentence'],\n",
    "        truncation=True,\n",
    "        max_length=config.max_length\n",
    "    )\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, remove_columns=['question', 'sentence', 'idx'])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, remove_columns=['question', 'sentence', 'idx'])\n",
    "\n",
    "\n",
    "# Compute total training steps\n",
    "train_len = len(tokenized_train_dataset)\n",
    "eff_batch = config.batch_size * config.gradient_accumulation_steps\n",
    "total_steps = math.ceil(train_len / eff_batch) * config.num_epochs\n",
    "print(f\"🏁 Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchandrav\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250803_161947-e47uj1k6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/RAAPID/GRIT-Final/runs/e47uj1k6' target=\"_blank\">grit-Llama-3.2-3B-glue-QNLI</a></strong> to <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/RAAPID/GRIT-Final/runs/e47uj1k6' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final/runs/e47uj1k6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_name = f\"grit-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}-QNLI\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    entity=\"RAAPID\",\n",
    "    project=\"GRIT-Final\",\n",
    "    name=run_name,\n",
    "    job_type=\"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load the accuracy and F1 metrics from the library\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    # *** KEY CHANGE: Use average=\"binary\" for F1 score in a two-class task ***\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"binary\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing GRITManager on device: cuda\n",
      "🎯 Instrumented 112 modules for GRIT optimization with custom autograd:\n",
      "   • 112 attention modules\n",
      "🚀 Using r-dim (16x16) covariances for maximum memory efficiency.\n",
      "GRITManager: Initialization complete.\n",
      "🔍 Optimizing 112 key LoRA modules.\n",
      "💾 K-FAC matrices stored on CPU for memory efficiency.\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Training Arguments ----------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    run_name=run_name,\n",
    "\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    learning_rate=config.learning_rate,\n",
    "\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=1000,                 # Evaluate less often\n",
    "    logging_steps=200,\n",
    "    #eval_accumulation_steps=1,\n",
    "    #max_steps=200,\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "    #save_steps=250,                 # Match eval_steps for early stopping\n",
    "    save_total_limit=2,\n",
    "\n",
    "    fp16=config.precision == \"fp16\",\n",
    "    bf16=config.precision == \"bf16\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=config.num_workers,\n",
    "    dataloader_pin_memory=config.pin_memory,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    report_to=\"wandb\",\n",
    "    #metric_for_best_model=\"bleu\",\n",
    "    #greater_is_better=True,\n",
    "    #predict_with_generate=True,\n",
    "    \n",
    "    # --- Generation settings for faster evaluation ---\n",
    "    # generation_max_length=config.max_length + 128,\n",
    "    # generation_num_beams=1,\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    #load_best_model_at_end=config.enable_early_stopping,\n",
    "\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"🚀 Initializing GRITManager on device:\", device)\n",
    "grit_manager = GRITManager(model, config, device)\n",
    "\n",
    "# Instantiate the new callback\n",
    "grit_callback = GritCallback(grit_manager)\n",
    "\n",
    "# Let the Trainer use its default high-performance optimizer\n",
    "# The custom 8-bit optimizer is not needed when memory is abundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Parameter Statistics:\n",
      "🔢 Total parameters: 1,812,651,008\n",
      "🔥 Trainable parameters: 9,181,184 (0.507%)\n",
      "📐 LoRA parameters: 9,175,040\n",
      "\n",
      "📍 Layer-wise LoRA Distribution:\n",
      "   Layer 0: 327,680 params\n",
      "   Layer 1: 327,680 params\n",
      "   Layer 2: 327,680 params\n",
      "   Layer 3: 327,680 params\n",
      "   Layer 4: 327,680 params\n",
      "   Layer 5: 327,680 params\n",
      "   Layer 6: 327,680 params\n",
      "   Layer 7: 327,680 params\n",
      "   Layer 8: 327,680 params\n",
      "   Layer 9: 327,680 params\n",
      "   Layer 10: 327,680 params\n",
      "   Layer 11: 327,680 params\n",
      "   Layer 12: 327,680 params\n",
      "   Layer 13: 327,680 params\n",
      "   Layer 14: 327,680 params\n",
      "   Layer 15: 327,680 params\n",
      "   Layer 16: 327,680 params\n",
      "   Layer 17: 327,680 params\n",
      "   Layer 18: 327,680 params\n",
      "   Layer 19: 327,680 params\n",
      "   Layer 20: 327,680 params\n",
      "   Layer 21: 327,680 params\n",
      "   Layer 22: 327,680 params\n",
      "   Layer 23: 327,680 params\n",
      "   Layer 24: 327,680 params\n",
      "   Layer 25: 327,680 params\n",
      "   Layer 26: 327,680 params\n",
      "   Layer 27: 327,680 params\n",
      "\n",
      "🎯 Strategy: LoRA + GRIT applied to layers 0-27\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count and display parameter statistics\"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    lora_params = 0\n",
    "    \n",
    "    layer_params = {}  # Track params by layer\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            if \"lora_\" in name:\n",
    "                lora_params += param.numel()\n",
    "                \n",
    "                # Extract layer number for statistics\n",
    "                for i in range(32):\n",
    "                    if f'layers.{i}.' in name:\n",
    "                        if i not in layer_params:\n",
    "                            layer_params[i] = 0\n",
    "                        layer_params[i] += param.numel()\n",
    "                        break\n",
    "    \n",
    "    print(f\"\\n📊 Parameter Statistics:\")\n",
    "    print(f\"🔢 Total parameters: {total_params:,}\")\n",
    "    print(f\"🔥 Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.3f}%)\")\n",
    "    print(f\"📐 LoRA parameters: {lora_params:,}\")\n",
    "    \n",
    "    print(f\"\\n📍 Layer-wise LoRA Distribution:\")\n",
    "    active_layers = sorted(layer_params.keys())\n",
    "    for layer_id in active_layers:\n",
    "        print(f\"   Layer {layer_id}: {layer_params[layer_id]:,} params\")\n",
    "    \n",
    "    if active_layers:\n",
    "        print(f\"\\n🎯 Strategy: LoRA + GRIT applied to layers {min(active_layers)}-{max(active_layers)}\")\n",
    "\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Call this after model setup\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "if wandb.run:\n",
    "    wandb.config.update({\n",
    "        \"total_model_params\": total_params,\n",
    "        \"lora_trainable_params\": trainable_params,\n",
    "        \"initial_lora_rank_r\": config.lora_rank,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_705/508026746.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `GritTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GritTrainer: Initialized with GRIT implementation.\n",
      "🎯 Effective batch (per step): 64\n",
      "⚡ Mixed precision: bf16\n",
      "📏 Max sequence length: 256\n",
      "🔧 LoRA rank: 16\n",
      "🚀 Starting training now...\n",
      "🎁 Wrapping the optimizer with GRIT preconditioning logic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1637' max='1637' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1637/1637 1:36:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.223400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.208500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRITManager: Inverting K-FAC factors at step 100...\n",
      "\n",
      "GRITManager: Neural reprojection at step 100...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   8,601,600\n",
      "   - Reduction:           573,440 (6.25%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 100 to 150...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 200...\n",
      "\n",
      "GRITManager: Neural reprojection at step 200...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   8,028,160\n",
      "   - Reduction:           1,146,880 (12.50%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 200 to 250...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 275...\n",
      "\n",
      "GRITManager: Neural reprojection at step 275...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   7,454,720\n",
      "   - Reduction:           1,720,320 (18.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 300 to 350...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 350...\n",
      "\n",
      "GRITManager: Neural reprojection at step 350...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   6,881,280\n",
      "   - Reduction:           2,293,760 (25.00%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 400 to 450...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 425...\n",
      "\n",
      "GRITManager: Neural reprojection at step 425...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   6,307,840\n",
      "   - Reduction:           2,867,200 (31.25%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 500...\n",
      "\n",
      "GRITManager: Neural reprojection at step 500...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,734,400\n",
      "   - Reduction:           3,440,640 (37.50%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 500 to 550...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 575...\n",
      "\n",
      "GRITManager: Neural reprojection at step 575...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 600 to 650...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 650...\n",
      "\n",
      "GRITManager: Neural reprojection at step 650...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 700 to 750...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 725...\n",
      "\n",
      "GRITManager: Neural reprojection at step 725...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 800...\n",
      "\n",
      "GRITManager: Neural reprojection at step 800...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 800 to 850...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 875...\n",
      "\n",
      "GRITManager: Neural reprojection at step 875...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 900 to 950...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 950...\n",
      "\n",
      "GRITManager: Neural reprojection at step 950...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1000 to 1050...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1025...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1025...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1100...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1100...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1100 to 1150...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1175...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1175...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1200 to 1250...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1250...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1250...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1300 to 1350...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1325...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1325...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1400...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1400...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1400 to 1450...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1475...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1475...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1500 to 1550...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1550...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1550...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 1600 to 1650...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 1625...\n",
      "\n",
      "GRITManager: Neural reprojection at step 1625...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "🎉 Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Trainer uses GRITManager for K-FAC steps\n",
    "trainer = GritTrainer(\n",
    "    grit_manager=grit_manager,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[grit_callback],\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)] if config.enable_early_stopping else [],\n",
    ")\n",
    "\n",
    "# Final memory cleanup\n",
    "import gc\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(f\"🎯 Effective batch (per step): {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(\"⚡ Mixed precision:\", config.precision)\n",
    "print(\"📏 Max sequence length:\", config.max_length)\n",
    "print(\"🔧 LoRA rank:\", config.lora_rank)\n",
    "print(\"🚀 Starting training now...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"🎉 Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧹 Clearing VRAM before evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5463' max='5463' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5463/5463 29:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.k_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.o_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.q_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.v_proj</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>Parameters/GRIT Final Params</td><td>█▇▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters/GRIT Initial Params</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters/GRIT Param Reduction (%)</td><td>▁▂▃▅▆▇███████████████</td></tr><tr><td>adaptive_kfac_damping</td><td>██▁▁▅▅▅▅██▅▅▁▁</td></tr><tr><td>eval/accuracy</td><td>▁</td></tr><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄▁▂▂▇█▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.v_proj</td><td>43.75</td></tr><tr><td>Parameters/GRIT Final Params</td><td>5160960</td></tr><tr><td>Parameters/GRIT Initial Params</td><td>9175040</td></tr><tr><td>Parameters/GRIT Param Reduction (%)</td><td>43.75</td></tr><tr><td>adaptive_kfac_damping</td><td>0.001</td></tr><tr><td>eval/accuracy</td><td>0.93575</td></tr><tr><td>eval/f1</td><td>0.93573</td></tr><tr><td>eval/loss</td><td>0.16641</td></tr><tr><td>eval/runtime</td><td>1764.8342</td></tr><tr><td>eval/samples_per_second</td><td>3.095</td></tr><tr><td>eval/steps_per_second</td><td>3.095</td></tr><tr><td>total_flos</td><td>1.5904338234909696e+17</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>1637</td></tr><tr><td>train/grad_norm</td><td>22.62631</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2085</td></tr><tr><td>train_loss</td><td>0.25273</td></tr><tr><td>train_runtime</td><td>5775.4816</td></tr><tr><td>train_samples_per_second</td><td>18.136</td></tr><tr><td>train_steps_per_second</td><td>0.283</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grit-Llama-3.2-3B-glue-QNLI</strong> at: <a href='https://wandb.ai/RAAPID/GRIT-Final/runs/e47uj1k6' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final/runs/e47uj1k6</a><br> View project at: <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final</a><br>Synced 5 W&B file(s), 7056 media file(s), 14112 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Uploading fine-tuned model to Hugging Face Hub...\n",
      "\n",
      "🚀 Uploading fine-tuned model to Hugging Face Hub...\n",
      "📝 Model card created with training details.\n",
      "🌐 Uploading model to https://huggingface.co/te4bag/GRIT-Full-GLUE-QNLI-llama-3.2-3B-Energy-0.9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py:9696: UserWarning: Warnings while validating metadata in README.md:\n",
      "- The pipeline tag \"Sequence Classification\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, audio-text-to-text, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-ranking, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, video-text-to-text, keypoint-detection, visual-document-retrieval, any-to-any, video-to-video, other\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000c2ce1ad4b4732bedb5b7f6805b578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8f7ffa597c46d4bbfdd3a353344177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e361cff9c0684182851f043f90cf713b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /grit_trained_model/tokenizer.json    : 100%|##########| 17.2MB / 17.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab8ae890f4e42e3866d193d61261f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ned_model/adapter_model.safetensors:   0%|          | 26.1kB / 36.8MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model successfully uploaded: https://huggingface.co/te4bag/GRIT-Full-GLUE-QNLI-llama-3.2-3B-Energy-0.9\n",
      "🎉 GRIT fine-tuning script completed!\n"
     ]
    }
   ],
   "source": [
    "# === Hugging Face Upload ===\n",
    "from huggingface_hub import create_repo, upload_folder, HfApi\n",
    "\n",
    "print(\"\\n🚀 Uploading fine-tuned model to Hugging Face Hub...\")\n",
    "\n",
    "# === Hugging Face Upload ===\n",
    "print(\"\\n🚀 Uploading fine-tuned model to Hugging Face Hub...\")\n",
    "\n",
    "HF_USERNAME = \"te4bag\" # Replace with your Hugging Face username\n",
    "FULL_MODEL_NAME = f\"{HF_USERNAME}/GRIT-Full-GLUE-QNLI-llama-3.2-3B-Energy-0.9\"\n",
    "\n",
    "try:\n",
    "    # Save model and tokenizer locally first\n",
    "    output_dir = \"./grit_trained_model\"\n",
    "    model.save_pretrained(output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Create/update model card (with hardware and hyperparams)\n",
    "    model_card_content = f\"\"\"---\n",
    "tags:\n",
    "- llama\n",
    "- Natural Language Inference\n",
    "- grit\n",
    "- lora\n",
    "- qlora\n",
    "- unsloth\n",
    "- instruction-tuning\n",
    "- fine-tuned\n",
    "base_model: {config.model_id}\n",
    "library_name: peft\n",
    "license: apache-2.0\n",
    "datasets:\n",
    "- {config.dataset_name}\n",
    "language:\n",
    "- en\n",
    "pipeline_tag: Sequence Classification\n",
    "---\n",
    "\n",
    "# {config.model_id} Fine-tuned with GRIT and QLoRA\n",
    "\n",
    "This model is a fine-tuned version of [{config.model_id}](https://huggingface.co/{config.model_id}) using the **GRIT** (Geometric Reprojection Instruction Tuning) algorithm and **QLoRA** on the [{config.dataset_name} dataset](https://huggingface.co/datasets/{config.dataset_name}).\n",
    "\n",
    "The base model is quantized to 4-bit (NF4) to enable efficient fine-tuning.\n",
    "\n",
    "## 🚀 Training Details\n",
    "\n",
    "### GRIT Algorithm\n",
    "- **K-FAC Updates**: Every {config.kfac_update_freq} steps (adaptive) for second-order preconditioning.\n",
    "- **Neural Reprojection**: Every {config.reprojection_freq} steps (adaptive) for rank optimization.\n",
    "- **Rank Adaptation**: {'Enabled' if config.enable_rank_adaptation else 'Disabled'} (Threshold: {config.rank_adaptation_threshold}, Min Rank: {config.min_lora_rank}).\n",
    "- **Optimized LoRA Modules**: {config.lora_target_modules}\n",
    "\n",
    "### Fine-tuning Configuration\n",
    "- **Base Model**: {config.model_id}\n",
    "- **Quantization**: 4-bit (NF4) with {config.precision} compute.\n",
    "- **LoRA Rank**: {config.lora_rank}\n",
    "- **LoRA Alpha**: {config.lora_alpha}\n",
    "- **Batch Size**: {config.batch_size} (per device)\n",
    "- **Gradient Accumulation**: {config.gradient_accumulation_steps} (Effective batch = {config.batch_size * config.gradient_accumulation_steps})\n",
    "- **Learning Rate**: {config.learning_rate:.1e}\n",
    "- **Precision**: {config.precision} mixed precision\n",
    "- **Sequence Length**: {config.max_length} tokens\n",
    "- **Gradient Checkpointing**: Enabled\n",
    "\n",
    "### Performance Improvements\n",
    "- ✅ **Faster Convergence**: K-FAC preconditioning aligns updates with curvature.\n",
    "- ✅ **Memory-Efficient**: 4-bit quantization (QLoRA) and gradient checkpointing used.\n",
    "- ✅ **Adaptive Rank**: Dynamically prunes LoRA rank to improve parameter efficiency.\n",
    "\n",
    "## 📊 Training Metrics\n",
    "- **Total Steps**: {trainer.state.global_step if 'trainer' in locals() else 'N/A'}\n",
    "- **Final Loss**: {trainer.state.log_history[-1].get('train_loss', 'N/A') if 'trainer' in locals() and trainer.state.log_history else 'N/A'}\n",
    "- **Trainable Params**: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\n",
    "\n",
    "## 📝 Algorithm Details\n",
    "- **K-FAC Preconditioning** (Natural Gradient) and **Neural Reprojection** as per GRIT method.\n",
    "- **Memory Efficient**: Covariance matrices on CPU to reduce GPU load.\n",
    "\n",
    "## 🏆 Results\n",
    "In benchmark comparisons, GRIT has shown **faster convergence and better stability** than standard LoRA or fine-tuning, making it well-suited for efficient single-epoch training. The use of Unsloth further accelerates this process.\n",
    "\n",
    "## 📝 Citation\n",
    "If you use this model, please cite the original GRIT paper and:\n",
    "```bibtex\n",
    "@misc{{grit-lora-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}}},\n",
    "  title={{ {config.model_id} Fine-tuned with GRIT on {config.dataset_name} }},\n",
    "  author={{{HF_USERNAME}}},\n",
    "  year={{2024}},\n",
    "  publisher={{Hugging Face}},\n",
    "  url={{https://huggingface.co/{FULL_MODEL_NAME}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## ⚖️ License\n",
    "This model inherits the Apache 2.0 license.\n",
    "\"\"\"\n",
    "    with open(f\"{output_dir}/README.md\", \"w\") as f:\n",
    "        f.write(model_card_content)\n",
    "    print(\"📝 Model card created with training details.\")\n",
    "\n",
    "    # Create repository on the Hub and upload the folder\n",
    "    print(f\"🌐 Uploading model to https://huggingface.co/{FULL_MODEL_NAME} ...\")\n",
    "    create_repo(FULL_MODEL_NAME, exist_ok=True)\n",
    "    api = HfApi()\n",
    "    api.upload_folder(\n",
    "        folder_path=output_dir,\n",
    "        repo_id=FULL_MODEL_NAME,\n",
    "        commit_message=f\"GRIT fine-tuned {config.model_id.split('/')[-1]} on {config.dataset_name.split('/')[-1]}\"\n",
    "    )\n",
    "    print(f\"✅ Model successfully uploaded: https://huggingface.co/{FULL_MODEL_NAME}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Upload failed: {e}\\\\n(Model saved locally in {output_dir})\")\n",
    "\n",
    "print(\"🎉 GRIT fine-tuning script completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
