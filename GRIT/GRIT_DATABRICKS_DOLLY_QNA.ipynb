{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.33.4)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.53.3)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub) (2025.7.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.46.1)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bitsandbytes) (2.7.1+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==3.3.1->torch<3,>=2.2->bitsandbytes) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.9.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (2.7.1+cu128)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==3.3.1->torch>=2.0.0->accelerate) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (2.7.1+cu128)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (4.53.3)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (1.9.0)\n",
      "Requirement already satisfied: safetensors in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (0.33.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==3.3.1->torch>=1.13.0->peft) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.7.14)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers->peft) (0.21.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchao in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: torchtune in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.6.1)\n",
      "Requirement already satisfied: torchdata==0.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (0.11.0)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (0.33.4)\n",
      "Requirement already satisfied: safetensors in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (0.5.3)\n",
      "Requirement already satisfied: kagglehub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (0.3.12)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (0.9.0)\n",
      "Requirement already satisfied: blobfile>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (3.0.0)\n",
      "Requirement already satisfied: tokenizers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (0.21.2)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (4.67.1)\n",
      "Requirement already satisfied: omegaconf in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (2.3.0)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (7.0.0)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchtune) (11.3.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchdata==0.11.0->torchtune) (2.5.0)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchdata==0.11.0->torchtune) (2.32.4)\n",
      "Requirement already satisfied: torch>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchdata==0.11.0->torchtune) (2.7.1+cu128)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from blobfile>=2->torchtune) (3.23.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from blobfile>=2->torchtune) (6.0.0)\n",
      "Requirement already satisfied: filelock>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from blobfile>=2->torchtune) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==3.3.1->torch>=2->torchdata==0.11.0->torchtune) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2->torchdata==0.11.0->torchtune) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->torchtune) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->torchtune) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->torchtune) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (3.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub[hf_transfer]->torchtune) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torchdata==0.11.0->torchtune) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torchdata==0.11.0->torchtune) (2025.7.14)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=2->torchdata==0.11.0->torchtune) (3.0.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from omegaconf->torchtune) (4.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tiktoken->torchtune) (2024.11.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchao torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.23.4)\n",
      "Requirement already satisfied: pydantic<3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.32.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable the tokenizer parallelism warning to avoid spam\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    get_scheduler,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import wandb\n",
    "import bitsandbytes as bnb\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from huggingface_hub import login, HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9820f8fb454504a784f5a1c1ab9be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFACAutogradFunction(torch.autograd.Function):\n",
    "    \"\"\"Custom autograd function to capture activations and gradients for K-FAC.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, module, output, input):\n",
    "        ctx.module = module\n",
    "        # We only need the input for the backward pass to compute activation stats\n",
    "        ctx.save_for_backward(input.detach())\n",
    "        # We pass the output through, it's what the next layer will see\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_wrt_output):\n",
    "        # This grad is the gradient w.r.t the output of the LoraLayer\n",
    "        module = ctx.module\n",
    "        input, = ctx.saved_tensors # This is the original input to the LoraLayer\n",
    "        manager = module.grit_manager\n",
    "        manager.backward_step += 1\n",
    "\n",
    "        if not module.training:\n",
    "            # Pass gradients through without modification if not training\n",
    "            return None, grad_wrt_output, None\n",
    "            \n",
    "        # --- Run covariance updates periodically to avoid bottlenecking ---\n",
    "        if manager.backward_step % manager.config.grit_cov_update_freq != 0:\n",
    "            return None, grad_wrt_output, None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # --- 1. Project activations into r-space and update covariance ---\n",
    "            # a has shape (batch*seq, in_features)\n",
    "            a = input.reshape(-1, input.shape[-1])\n",
    "            if a.shape[0] > 0 and 'default' in module.lora_A:\n",
    "                # A has shape (r, in_features), so A.T has (in_features, r)\n",
    "                # Cast weight to the same dtype as activation `a` to prevent mismatch\n",
    "                lora_A_T = module.lora_A['default'].weight.data.T.to(device=a.device, dtype=a.dtype, non_blocking=True)\n",
    "                # projected_a has shape (batch*seq, r)\n",
    "                projected_a = a @ lora_A_T\n",
    "                a_cov_sample = projected_a.T @ projected_a\n",
    "                \n",
    "                # Online update for a_covs\n",
    "                current_cov = manager.a_covs[module]\n",
    "                n = manager.num_samples_a[module]\n",
    "                new_n = n + projected_a.shape[0]\n",
    "                if new_n > 0:\n",
    "                    updated_cov = (current_cov.float() * n + a_cov_sample.cpu().float()) / new_n\n",
    "                    manager.a_covs[module].copy_(updated_cov)\n",
    "                    manager.num_samples_a[module] = new_n\n",
    "\n",
    "            # --- 2. Project gradients into r-space and update covariance ---\n",
    "            # g has shape (batch*seq, out_features)\n",
    "            g = grad_wrt_output.reshape(-1, grad_wrt_output.shape[-1])\n",
    "            if g.shape[0] > 0 and 'default' in module.lora_B:\n",
    "                # B has shape (out_features, r)\n",
    "                # Cast weight to the same dtype as gradient `g` to prevent mismatch\n",
    "                lora_B = module.lora_B['default'].weight.data.to(device=g.device, dtype=g.dtype, non_blocking=True)\n",
    "                # projected_g has shape (batch*seq, r)\n",
    "                projected_g = g @ lora_B\n",
    "                g_cov_sample = projected_g.T @ projected_g\n",
    "\n",
    "                # Online update for g_covs\n",
    "                current_cov = manager.g_covs[module]\n",
    "                n = manager.num_samples_g[module]\n",
    "                new_n = n + projected_g.shape[0]\n",
    "                if new_n > 0:\n",
    "                    updated_cov = (current_cov.float() * n + g_cov_sample.cpu().float()) / new_n\n",
    "                    manager.g_covs[module].copy_(updated_cov)\n",
    "                    manager.num_samples_g[module] = new_n\n",
    "\n",
    "        # We return gradients for the inputs of the `forward` method:\n",
    "        # (module, output, input)\n",
    "        # 1. module: Not a tensor, so None\n",
    "        # 2. output: The gradient is `grad_wrt_output`, pass it back to the LoraLayer\n",
    "        # 3. input: This function does not depend on `input` for its output's value,\n",
    "        #    so its gradient contribution w.r.t. `input` is zero.\n",
    "        return None, grad_wrt_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this function outside your GRITManager class.\n",
    "# It only knows how to work with Tensors.\n",
    "def jit_invert_tensor_pair(a_cov: torch.Tensor, g_cov: torch.Tensor, kfac_damping: float):\n",
    "    \"\"\"\n",
    "    JIT-compatible function to invert a SINGLE pair of covariance tensors.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        damping = max(kfac_damping, 1e-6)\n",
    "        \n",
    "        a_cov_damped = a_cov.float() + damping * torch.eye(a_cov.shape[0], device='cpu')\n",
    "        g_cov_damped = g_cov.float() + damping * torch.eye(g_cov.shape[0], device='cpu')\n",
    "        \n",
    "        L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n",
    "        L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n",
    "        \n",
    "        if info_a == 0 and info_g == 0:\n",
    "            a_inv = torch.cholesky_inverse(L_a).half()\n",
    "            g_inv = torch.cholesky_inverse(L_g).half()\n",
    "            return a_inv, g_inv\n",
    "        else:\n",
    "            # Return empty tensors on failure, which we can check for later\n",
    "            return torch.empty(0), torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRITConfig:\n",
    "    \"\"\"Configuration class for GRIT with Unsloth support.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Using a 3B model, optimized for a powerful A100 GPU\n",
    "        self.model_id = \"meta-llama/Llama-3.2-3B\"  # Using Unsloth's pre-optimized model\n",
    "        \n",
    "        # -------- Training Configuration (Now Optimized for Kaggle -> 16GB VRAM) --------\n",
    "        self.batch_size = 8  # Lowered from 8 for memory constraints\n",
    "        self.gradient_accumulation_steps = 4 # Increased from 1 to maintain effective batch size of 8\n",
    "        self.num_epochs = 1\n",
    "        self.learning_rate = 2e-5\n",
    "        self.precision = \"bf16\" # bf16 is still optimal for modern GPUs\n",
    "        self.max_length = 1024\n",
    "\n",
    "        # LoRA configuration (optimized for memory)\n",
    "        self.lora_rank = 16   # Start with a higher rank to allow for pruning\n",
    "        self.lora_alpha = 32 # Adjusted for new rank (2 * rank)\n",
    "        self.lora_dropout = 0.0\n",
    "        # 5W heuristic for layer selection: include attention and key MLP layers\n",
    "        self.lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "        # GRIT parameters (tuned for speed vs. quality)\n",
    "        self.kfac_update_freq = 50 # Invert less often\n",
    "        self.kfac_damping = 0.001\n",
    "        self.reprojection_freq = 50\n",
    "        self.reprojection_k = 8\n",
    "        self.grit_cov_update_freq = 15 # Update covs a bit more often with more budget\n",
    "        \n",
    "        # --- Rank-Adaptive LoRA Configuration ---\n",
    "        self.enable_rank_adaptation = True\n",
    "        self.rank_adaptation_threshold = 0.9  # Cumulative energy threshold\n",
    "        self.min_lora_rank = 4                 # Minimum rank to prevent collapse\n",
    "\n",
    "        # --- Convergence Control ---\n",
    "        self.enable_early_stopping = True\n",
    "        self.early_stopping_patience = 3 # Stop after 3 evaluations with no improvement\n",
    "\n",
    "        # Unsloth-specific parameters\n",
    "        self.use_unsloth = False\n",
    "        self.unsloth_max_seq_length = self.max_length\n",
    "        self.unsloth_dtype = torch.bfloat16 if self.precision == \"bf16\" else torch.float16\n",
    "        self.unsloth_load_in_4bit = True  # Use 4-bit quantization\n",
    "        \n",
    "        # Data loading configuration\n",
    "        self.dataset_name = \"databricks/databricks-dolly-15k\" # Default dataset\n",
    "        self.num_workers = 2 # Reduced from 8 for Kaggle's lower CPU/RAM resources\n",
    "        self.pin_memory = False # This is generally fine\n",
    "        self.drop_last = True\n",
    "\n",
    "config = GRITConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRITManager:\n",
    "    \"\"\"\n",
    "    Memory-efficient GRIT Manager with CPU-based K-FAC storage and selective optimization.\n",
    "    This addresses both performance and memory issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.global_step = 0\n",
    "        self.backward_step = 0\n",
    "        \n",
    "        # Adaptive frequency state\n",
    "        self.loss_history = []\n",
    "        self.loss_history_capacity = 20  # Corresponds to window size in _get_adaptive_freq\n",
    "        self.last_kfac_update_step = 0\n",
    "        self.last_reprojection_step = 0\n",
    "        \n",
    "        # Memory-efficient K-FAC state storage (CPU-based)\n",
    "        self.a_covs = {}  # Input activation covariances (stored on CPU)\n",
    "        self.g_covs = {}  # Output gradient covariances (stored on CPU)\n",
    "        self.a_invs = {}  # Inverted input covariances (CPU)\n",
    "        self.g_invs = {}  # Inverted output covariances (CPU)\n",
    "        \n",
    "        # Online estimation counters\n",
    "        self.num_samples_a = {}\n",
    "        self.num_samples_g = {}\n",
    "\n",
    "        # Track which modules we're optimizing (subset for memory efficiency)\n",
    "        self.optimized_modules = []\n",
    "        \n",
    "        self.factors_are_ready = False\n",
    "        self._instrument_model()\n",
    "        \n",
    "        # Debug: Check how many LoRA layers we found\n",
    "        total_modules = len(self.optimized_modules)\n",
    "        print(f\"GRITManager: Initialization complete.\")\n",
    "        print(f\"🔍 Optimizing {total_modules} key LoRA modules.\")\n",
    "        print(f\"💾 K-FAC matrices stored on CPU for memory efficiency.\")\n",
    "\n",
    "    def _instrument_model(self):\n",
    "        \"\"\"\n",
    "        Replaces forward passes with a version that includes our autograd function.\n",
    "        Optimized to only instrument q_proj modules in the last 8 layers and use r-dim\n",
    "        covariance matrices to significantly reduce CPU RAM usage.\n",
    "        \"\"\"\n",
    "        attention_modules = 0\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, LoraLayer) and module.r['default'] > 0:\n",
    "                # Check if this module is in the last 8 layers\n",
    "                is_last8 = any(f'model.layers.{i}' in name for i in range(0,32)) #(for llama 3b 20-27)\n",
    "\n",
    "                if not is_last8:\n",
    "                    continue\n",
    "\n",
    "                # All LoRA modules in last 8 layers get GRIT optimization\n",
    "                module.lora_name = name # Store name for logging\n",
    "                module.grit_manager = self \n",
    "                self.num_samples_a[module] = 0\n",
    "                self.num_samples_g[module] = 0\n",
    "                \n",
    "                # Store original forward method\n",
    "                module.original_forward = module.forward\n",
    "                \n",
    "                # Replace forward method to apply the hook *after* the original forward\n",
    "                def new_forward(self, x):\n",
    "                    y = self.original_forward(x)\n",
    "                    return KFACAutogradFunction.apply(self, y, x)\n",
    "\n",
    "                module.forward = new_forward.__get__(module, LoraLayer)\n",
    "\n",
    "                # --- store covariances in r-dim space (MASSIVE memory saving) ---\n",
    "                r = module.r['default']\n",
    "                self.a_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n",
    "                self.g_covs[module] = torch.zeros((r, r), device='cpu', dtype=torch.float16)\n",
    "                \n",
    "                self.optimized_modules.append(module)\n",
    "                attention_modules += 1\n",
    "        \n",
    "        print(f\"🎯 Instrumented {attention_modules} modules for GRIT optimization with custom autograd:\")\n",
    "        print(f\"   • {attention_modules} attention modules\")\n",
    "        print(f\"🚀 Using r-dim ({self.config.lora_rank}x{self.config.lora_rank}) covariances for maximum memory efficiency.\")\n",
    "\n",
    "    def _get_adaptive_freq(self, base_freq, min_freq=1, max_freq=1000, window=20):\n",
    "        \"\"\"Calculates adaptive frequency based on loss stability.\"\"\"\n",
    "        if len(self.loss_history) < window:\n",
    "            return base_freq\n",
    "        \n",
    "        # Check loss trend over the last `window` steps\n",
    "        recent_losses = self.loss_history[-window:]\n",
    "        first_half = sum(recent_losses[:window//2]) / (window//2)\n",
    "        second_half = sum(recent_losses[window//2:]) / (window//2)\n",
    "        \n",
    "        # If loss is decreasing (learning is stable), decrease frequency (run less often)\n",
    "        if second_half < first_half * 0.99:\n",
    "            new_freq = int(base_freq * 1.5)\n",
    "        # If loss is fluctuating or increasing, increase frequency (run more often)\n",
    "        else:\n",
    "            new_freq = int(base_freq * 0.75)\n",
    "            \n",
    "        return max(min_freq, min(new_freq, max_freq))\n",
    "\n",
    "    def step(self, loss=None):\n",
    "        \"\"\"Called after each optimizer step to manage periodic updates\"\"\"\n",
    "        self.global_step += 1\n",
    "        if loss is not None:\n",
    "            self.loss_history.append(loss)\n",
    "            if len(self.loss_history) > self.loss_history_capacity:\n",
    "                self.loss_history = self.loss_history[-self.loss_history_capacity:]\n",
    "\n",
    "        # --- Second-order damping schedule ---\n",
    "        if len(self.loss_history) > 10: # Need enough history to compute variance\n",
    "            loss_variance = torch.tensor(self.loss_history).var().item()\n",
    "            # Scale damping with variance, with min/max caps\n",
    "            self.config.kfac_damping = max(1e-6, min(0.01, 0.001 + math.sqrt(loss_variance)))\n",
    "            if self.global_step % 100 == 0: # Log periodically\n",
    "                 wandb.log({\"adaptive_kfac_damping\": self.config.kfac_damping})\n",
    "\n",
    "        # Adaptive K-FAC update frequency\n",
    "        kfac_freq = self._get_adaptive_freq(self.config.kfac_update_freq)\n",
    "        if self.global_step - self.last_kfac_update_step >= kfac_freq:\n",
    "            self.update_and_invert_factors()\n",
    "            self.last_kfac_update_step = self.global_step\n",
    "        \n",
    "        # Adaptive neural reprojection frequency\n",
    "        reproj_freq = self._get_adaptive_freq(self.config.reprojection_freq, min_freq=1, max_freq=2000)\n",
    "        if self.global_step - self.last_reprojection_step >= reproj_freq:\n",
    "            self.neural_reprojection()\n",
    "            self.last_reprojection_step = self.global_step\n",
    "\n",
    "    # Inside your GRITManager class\n",
    "    def update_and_invert_factors(self):\n",
    "        print(f\"\\nGRITManager: Inverting K-FAC factors at step {self.global_step}...\")\n",
    "\n",
    "        # JIT script the new, simpler function ONCE outside the loop\n",
    "        scripted_invert_fn = torch.jit.script(jit_invert_tensor_pair)\n",
    "\n",
    "        # The loop over modules stays in regular Python, where module keys are OK\n",
    "        for module in self.optimized_modules:\n",
    "            a_cov = self.a_covs[module]\n",
    "            g_cov = self.g_covs[module]\n",
    "\n",
    "            # Call the JIT function with TENSORS ONLY\n",
    "            a_inv, g_inv = scripted_invert_fn(\n",
    "                a_cov=a_cov,\n",
    "                g_cov=g_cov,\n",
    "                kfac_damping=self.config.kfac_damping\n",
    "            )\n",
    "\n",
    "            # Check if the inversion was successful and update the dictionaries\n",
    "            if a_inv.numel() > 0 and g_inv.numel() > 0:\n",
    "                self.a_invs[module] = a_inv\n",
    "                self.g_invs[module] = g_inv\n",
    "            else:\n",
    "                print(f\"K-FAC inversion failed for a module. Skipping.\")\n",
    "        \n",
    "        self.factors_are_ready = True\n",
    "\n",
    "    def _invert_factors_fn(self):\n",
    "        with torch.no_grad():\n",
    "            for module in self.optimized_modules:\n",
    "                # Ensure damping is non-negative\n",
    "                damping = max(self.config.kfac_damping, 1e-6)\n",
    "                \n",
    "                # --- Invert Activation Covariance (a_cov) ---\n",
    "                a_cov_damped = self.a_covs[module].float() + damping * torch.eye(\n",
    "                    self.a_covs[module].shape[0], device='cpu'\n",
    "                )\n",
    "                \n",
    "                # Use cholesky_ex to check for positive-definiteness (a proxy for invertibility here)\n",
    "                # L_a is the Cholesky factor, info_a is 0 on success\n",
    "                L_a, info_a = torch.linalg.cholesky_ex(a_cov_damped)\n",
    "                \n",
    "                # --- Invert Gradient Covariance (g_cov) ---\n",
    "                g_cov_damped = self.g_covs[module].float() + damping * torch.eye(\n",
    "                    self.g_covs[module].shape[0], device='cpu'\n",
    "                )\n",
    "                L_g, info_g = torch.linalg.cholesky_ex(g_cov_damped)\n",
    "\n",
    "                # Check if both decompositions succeeded\n",
    "                if info_a == 0 and info_g == 0:\n",
    "                    # If successful, compute inverse from Cholesky factor (more stable)\n",
    "                    self.a_invs[module] = torch.cholesky_inverse(L_a).half()\n",
    "                    self.g_invs[module] = torch.cholesky_inverse(L_g).half()\n",
    "                else:\n",
    "                    # This block runs if inversion would have failed\n",
    "                    print(f\"K-FAC inversion check failed for a module. Skipping update.\")\n",
    "                    # You can still log this event if needed, but wandb calls might not be JIT-friendly.\n",
    "                    # It's better to handle logging outside the JIT-compiled function.\n",
    "                    continue\n",
    "\n",
    "    def precondition_gradients(self):\n",
    "        \"\"\"Apply K-FAC preconditioning with efficient CPU-GPU transfers\"\"\"\n",
    "        if not self.factors_are_ready:\n",
    "            return\n",
    "        if self.global_step % self.config.kfac_update_freq == 0:\n",
    "            print(f\"\\nGRITManager: Applying Natural Gradient preconditioner at step {self.global_step} to {self.global_step + 50}...\")\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # Determine correct dtype based on config\n",
    "            dtype = torch.float16 if self.config.precision == \"fp16\" else torch.bfloat16\n",
    "\n",
    "            for module in self.optimized_modules:\n",
    "                if module not in self.a_invs or module not in self.g_invs:\n",
    "                    continue\n",
    "                    \n",
    "                lora_a = module.lora_A['default']\n",
    "                lora_b = module.lora_B['default']\n",
    "                \n",
    "                if (lora_a is None or lora_b is None or \n",
    "                    lora_a.weight.grad is None or lora_b.weight.grad is None):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Move inverse matrices to GPU only when needed, using correct dtype\n",
    "                    a_inv = self.a_invs[module].to(self.device, dtype=dtype)\n",
    "                    g_inv = self.g_invs[module].to(self.device, dtype=dtype)\n",
    "                    \n",
    "                    # --- True Natural Gradient Computation: grad' = G_inv @ grad @ A_inv ---\n",
    "                    \n",
    "                    # Precondition LoRA B gradient (the \"easy\" part)\n",
    "                    # Original grad_b has shape (out, r)\n",
    "                    grad_b = lora_b.weight.grad.to(dtype)\n",
    "                    # Corrected multiplication: (out, r) @ (r, r) -> (out, r)\n",
    "                    preconditioned_b_grad = grad_b @ g_inv\n",
    "                    \n",
    "                    # Precondition LoRA A gradient (the \"hard\" part)\n",
    "                    # Original grad_a has shape (r, in)\n",
    "                    grad_a = lora_a.weight.grad.to(dtype)\n",
    "                    \n",
    "                    # Safety reshape for larger ranks\n",
    "                    r = module.r['default']\n",
    "                    if r > 0:\n",
    "                        grad_a = grad_a.view(r, -1)\n",
    "                    \n",
    "                    # Instead, we apply the preconditioning to each matrix's gradient.\n",
    "                    # grad_a' = A_inv @ grad_a\n",
    "                    # Corrected multiplication: (r, r) @ (r, in) -> (r, in)\n",
    "                    preconditioned_a_grad = a_inv @ grad_a\n",
    "\n",
    "                    # Copy back the preconditioned gradients\n",
    "                    lora_a.weight.grad.copy_(preconditioned_a_grad.to(lora_a.weight.grad.dtype))\n",
    "                    lora_b.weight.grad.copy_(preconditioned_b_grad.to(lora_b.weight.grad.dtype))\n",
    "                    \n",
    "                    # Clean up GPU tensors immediately\n",
    "                    del a_inv, g_inv, grad_a, grad_b, preconditioned_a_grad, preconditioned_b_grad\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Gradient preconditioning failed: {e}\")\n",
    "                    wandb.log({\"preconditioning_error\": 1})\n",
    "                    continue\n",
    "\n",
    "    def neural_reprojection(self):\n",
    "        \"\"\"Perform neural reprojection and log the parameter reduction.\"\"\"\n",
    "        print(f\"\\nGRITManager: Neural reprojection at step {self.global_step}...\")\n",
    "        \n",
    "        initial_params = 0\n",
    "        final_params = 0\n",
    "        log_dict = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # First, calculate initial effective parameters for all GRIT-optimized modules\n",
    "            for module in self.optimized_modules:\n",
    "                if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                    # Effective parameters in LoRA = r * (in_features + out_features)\n",
    "                    initial_params += module.r['default'] * (module.in_features + module.out_features)\n",
    "\n",
    "            for module in self.optimized_modules:\n",
    "                try:\n",
    "                    lora_a = module.lora_A['default']\n",
    "                    lora_b = module.lora_B['default']\n",
    "                    \n",
    "                    if lora_a is None or lora_b is None:\n",
    "                        continue\n",
    "                        \n",
    "                    A = lora_a.weight.data.float()\n",
    "                    B = lora_b.weight.data.float()\n",
    "                    \n",
    "                    r = A.shape[0]  # Current LoRA rank\n",
    "                    k = r # Initialize k to the current rank as a fallback\n",
    "                    \n",
    "                    # Form the r x r covariance matrix M = A @ A.T\n",
    "                    M = A @ A.T\n",
    "                    \n",
    "                    # --- Numerical Stability Check ---\n",
    "                    if torch.isnan(M).any() or torch.isinf(M).any():\n",
    "                        print(f\"WARNING: Covariance matrix M for {module.lora_name} contains NaN/Inf. Skipping reprojection for this module.\")\n",
    "                        log_dict[f\"reprojection_errors/nan_inf/{module.lora_name}\"] = 1\n",
    "                        # If skipped, the rank does not change.\n",
    "                        final_params += r * (module.in_features + module.out_features)\n",
    "                        continue\n",
    "\n",
    "                    # --- Rank Adaptation & Pruning Logic ---\n",
    "                    if self.config.enable_rank_adaptation and r > self.config.min_lora_rank:\n",
    "                        # --- Full Eigendecomposition for Adaptive Rank ---\n",
    "                        try:\n",
    "                            eigenvals, V = torch.linalg.eigh(M)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Eigendecomposition failed for module: {e}\", \"error\")\n",
    "                            log_dict[\"reprojection_errors/eigh_error\"] = log_dict.get(\"reprojection_errors/eigh_error\", 0) + 1\n",
    "                            final_params += r * (module.in_features + module.out_features)\n",
    "                            continue\n",
    "                        \n",
    "                        sorted_indices = torch.argsort(eigenvals, descending=True)\n",
    "                        sorted_eigenvals = eigenvals[sorted_indices]\n",
    "                        V = V[:, sorted_indices]\n",
    "                        \n",
    "                        # Log the sorted eigenvalue distribution to show energy decay\n",
    "                        # --- Enhanced WandB Logging for Eigen-spectra ---\n",
    "                        # --- Log Data Tables for Manual Plotting ---\n",
    "                        try:\n",
    "                            # 1. Log Eigen value spectra\n",
    "                            eigen_spectra_list = sorted_eigenvals.cpu().numpy().tolist()\n",
    "                            data_for_table = [[s] for s in eigen_spectra_list]\n",
    "                            table = wandb.Table(data=data_for_table, columns=[\"eigenvalue\"])\n",
    "                            hist_plot = wandb.plot.histogram(table, \"eigenvalue\", title=f\"Eigenvalue Spectrum - {module.lora_name}\")\n",
    "                            log_dict[f\"Charts/Eigenvalue Spectrum/{module.lora_name}\"] = hist_plot\n",
    "                            \n",
    "                            # 2. Log Eigenvector data as a Table for the heatmap\n",
    "                            V_k_T = V[:, :k].T.cpu().numpy()\n",
    "                            heatmap_data = []\n",
    "                            for x, eigenvector in enumerate(V_k_T):\n",
    "                                for y, component in enumerate(eigenvector):\n",
    "                                    heatmap_data.append([x, y, component])\n",
    "\n",
    "                            heatmap_table = wandb.Table(\n",
    "                                data=heatmap_data,\n",
    "                                columns=[\"eigenvector_idx\", \"component_idx\", \"value\"]\n",
    "                            )\n",
    "                            log_dict[f\"Data_Tables/Eigenvectors/{module.lora_name}\"] = heatmap_table\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Data logging for {module.lora_name} failed: {e}\")\n",
    "\n",
    "                        total_energy = torch.sum(sorted_eigenvals)\n",
    "                        if total_energy > 1e-6:\n",
    "                            cumulative_energy = torch.cumsum(sorted_eigenvals, dim=0) / total_energy\n",
    "                            k = (cumulative_energy < self.config.rank_adaptation_threshold).sum().item() + 1\n",
    "                        \n",
    "                        k = max(k, self.config.min_lora_rank) # Enforce minimum rank\n",
    "                        rank_reduction_percent = (1 - k / r) * 100 if r > 0 else 0\n",
    "                        \n",
    "                        log_dict[f\"GRIT/Effective Rank (k)/{module.lora_name}\"] = k\n",
    "                        log_dict[f\"GRIT/Rank Reduction (%)/{module.lora_name}\"] = rank_reduction_percent\n",
    "                        \n",
    "                        V_k = V[:, :k]\n",
    "                    else:\n",
    "                        # --- Fixed Rank Reprojection ---\n",
    "                        k = min(self.config.reprojection_k, r)\n",
    "                        try:\n",
    "                            if k < r and k > 0:\n",
    "                                _, V_k = torch.lobpcg(lambda v: M @ v, X=torch.randn(r, k, device=M.device, dtype=M.dtype), k=k, largest=True)\n",
    "                            else:\n",
    "                                _, V = torch.linalg.eigh(M)\n",
    "                                eigenvals, _ = torch.linalg.eigh(M)\n",
    "                                V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n",
    "                        except Exception as e:\n",
    "                            print(f\"LOBPCG failed for module, falling back to eigh: {e}\")\n",
    "                            _, V = torch.linalg.eigh(M)\n",
    "                            eigenvals, _ = torch.linalg.eigh(M)\n",
    "                            V_k = V[:, torch.argsort(eigenvals, descending=True)][:, :k]\n",
    "                    \n",
    "                    # Accumulate the new effective parameter count for this module\n",
    "                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                        final_params += k * (module.in_features + module.out_features)\n",
    "\n",
    "                    # --- Project onto the top-k eigenvectors and prune via zeroing ---\n",
    "                    A_proj = V_k.T @ A\n",
    "                    B_proj = B @ V_k\n",
    "\n",
    "                    A_new, B_new = torch.zeros_like(A), torch.zeros_like(B)\n",
    "                    A_new[:k, :], B_new[:, :k] = A_proj, B_proj\n",
    "\n",
    "                    lora_a.weight.data.copy_(A_new.to(A.dtype))\n",
    "                    lora_b.weight.data.copy_(B_new.to(B.dtype))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Neural reprojection failed for module {module.lora_name}: {e}\")\n",
    "                    log_dict[\"reprojection_errors/general_error\"] = log_dict.get(\"reprojection_errors/general_error\", 0) + 1\n",
    "                    # On failure, assume rank does not change for this module\n",
    "                    if hasattr(module, 'in_features') and hasattr(module, 'out_features'):\n",
    "                        final_params += module.r['default'] * (module.in_features + module.out_features)\n",
    "                    continue\n",
    "\n",
    "        # --- Log the final results ---\n",
    "        if initial_params > 0:\n",
    "            param_reduction = initial_params - final_params\n",
    "            reduction_percent = (param_reduction / initial_params) * 100\n",
    "            print(f\"✅ Neural reprojection completed. Effective parameter count reduced.\")\n",
    "            print(f\"   - Initial GRIT params: {initial_params:,}\")\n",
    "            print(f\"   - Final GRIT params:   {final_params:,}\")\n",
    "            print(f\"   - Reduction:           {param_reduction:,} ({reduction_percent:.2f}%)\")\n",
    "            \n",
    "            log_dict.update({\n",
    "                \"Parameters/GRIT Initial Params\": initial_params,\n",
    "                \"Parameters/GRIT Final Params\": final_params,\n",
    "                \"Parameters/GRIT Param Reduction (%)\": reduction_percent,\n",
    "            })\n",
    "        else:\n",
    "            print(f\"✅ Neural reprojection completed for {len(self.optimized_modules)} modules.\")\n",
    "        \n",
    "        if log_dict:\n",
    "            wandb.log(log_dict, step=self.global_step)\n",
    "        \n",
    "        # Strategic memory cleanup after expensive operations\n",
    "        gc.collect()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "class GritOptimizer(Optimizer):\n",
    "    \"\"\"\n",
    "    A wrapper for a PyTorch optimizer that applies GRIT's K-FAC gradient\n",
    "    preconditioning before the actual optimization step. This ensures that the\n",
    "    optimizer uses the natural gradient instead of the standard gradient.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer: Optimizer, grit_manager: 'GRITManager'):\n",
    "        self.optimizer = optimizer\n",
    "        self.grit_manager = grit_manager\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.optimizer.state\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "\n",
    "    @param_groups.setter\n",
    "    def param_groups(self, value):\n",
    "        self.optimizer.param_groups = value\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        1.  Applies K-FAC preconditioning to the accumulated gradients.\n",
    "        2.  Calls the underlying optimizer's step function.\n",
    "        \"\"\"\n",
    "        # Apply K-FAC preconditioning to the accumulated gradients\n",
    "        if self.grit_manager.factors_are_ready:\n",
    "            self.grit_manager.precondition_gradients()\n",
    "\n",
    "        # Call the underlying optimizer's step function\n",
    "        self.optimizer.step(closure)\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = False):\n",
    "        self.optimizer.zero_grad(set_to_none=set_to_none)\n",
    "\n",
    "    def add_param_group(self, param_group: dict):\n",
    "        self.optimizer.add_param_group(param_group)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"GritOptimizer({self.optimizer.__repr__()})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class GritCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    This callback injects GRIT's logic into the training loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, grit_manager):\n",
    "        self.grit_manager = grit_manager\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Triggered at the end of each training step.\n",
    "        \"\"\"\n",
    "        last_loss = state.log_history[-1].get(\"loss\") if state.log_history else None\n",
    "        self.grit_manager.step(loss=last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GritTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"\n",
    "    Optimized GRIT Trainer that addresses the core performance issues and\n",
    "    correctly applies gradient preconditioning by wrapping the optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grit_manager, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.grit_manager = grit_manager\n",
    "        print(\"GritTrainer: Initialized with GRIT implementation.\")\n",
    "\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        \"\"\"\n",
    "        Overrides the base method to wrap the created optimizer with our\n",
    "        GritOptimizer, which handles gradient preconditioning.\n",
    "        \"\"\"\n",
    "        super().create_optimizer_and_scheduler(num_training_steps)\n",
    "\n",
    "        if self.optimizer is not None:\n",
    "            print(\"🎁 Wrapping the optimizer with GRIT preconditioning logic.\")\n",
    "            self.optimizer = GritOptimizer(self.optimizer, self.grit_manager)\n",
    "\n",
    "    # --- THIS IS THE CORRECTED METHOD ---\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        The training step's signature is now aligned with the parent class.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        self.accelerator.backward(loss)\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "        \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        \"\"\"Overrides the default evaluate method to add aggressive memory cleanup.\"\"\"\n",
    "        print(\"\\n🧹 Clearing VRAM before evaluation...\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return super().evaluate(*args, **kwargs)\n",
    "\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "\n",
    "        has_labels = \"labels\" in inputs\n",
    "        if has_labels:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        _, generated_tokens, _ = super().prediction_step(\n",
    "            model, inputs, prediction_loss_only, ignore_keys\n",
    "        )\n",
    "\n",
    "        if has_labels:\n",
    "            inputs[\"labels\"] = labels\n",
    "\n",
    "        loss = None\n",
    "        if has_labels:\n",
    "            with torch.no_grad():\n",
    "                loss = self.compute_loss(model, inputs.copy()).detach()\n",
    "\n",
    "        if generated_tokens is not None and type(generated_tokens).__name__ == 'EmptyLogits':\n",
    "            batch_size = inputs[\"input_ids\"].shape[0]\n",
    "            seq_length = labels.shape[1] if labels is not None else inputs[\"input_ids\"].shape[1]\n",
    "            vocab_size = self.model.config.vocab_size\n",
    "\n",
    "            generated_tokens = torch.zeros(\n",
    "                (batch_size, seq_length, vocab_size),\n",
    "                device=self.accelerator.device,\n",
    "                dtype=config.unsloth_dtype\n",
    "            )\n",
    "\n",
    "        if labels is not None:\n",
    "            if len(labels.shape) == 3:\n",
    "                labels = torch.argmax(labels, dim=-1)\n",
    "            elif len(labels.shape) == 1:\n",
    "                batch_size = inputs[\"input_ids\"].shape[0]\n",
    "                labels = labels.view(batch_size, -1)\n",
    "\n",
    "        return loss, generated_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a442caa6cc054d55a776b8d474238720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,175,040 || all params: 3,221,924,864 || trainable%: 0.2848\n"
     ]
    }
   ],
   "source": [
    "# Your original QLoRA loading code\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16 if config.precision == \"fp16\" else torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_id, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "target_modules = []\n",
    "for i in range(0, 32):  # Last 8 layers only(for llama 3b 20-27)\n",
    "    for module in config.lora_target_modules:\n",
    "        if \"proj\" in module:  # attention modules\n",
    "            target_modules.append(f\"model.layers.{i}.self_attn.{module}\")\n",
    "        else:  # MLP modules  \n",
    "            target_modules.append(f\"model.layers.{i}.mlp.{module}\")\n",
    "            \n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_rank,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3131d86a1f3c4e69a970618c085315c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13509 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30376f7618094a23b895ab269d17e303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏁 Total training steps: 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchandrav\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250731_172428-lkprrai5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/RAAPID/GRIT-Final/runs/lkprrai5' target=\"_blank\">grit-Llama-3.2-3B-databricks-dolly-15k</a></strong> to <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/RAAPID/GRIT-Final/runs/lkprrai5' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final/runs/lkprrai5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- Dataset Preparation ----------------\n",
    "\n",
    "full_dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "\n",
    "# Use only 50% of the total data\n",
    "#reduced_dataset = full_dataset.select(range(len(full_dataset) // 2))\n",
    "\n",
    "dataset = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "# Reduce the validation set to a small, representative sample for faster evaluation\n",
    "#val_dataset = val_dataset.select(range(64))\n",
    "#print(f\"📉 Using a reduced validation set of {len(val_dataset)} samples for speed.\")\n",
    "\n",
    "def tokenize(example):\n",
    "    # The Dolly dataset uses 'instruction', 'context', and 'response'\n",
    "    instr = example['instruction'].strip()\n",
    "    inp = example['context'].strip()    # Changed from 'input' to 'context'\n",
    "    out = example['response'].strip()   # Changed from 'output' to 'response'\n",
    "    \n",
    "    if inp:\n",
    "        # The prompt structure is kept the same as your original code\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n{out}\"\n",
    "\n",
    "    tok = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=config.max_length,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # This labeling strategy is preserved from your original code\n",
    "    labels = tok[\"input_ids\"][:]\n",
    "    tok[\"labels\"] = labels\n",
    "    return tok\n",
    "\n",
    "# The rest of your code remains exactly the same\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)\n",
    "\n",
    "# ... your training script continues\n",
    "\n",
    "# Compute total training steps\n",
    "train_len = len(tokenized_train_dataset)\n",
    "eff_batch = config.batch_size * config.gradient_accumulation_steps\n",
    "total_steps = math.ceil(train_len / eff_batch) * config.num_epochs\n",
    "print(f\"🏁 Total training steps: {total_steps}\")\n",
    "\n",
    "run_name = f\"grit-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    entity=\"RAAPID\",\n",
    "    project=\"GRIT-Final\",\n",
    "    name=run_name,\n",
    "    job_type=\"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing GRITManager on device: cuda\n",
      "🎯 Instrumented 112 modules for GRIT optimization with custom autograd:\n",
      "   • 112 attention modules\n",
      "🚀 Using r-dim (16x16) covariances for maximum memory efficiency.\n",
      "GRITManager: Initialization complete.\n",
      "🔍 Optimizing 112 key LoRA modules.\n",
      "💾 K-FAC matrices stored on CPU for memory efficiency.\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Training Arguments ----------------\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    run_name=run_name,\n",
    "\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    learning_rate=config.learning_rate,\n",
    "\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=100,                 # Evaluate less often\n",
    "    logging_steps=50,\n",
    "    eval_accumulation_steps=1,\n",
    "    #max_steps=200,\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "    #save_steps=250,                 # Match eval_steps for early stopping\n",
    "    save_total_limit=2,\n",
    "\n",
    "    #fp16=config.precision == \"fp16\",\n",
    "    bf16=config.precision == \"bf16\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=config.num_workers,\n",
    "    dataloader_pin_memory=config.pin_memory,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    report_to=\"wandb\",\n",
    "    #metric_for_best_model=\"bleu\",\n",
    "    #greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    \n",
    "    # --- Generation settings for faster evaluation ---\n",
    "    generation_max_length=config.max_length + 128,\n",
    "    generation_num_beams=1,\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    #load_best_model_at_end=config.enable_early_stopping,\n",
    "\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"🚀 Initializing GRITManager on device:\", device)\n",
    "grit_manager = GRITManager(model, config, device)\n",
    "\n",
    "# Instantiate the new callback\n",
    "grit_callback = GritCallback(grit_manager)\n",
    "\n",
    "# Let the Trainer use its default high-performance optimizer\n",
    "# The custom 8-bit optimizer is not needed when memory is abundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Parameter Statistics:\n",
      "🔢 Total parameters: 1,812,638,720\n",
      "🔥 Trainable parameters: 9,175,040 (0.506%)\n",
      "📐 LoRA parameters: 9,175,040\n",
      "\n",
      "📍 Layer-wise LoRA Distribution:\n",
      "   Layer 0: 327,680 params\n",
      "   Layer 1: 327,680 params\n",
      "   Layer 2: 327,680 params\n",
      "   Layer 3: 327,680 params\n",
      "   Layer 4: 327,680 params\n",
      "   Layer 5: 327,680 params\n",
      "   Layer 6: 327,680 params\n",
      "   Layer 7: 327,680 params\n",
      "   Layer 8: 327,680 params\n",
      "   Layer 9: 327,680 params\n",
      "   Layer 10: 327,680 params\n",
      "   Layer 11: 327,680 params\n",
      "   Layer 12: 327,680 params\n",
      "   Layer 13: 327,680 params\n",
      "   Layer 14: 327,680 params\n",
      "   Layer 15: 327,680 params\n",
      "   Layer 16: 327,680 params\n",
      "   Layer 17: 327,680 params\n",
      "   Layer 18: 327,680 params\n",
      "   Layer 19: 327,680 params\n",
      "   Layer 20: 327,680 params\n",
      "   Layer 21: 327,680 params\n",
      "   Layer 22: 327,680 params\n",
      "   Layer 23: 327,680 params\n",
      "   Layer 24: 327,680 params\n",
      "   Layer 25: 327,680 params\n",
      "   Layer 26: 327,680 params\n",
      "   Layer 27: 327,680 params\n",
      "\n",
      "🎯 Strategy: LoRA + GRIT applied to layers 0-27\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count and display parameter statistics\"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    lora_params = 0\n",
    "    \n",
    "    layer_params = {}  # Track params by layer\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            if \"lora_\" in name:\n",
    "                lora_params += param.numel()\n",
    "                \n",
    "                # Extract layer number for statistics\n",
    "                for i in range(32):\n",
    "                    if f'layers.{i}.' in name:\n",
    "                        if i not in layer_params:\n",
    "                            layer_params[i] = 0\n",
    "                        layer_params[i] += param.numel()\n",
    "                        break\n",
    "    \n",
    "    print(f\"\\n📊 Parameter Statistics:\")\n",
    "    print(f\"🔢 Total parameters: {total_params:,}\")\n",
    "    print(f\"🔥 Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.3f}%)\")\n",
    "    print(f\"📐 LoRA parameters: {lora_params:,}\")\n",
    "    \n",
    "    print(f\"\\n📍 Layer-wise LoRA Distribution:\")\n",
    "    active_layers = sorted(layer_params.keys())\n",
    "    for layer_id in active_layers:\n",
    "        print(f\"   Layer {layer_id}: {layer_params[layer_id]:,} params\")\n",
    "    \n",
    "    if active_layers:\n",
    "        print(f\"\\n🎯 Strategy: LoRA + GRIT applied to layers {min(active_layers)}-{max(active_layers)}\")\n",
    "\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Call this after model setup\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "if wandb.run:\n",
    "    wandb.config.update({\n",
    "        \"total_model_params\": total_params,\n",
    "        \"lora_trainable_params\": trainable_params,\n",
    "        \"initial_lora_rank_r\": config.lora_rank,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2340/1386230625.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `GritTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GritTrainer: Initialized with GRIT implementation.\n",
      "🎯 Effective batch (per step): 32\n",
      "⚡ Mixed precision: bf16\n",
      "📏 Max sequence length: 1024\n",
      "💾 Checkpoints will be saved every 500 steps.\n",
      "🔧 LoRA rank: 16\n",
      "🚀 Starting training now...\n",
      "🎁 Wrapping the optimizer with GRIT preconditioning logic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='423' max='423' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [423/423 49:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.298200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRITManager: Inverting K-FAC factors at step 50...\n",
      "\n",
      "GRITManager: Neural reprojection at step 50...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   8,601,600\n",
      "   - Reduction:           573,440 (6.25%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 50 to 100...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 87...\n",
      "\n",
      "GRITManager: Neural reprojection at step 87...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   8,028,160\n",
      "   - Reduction:           1,146,880 (12.50%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 100 to 150...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 124...\n",
      "\n",
      "GRITManager: Neural reprojection at step 124...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   7,454,720\n",
      "   - Reduction:           1,720,320 (18.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 150 to 200...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 169...\n",
      "\n",
      "GRITManager: Neural reprojection at step 169...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   6,881,280\n",
      "   - Reduction:           2,293,760 (25.00%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 200 to 250...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 206...\n",
      "\n",
      "GRITManager: Neural reprojection at step 206...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   6,307,840\n",
      "   - Reduction:           2,867,200 (31.25%)\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 243...\n",
      "\n",
      "GRITManager: Neural reprojection at step 243...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,734,400\n",
      "   - Reduction:           3,440,640 (37.50%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 250 to 300...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 280...\n",
      "\n",
      "GRITManager: Neural reprojection at step 280...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 300 to 350...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 317...\n",
      "\n",
      "GRITManager: Neural reprojection at step 317...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 350 to 400...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 368...\n",
      "\n",
      "GRITManager: Neural reprojection at step 368...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n",
      "\n",
      "GRITManager: Applying Natural Gradient preconditioner at step 400 to 450...\n",
      "\n",
      "GRITManager: Inverting K-FAC factors at step 405...\n",
      "\n",
      "GRITManager: Neural reprojection at step 405...\n",
      "✅ Neural reprojection completed. Effective parameter count reduced.\n",
      "   - Initial GRIT params: 9,175,040\n",
      "   - Final GRIT params:   5,160,960\n",
      "   - Reduction:           4,014,080 (43.75%)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.k_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.o_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.q_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.v_proj</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.k_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.o_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.q_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.v_proj</td><td>▁▂▃▅▆▇████</td></tr><tr><td>Parameters/GRIT Final Params</td><td>█▇▆▅▃▂▁▁▁▁</td></tr><tr><td>Parameters/GRIT Initial Params</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Parameters/GRIT Param Reduction (%)</td><td>▁▂▃▅▆▇████</td></tr><tr><td>adaptive_kfac_damping</td><td>▁█▁▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▃▃▄▄▅▅▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>▃█▃▃▇▂▆▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.0.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.1.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.10.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.11.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.12.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.13.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.14.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.15.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.16.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.17.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.18.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.19.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.2.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.20.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.21.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.22.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.23.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.24.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.25.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.26.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.27.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.3.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.4.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.5.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.6.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.7.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.8.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.k_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.o_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.q_proj</td><td>9</td></tr><tr><td>GRIT/Effective Rank (k)/base_model.model.model.layers.9.self_attn.v_proj</td><td>9</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.0.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.1.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.10.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.11.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.12.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.13.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.14.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.15.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.16.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.17.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.18.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.19.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.2.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.20.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.21.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.22.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.23.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.24.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.25.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.26.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.27.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.3.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.4.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.5.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.6.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.7.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.8.self_attn.v_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.k_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.o_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.q_proj</td><td>43.75</td></tr><tr><td>GRIT/Rank Reduction (%)/base_model.model.model.layers.9.self_attn.v_proj</td><td>43.75</td></tr><tr><td>Parameters/GRIT Final Params</td><td>5160960</td></tr><tr><td>Parameters/GRIT Initial Params</td><td>9175040</td></tr><tr><td>Parameters/GRIT Param Reduction (%)</td><td>43.75</td></tr><tr><td>adaptive_kfac_damping</td><td>0.001</td></tr><tr><td>total_flos</td><td>2.3471557099860787e+17</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>423</td></tr><tr><td>train/grad_norm</td><td>0.59405</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2982</td></tr><tr><td>train_loss</td><td>0.43427</td></tr><tr><td>train_runtime</td><td>2954.0491</td></tr><tr><td>train_samples_per_second</td><td>4.573</td></tr><tr><td>train_steps_per_second</td><td>0.143</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grit-Llama-3.2-3B-databricks-dolly-15k</strong> at: <a href='https://wandb.ai/RAAPID/GRIT-Final/runs/lkprrai5' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final/runs/lkprrai5</a><br> View project at: <a href='https://wandb.ai/RAAPID/GRIT-Final' target=\"_blank\">https://wandb.ai/RAAPID/GRIT-Final</a><br>Synced 5 W&B file(s), 2240 media file(s), 4480 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250731_172428-lkprrai5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Trainer uses GRITManager for K-FAC steps\n",
    "trainer = GritTrainer(\n",
    "    grit_manager=grit_manager,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[grit_callback],\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)] if config.enable_early_stopping else [],\n",
    ")\n",
    "\n",
    "# Final memory cleanup\n",
    "import gc\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(f\"🎯 Effective batch (per step): {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(\"⚡ Mixed precision:\", config.precision)\n",
    "print(\"📏 Max sequence length:\", config.max_length)\n",
    "print(f\"💾 Checkpoints will be saved every {training_args.save_steps} steps.\")\n",
    "print(\"🔧 LoRA rank:\", config.lora_rank)\n",
    "print(\"🚀 Starting training now...\")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "\n",
    "print(\"🎉 Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Uploading fine-tuned model to Hugging Face Hub...\n",
      "\n",
      "🚀 Uploading fine-tuned model to Hugging Face Hub...\n",
      "📝 Model card created with training details.\n",
      "🌐 Uploading model to https://huggingface.co/te4bag/GRIT-Full-databricks-llama-3.2-3B-Energy-0.9 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9f02b0a0f144f08783731cab05b20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/53.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model successfully uploaded: https://huggingface.co/te4bag/GRIT-Full-databricks-llama-3.2-3B-Energy-0.9\n",
      "🎉 GRIT fine-tuning script completed!\n"
     ]
    }
   ],
   "source": [
    "# === Hugging Face Upload ===\n",
    "from huggingface_hub import create_repo, upload_folder, HfApi\n",
    "\n",
    "print(\"\\n🚀 Uploading fine-tuned model to Hugging Face Hub...\")\n",
    "\n",
    "# === Hugging Face Upload ===\n",
    "print(\"\\n🚀 Uploading fine-tuned model to Hugging Face Hub...\")\n",
    "\n",
    "HF_USERNAME = \"te4bag\" # Replace with your Hugging Face username\n",
    "FULL_MODEL_NAME = f\"{HF_USERNAME}/GRIT-Full-databricks-llama-3.2-3B-Energy-0.9\"\n",
    "\n",
    "try:\n",
    "    # Save model and tokenizer locally first\n",
    "    output_dir = \"./grit_trained_model\"\n",
    "    model.save_pretrained(output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Create/update model card (with hardware and hyperparams)\n",
    "    model_card_content = f\"\"\"---\n",
    "tags:\n",
    "- llama\n",
    "- alpaca\n",
    "- grit\n",
    "- lora\n",
    "- qlora\n",
    "- unsloth\n",
    "- instruction-tuning\n",
    "- fine-tuned\n",
    "base_model: {config.model_id}\n",
    "library_name: peft\n",
    "license: apache-2.0\n",
    "datasets:\n",
    "- {config.dataset_name}\n",
    "language:\n",
    "- en\n",
    "pipeline_tag: text-generation\n",
    "---\n",
    "\n",
    "# {config.model_id} Fine-tuned with GRIT and QLoRA\n",
    "\n",
    "This model is a fine-tuned version of [{config.model_id}](https://huggingface.co/{config.model_id}) using the **GRIT** (Geometric Reprojection Instruction Tuning) algorithm and **QLoRA** on the [{config.dataset_name} dataset](https://huggingface.co/datasets/{config.dataset_name}).\n",
    "\n",
    "The base model is quantized to 4-bit (NF4) to enable efficient fine-tuning.\n",
    "\n",
    "## 🚀 Training Details\n",
    "\n",
    "### GRIT Algorithm\n",
    "- **K-FAC Updates**: Every {config.kfac_update_freq} steps (adaptive) for second-order preconditioning.\n",
    "- **Neural Reprojection**: Every {config.reprojection_freq} steps (adaptive) for rank optimization.\n",
    "- **Rank Adaptation**: {'Enabled' if config.enable_rank_adaptation else 'Disabled'} (Threshold: {config.rank_adaptation_threshold}, Min Rank: {config.min_lora_rank}).\n",
    "- **Optimized LoRA Modules**: {config.lora_target_modules}\n",
    "\n",
    "### Fine-tuning Configuration\n",
    "- **Base Model**: {config.model_id}\n",
    "- **Quantization**: 4-bit (NF4) with {config.precision} compute.\n",
    "- **LoRA Rank**: {config.lora_rank}\n",
    "- **LoRA Alpha**: {config.lora_alpha}\n",
    "- **Batch Size**: {config.batch_size} (per device)\n",
    "- **Gradient Accumulation**: {config.gradient_accumulation_steps} (Effective batch = {config.batch_size * config.gradient_accumulation_steps})\n",
    "- **Learning Rate**: {config.learning_rate:.1e}\n",
    "- **Precision**: {config.precision} mixed precision\n",
    "- **Sequence Length**: {config.max_length} tokens\n",
    "- **Gradient Checkpointing**: Enabled\n",
    "\n",
    "### Performance Improvements\n",
    "- ✅ **Faster Convergence**: K-FAC preconditioning aligns updates with curvature.\n",
    "- ✅ **Memory-Efficient**: 4-bit quantization (QLoRA) and gradient checkpointing used.\n",
    "- ✅ **Adaptive Rank**: Dynamically prunes LoRA rank to improve parameter efficiency.\n",
    "\n",
    "## 📊 Training Metrics\n",
    "- **Total Steps**: {trainer.state.global_step if 'trainer' in locals() else 'N/A'}\n",
    "- **Final Loss**: {trainer.state.log_history[-1].get('train_loss', 'N/A') if 'trainer' in locals() and trainer.state.log_history else 'N/A'}\n",
    "- **Trainable Params**: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\n",
    "\n",
    "## 📝 Algorithm Details\n",
    "- **K-FAC Preconditioning** (Natural Gradient) and **Neural Reprojection** as per GRIT method.\n",
    "- **Memory Efficient**: Covariance matrices on CPU to reduce GPU load.\n",
    "\n",
    "## 🏆 Results\n",
    "In benchmark comparisons, GRIT has shown **faster convergence and better stability** than standard LoRA or fine-tuning, making it well-suited for efficient single-epoch training. The use of Unsloth further accelerates this process.\n",
    "\n",
    "## 📝 Citation\n",
    "If you use this model, please cite the original GRIT paper and:\n",
    "```bibtex\n",
    "@misc{{grit-lora-{config.model_id.split('/')[-1]}-{config.dataset_name.split('/')[-1]}}},\n",
    "  title={{ {config.model_id} Fine-tuned with GRIT on {config.dataset_name} }},\n",
    "  author={{{HF_USERNAME}}},\n",
    "  year={{2024}},\n",
    "  publisher={{Hugging Face}},\n",
    "  url={{https://huggingface.co/{FULL_MODEL_NAME}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## ⚖️ License\n",
    "This model inherits the Apache 2.0 license.\n",
    "\"\"\"\n",
    "    with open(f\"{output_dir}/README.md\", \"w\") as f:\n",
    "        f.write(model_card_content)\n",
    "    print(\"📝 Model card created with training details.\")\n",
    "\n",
    "    # Create repository on the Hub and upload the folder\n",
    "    print(f\"🌐 Uploading model to https://huggingface.co/{FULL_MODEL_NAME} ...\")\n",
    "    create_repo(FULL_MODEL_NAME, exist_ok=True)\n",
    "    api = HfApi()\n",
    "    api.upload_folder(\n",
    "        folder_path=output_dir,\n",
    "        repo_id=FULL_MODEL_NAME,\n",
    "        commit_message=f\"GRIT fine-tuned {config.model_id.split('/')[-1]} on {config.dataset_name.split('/')[-1]}\"\n",
    "    )\n",
    "    print(f\"✅ Model successfully uploaded: https://huggingface.co/{FULL_MODEL_NAME}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Upload failed: {e}\\\\n(Model saved locally in {output_dir})\")\n",
    "\n",
    "print(\"🎉 GRIT fine-tuning script completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
